{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivy Plus MARC Analysis with Enhanced Matching\n",
    "\n",
    "This notebook processes MARC data from Ivy Plus libraries to identify unique records held by Penn that are not held by other institutions in the consortium.\n",
    "\n",
    "## Enhanced Normalization and Matching\n",
    "\n",
    "The matching process has been improved with specialized normalization for different fields:\n",
    "\n",
    "1. **ISBN/LCCN Matching**: When standard identifiers are available, they are normalized and used as primary match keys\n",
    "   - ISBN-10 and ISBN-13 are properly normalized to ensure consistent matching\n",
    "   - LCCNs are standardized to handle different formats and prefixes\n",
    "\n",
    "2. **Match Key Creation**: For records without standard identifiers, a composite key is created from:\n",
    "   - Normalized title (with improved noise word removal)\n",
    "   - Normalized edition statement\n",
    "   - Normalized publication information with year extraction\n",
    "\n",
    "3. **Match Key Validation**: Each match key is validated for quality to detect potential issues\n",
    "   - Short or generic match keys are flagged\n",
    "   - Match key quality metrics are saved for analysis\n",
    "\n",
    "4. **Field Selection**: \n",
    "   - Leader (FLDR) is now included for record type identification\n",
    "   - F007 is excluded as it's not appropriate for unique record identification (not all records have this field).\n",
    "   - Core bibliographic fields (F001, F010, F020, F245, F250, F260) are used\n",
    "\n",
    "This improved approach maintains the principle that different editions, printings, and formats are unique bibliographic entities while enhancing the accuracy of matching across cataloging variations.\n",
    "\n",
    "## Initial load only - Institution-specific Processing\n",
    "Converts MARC to Parquet format for faster processing, maintaining institution-specific separation. This step ensures that each institution's MARC files are converted to separate Parquet files for consistent downstream processing.\n",
    "\n",
    "The conversion includes the leader field (FLDR) for each record while excluding the 007 field to optimize the output files. The leader contains important information about the record structure, material type, and bibliographic level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Optimized Spark Configuration:\n",
      "  - Adaptive Query Execution: Enabled\n",
      "  - Memory Allocation: Reduced and optimized\n",
      "  - Arrow Integration: Enabled for faster Pandas conversion\n",
      "  - Vectorized Reading: Enabled for Parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/15 06:35:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/15 06:35:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New SparkContext created\n",
      "‚úÖ Spark session initialized with optimized settings\n",
      "Spark UI available at: http://192.168.1.37:4041\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# OPTIMIZED: Spark Configuration for Performance\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PodProcessing-Optimized\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\\n",
    "    .set(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .set(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .set(\"spark.sql.parquet.columnarReaderBatchSize\", \"4096\")\n",
    "\n",
    "print(\"üöÄ Optimized Spark Configuration:\")\n",
    "print(\"  - Adaptive Query Execution: Enabled\")\n",
    "print(\"  - Memory Allocation: Reduced and optimized\")\n",
    "print(\"  - Arrow Integration: Enabled for faster Pandas conversion\")\n",
    "print(\"  - Vectorized Reading: Enabled for Parquet files\")\n",
    "\n",
    "# Check if SparkContext already exists and stop it if so\n",
    "try:\n",
    "    if 'sc' in locals() or 'sc' in globals():\n",
    "        print(\"‚ö†Ô∏è  Existing SparkContext detected. Stopping it...\")\n",
    "        sc.stop()\n",
    "        print(\"‚úÖ Previous SparkContext stopped\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if 'spark' in locals() or 'spark' in globals():\n",
    "        print(\"‚ö†Ô∏è  Existing SparkSession detected. Stopping it...\")\n",
    "        spark.stop()\n",
    "        print(\"‚úÖ Previous SparkSession stopped\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    pass\n",
    "\n",
    "# Initialize new SparkContext with optimized configuration\n",
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print(\"‚úÖ New SparkContext created\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SparkContext: {e}\")\n",
    "    # Try getting existing context\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "    print(\"‚úÖ Using existing SparkContext with new configuration\")\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "print(\"‚úÖ Spark session initialized with optimized settings\")\n",
    "print(f\"Spark UI available at: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: pymarc in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (5.1.2)\n",
      "Collecting poetry\n",
      "  Downloading poetry-2.1.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: marctable in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (0.4.0)\n",
      "Collecting fuzzywuzzy\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting build<2.0.0,>=1.2.1 (from poetry)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cachecontrol<0.15.0,>=0.14.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Downloading cachecontrol-0.14.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
      "  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dulwich<0.23.0,>=0.22.6 (from poetry)\n",
      "  Downloading dulwich-0.22.8-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting fastjsonschema<3.0.0,>=2.18.0 (from poetry)\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting findpython<0.7.0,>=0.6.2 (from poetry)\n",
      "  Downloading findpython-0.6.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
      "  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
      "Collecting keyring<26.0.0,>=25.1.0 (from poetry)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=24.0 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from poetry) (24.0)\n",
      "Collecting pbs-installer<2026.0.0,>=2025.1.6 (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Downloading pbs_installer-2025.7.12-py3-none-any.whl.metadata (991 bytes)\n",
      "Collecting pkginfo<2.0,>=1.12 (from poetry)\n",
      "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.0.0 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from poetry) (4.2.0)\n",
      "Collecting poetry-core==2.1.3 (from poetry)\n",
      "  Downloading poetry_core-2.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pyproject-hooks<2.0.0,>=1.0.0 (from poetry)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from poetry) (2.31.0)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from poetry)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting shellingham<2.0,>=1.5 (from poetry)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tomlkit<1.0.0,>=0.11.4 (from poetry)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
      "  Downloading trove_classifiers-2025.5.9.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting virtualenv<21.0.0,>=20.26.6 (from poetry)\n",
      "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting xattr<2.0.0,>=1.0.0 (from poetry)\n",
      "  Downloading xattr-1.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting msgpack<2.0.0,>=0.5.2 (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting filelock>=3.8.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting crashtest<0.5.0,>=0.4.1 (from cleo<3.0.0,>=2.1.0->poetry)\n",
      "  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from dulwich<0.23.0,>=0.22.6->poetry) (2.2.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.11.4 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (7.1.0)\n",
      "Collecting jaraco.classes (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading jaraco_functools-4.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpx<1,>=0.27.0 (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting zstandard>=0.21.0 (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (3.7)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from requests<3.0,>=2.26->poetry) (3.3.2)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.26.6->poetry)\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting cffi>=1.16.0 (from xattr<2.0.0,>=1.0.0->poetry)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from marctable) (4.12.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from marctable) (8.1.7)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from marctable) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<15.0.0,>=14.0.2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from marctable) (14.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->marctable) (2.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->marctable) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->marctable) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->marctable) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->marctable) (2024.1)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: six in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Collecting pycparser (from cffi>=1.16.0->xattr<2.0.0,>=1.0.0->poetry)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from importlib_metadata>=4.11.4->keyring<26.0.0,>=25.1.0->poetry) (3.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.11.0)\n",
      "Collecting more-itertools (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading poetry-2.1.3-py3-none-any.whl (278 kB)\n",
      "Downloading poetry_core-2.1.3-py3-none-any.whl (332 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading cachecontrol-0.14.3-py3-none-any.whl (21 kB)\n",
      "Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
      "Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading dulwich-0.22.8-py3-none-any.whl (273 kB)\n",
      "Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Downloading findpython-0.6.3-py3-none-any.whl (20 kB)\n",
      "Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-macosx_10_9_x86_64.whl (82 kB)\n",
      "Downloading pbs_installer-2025.7.12-py3-none-any.whl (59 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-macosx_10_9_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading xattr-1.2.0-cp311-cp311-macosx_10_9_x86_64.whl (19 kB)\n",
      "Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp311-cp311-macosx_10_9_x86_64.whl (174 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-macosx_10_9_x86_64.whl (182 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading trove_classifiers-2025.5.9.12-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl (788 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m788.7/788.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading jaraco_functools-4.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: trove-classifiers, fuzzywuzzy, fastjsonschema, distlib, zstandard, tomlkit, sniffio, shellingham, rapidfuzz, pyproject-hooks, pycparser, poetry-core, pkginfo, pbs-installer, msgpack, more-itertools, langdetect, installer, h11, findpython, filelock, dulwich, crashtest, backports.tarfile, virtualenv, requests-toolbelt, Levenshtein, jaraco.functools, jaraco.context, jaraco.classes, httpcore, cleo, cffi, cachecontrol, build, anyio, xattr, python-Levenshtein, keyring, httpx, poetry\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41/41\u001b[0m [poetry] [poetry] [cachecontrol]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Levenshtein-0.27.1 anyio-4.9.0 backports.tarfile-1.2.0 build-1.2.2.post1 cachecontrol-0.14.3 cffi-1.17.1 cleo-2.1.0 crashtest-0.4.1 distlib-0.3.9 dulwich-0.22.8 fastjsonschema-2.21.1 filelock-3.18.0 findpython-0.6.3 fuzzywuzzy-0.18.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 installer-0.7.0 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.2.1 keyring-25.6.0 langdetect-1.0.9 more-itertools-10.7.0 msgpack-1.1.1 pbs-installer-2025.7.12 pkginfo-1.12.1.2 poetry-2.1.3 poetry-core-2.1.3 pycparser-2.22 pyproject-hooks-1.2.0 python-Levenshtein-0.27.1 rapidfuzz-3.13.0 requests-toolbelt-1.0.0 shellingham-1.5.4 sniffio-1.3.1 tomlkit-0.13.3 trove-classifiers-2025.5.9.12 virtualenv-20.31.2 xattr-1.2.0 zstandard-0.23.0\n",
      "Added /Users/jimhahn/.local/bin to PATH\n",
      "Added /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/bin to PATH\n",
      "‚úÖ marctable command found in PATH\n",
      "\n",
      "‚úÖ All packages installed and environment configured\n",
      "Current PATH: /Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/bin:/Users/jimhahn/.nvm/versions/node/v23.11.0/bin:/Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/bin:/Users/jimhahn/micromamba/condabin:/opt/homebrew/opt/node@14/bin:/Users/jimhahn/.rbenv/shims:/Users/jimhahn/.rbenv/bin:/Users/jimhahn/.pyenv/shims:/opt/homebrew/opt/ruby/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Applications/Privileges.app/Contents/Resources:/Library/TeX/texbin:/Users/jimhahn/.rvm/bin:/Users/jimhahn/.lmstudio/bin:/Users/jimhahn/.local/bin:/Users/jimhahn/Documents/GitHub/sinopia2alma_processing/.conda/bin\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pymarc poetry marctable fuzzywuzzy python-Levenshtein langdetect\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the user's local bin directory for macOS\n",
    "user_local_bin = os.path.expanduser('~/.local/bin')\n",
    "\n",
    "# Add the directory to PATH if it exists\n",
    "if os.path.exists(user_local_bin):\n",
    "    os.environ['PATH'] += os.pathsep + user_local_bin\n",
    "    print(f\"Added {user_local_bin} to PATH\")\n",
    "\n",
    "# Also add Python's user site-packages bin directory\n",
    "python_user_bin = os.path.join(sys.prefix, 'bin')\n",
    "if os.path.exists(python_user_bin):\n",
    "    os.environ['PATH'] += os.pathsep + python_user_bin\n",
    "    print(f\"Added {python_user_bin} to PATH\")\n",
    "\n",
    "# For Homebrew Python installations on macOS\n",
    "homebrew_bin = '/usr/local/bin'\n",
    "if os.path.exists(homebrew_bin) and homebrew_bin not in os.environ['PATH']:\n",
    "    os.environ['PATH'] += os.pathsep + homebrew_bin\n",
    "    print(f\"Added {homebrew_bin} to PATH\")\n",
    "\n",
    "# Check if marctable is accessible\n",
    "import shutil\n",
    "if shutil.which('marctable'):\n",
    "    print(\"‚úÖ marctable command found in PATH\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  marctable command not found in PATH - checking alternative locations...\")\n",
    "    # Try to find marctable in common locations\n",
    "    possible_locations = [\n",
    "        os.path.expanduser('~/Library/Python/3.11/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.10/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.9/bin'),\n",
    "        '/opt/homebrew/bin',\n",
    "        '/usr/local/bin',\n",
    "    ]\n",
    "    \n",
    "    for loc in possible_locations:\n",
    "        marctable_path = os.path.join(loc, 'marctable')\n",
    "        if os.path.exists(marctable_path):\n",
    "            os.environ['PATH'] += os.pathsep + loc\n",
    "            print(f\"‚úÖ Found marctable in {loc} and added to PATH\")\n",
    "            break\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed and environment configured\")\n",
    "print(f\"Current PATH: {os.environ['PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimized Spark SQL functions loaded - ready to replace UDFs\n",
      "üìö Enhanced edition normalization now handles:\n",
      "   - Spelled-out editions (first, second, third, fourth, fifth)\n",
      "   - Numeric editions with suffixes (1st, 2nd, 3rd, 4th)\n",
      "   - Various edition formats (ed., edition)\n",
      "   - All normalized to consistent 'N ed' format\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZATION: Replace Python UDFs with Spark SQL Functions\n",
    "\n",
    "def create_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Create match keys using pure Spark SQL functions - MUCH faster than UDFs\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"match_key\", \n",
    "        F.concat_ws(\"_\",\n",
    "            # Normalize title using SQL functions\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(\n",
    "                            F.lower(F.trim(F.col(\"F245\"))),\n",
    "                            \"^(the|a|an)\\\\s+\", \"\"  # Remove leading articles\n",
    "                        ),\n",
    "                        \"[^a-z0-9\\\\s]\", \"\"  # Remove special characters\n",
    "                    ),\n",
    "                    \"\\\\s+\", \" \"  # Normalize whitespace\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # ENHANCED: Normalize edition with spelled-out numbers\n",
    "            F.when(F.col(\"F250\").isNotNull(),\n",
    "                # First convert spelled-out editions to numbers\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(\n",
    "                            F.regexp_replace(\n",
    "                                F.regexp_replace(\n",
    "                                    F.regexp_replace(\n",
    "                                        F.lower(F.col(\"F250\")),\n",
    "                                        \"\\\\bfirst\\\\b\", \"1\"\n",
    "                                    ),\n",
    "                                    \"\\\\bsecond\\\\b\", \"2\"\n",
    "                                ),\n",
    "                                \"\\\\bthird\\\\b\", \"3\"\n",
    "                            ),\n",
    "                            \"\\\\bfourth\\\\b\", \"4\"\n",
    "                        ),\n",
    "                        \"\\\\bfifth\\\\b\", \"5\"\n",
    "                    ),\n",
    "                    # Then normalize all numeric editions to \"N ed\" format\n",
    "                    \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed\\\\.?|edition)\", \"$1 ed\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Extract year from publication using SQL functions\n",
    "            F.when(F.col(\"F260\").isNotNull(),\n",
    "                F.regexp_extract(F.col(\"F260\"), \"(1[0-9]{3}|20[0-9]{2})\", 1)\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def normalize_ids_spark(df):\n",
    "    \"\"\"\n",
    "    Normalize ISBN and LCCN using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"normalized_isbn\",\n",
    "        F.when(F.col(\"F020\").isNotNull(),\n",
    "            # Enhanced ISBN extraction - more specific pattern\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"F020\"), \"([0-9]{9,13}[0-9Xx]?)\", 1),\n",
    "                \"[^0-9X]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"normalized_lccn\", \n",
    "        F.when(F.col(\"F010\").isNotNull(),\n",
    "            F.regexp_replace(\n",
    "                F.trim(F.col(\"F010\")),\n",
    "                \"[^a-zA-Z0-9\\\\-]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def add_id_list_spark(df):\n",
    "    \"\"\"\n",
    "    Create id_list using Spark SQL array functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"id_list\",\n",
    "        F.array_remove(\n",
    "            F.array(\n",
    "                F.when(F.col(\"normalized_isbn\").isNotNull() & (F.col(\"normalized_isbn\") != \"\"), \n",
    "                       F.col(\"normalized_isbn\")),\n",
    "                F.when(F.col(\"normalized_lccn\").isNotNull() & (F.col(\"normalized_lccn\") != \"\"), \n",
    "                       F.col(\"normalized_lccn\"))\n",
    "            ),\n",
    "            None\n",
    "        )\n",
    "    )\n",
    "\n",
    "def validate_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Validate match keys using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_valid_match_key\",\n",
    "        (F.length(F.col(\"match_key\")) >= 5) &\n",
    "        (~F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"))\n",
    "    ).withColumn(\"match_key_message\",\n",
    "        F.when(F.length(F.col(\"match_key\")) < 5, \"Match key too short\")\n",
    "         .when(F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"), \"Generic match key\")\n",
    "         .otherwise(\"Valid match key\")\n",
    "    )\n",
    "\n",
    "def process_institution_optimized(df, institution_name):\n",
    "    \"\"\"\n",
    "    Apply all optimizations to an institution's DataFrame\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"source\", F.lit(institution_name))\n",
    "        .transform(normalize_ids_spark)\n",
    "        .transform(create_match_key_spark)\n",
    "        .transform(add_id_list_spark)\n",
    "        .transform(validate_match_key_spark)\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Optimized Spark SQL functions loaded - ready to replace UDFs\")\n",
    "print(\"üìö Enhanced edition normalization now handles:\")\n",
    "print(\"   - Spelled-out editions (first, second, third, fourth, fifth)\")\n",
    "print(\"   - Numeric editions with suffixes (1st, 2nd, 3rd, 4th)\")\n",
    "print(\"   - Various edition formats (ed., edition)\")\n",
    "print(\"   - All normalized to consistent 'N ed' format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing parquet files...\n",
      "No parquet files found. Running conversion...\n",
      "Found 13 processed MARC files in pod-processing-outputs/final\n",
      "\n",
      "Total institution-specific MARC files to process: 13\n",
      "  - brown: pod-processing-outputs/final/brown_filtered.mrc\n",
      "  - chicago: pod-processing-outputs/final/chicago_filtered.mrc\n",
      "  - columbia: pod-processing-outputs/final/columbia_filtered.mrc\n",
      "  - cornell: pod-processing-outputs/final/cornell_filtered.mrc\n",
      "  - dartmouth: pod-processing-outputs/final/dartmouth_filtered.mrc\n",
      "  - duke: pod-processing-outputs/final/duke_filtered.mrc\n",
      "  - harvard: pod-processing-outputs/final/harvard_filtered.mrc\n",
      "  - johns: pod-processing-outputs/final/johns_filtered.mrc\n",
      "  - mit: pod-processing-outputs/final/mit_filtered.mrc\n",
      "  - penn: pod-processing-outputs/final/penn_filtered.mrc\n",
      "  - princeton: pod-processing-outputs/final/princeton_filtered.mrc\n",
      "  - stanford: pod-processing-outputs/final/stanford_filtered.mrc\n",
      "  - yale: pod-processing-outputs/final/yale_filtered.mrc\n",
      "Running marctable: marctable parquet /var/folders/x_/_g_bgvs97r35l8ys1yvnqvrh0000gs/T/tmpqs3yoe0u pod-processing-outputs/brown_brown_filtered-marc21.parquet\n",
      "SUCCESS: Created pod-processing-outputs/brown_brown_filtered-marc21.parquet with 737290 brown records (100.0% success rate)\n",
      "  Note: FLDR (leader) field is included by default in marctable output\n",
      "Running marctable: marctable parquet /var/folders/x_/_g_bgvs97r35l8ys1yvnqvrh0000gs/T/tmpuz8sqfuz pod-processing-outputs/chicago_chicago_filtered-marc21.parquet\n",
      "SUCCESS: Created pod-processing-outputs/chicago_chicago_filtered-marc21.parquet with 12294163 chicago records (100.0% success rate)\n",
      "  Note: FLDR (leader) field is included by default in marctable output\n",
      "Running marctable: marctable parquet /var/folders/x_/_g_bgvs97r35l8ys1yvnqvrh0000gs/T/tmpmuw99utt pod-processing-outputs/columbia_columbia_filtered-marc21.parquet\n",
      "SUCCESS: Created pod-processing-outputs/columbia_columbia_filtered-marc21.parquet with 16836893 columbia records (100.0% success rate)\n",
      "  Note: FLDR (leader) field is included by default in marctable output\n",
      "Running marctable: marctable parquet /var/folders/x_/_g_bgvs97r35l8ys1yvnqvrh0000gs/T/tmpamfb9nwv pod-processing-outputs/cornell_cornell_filtered-marc21.parquet\n",
      "SUCCESS: Created pod-processing-outputs/cornell_cornell_filtered-marc21.parquet with 6944453 cornell records (100.0% success rate)\n",
      "  Note: FLDR (leader) field is included by default in marctable output\n",
      "Running marctable: marctable parquet /var/folders/x_/_g_bgvs97r35l8ys1yvnqvrh0000gs/T/tmplvru25c2 pod-processing-outputs/dartmouth_dartmouth_filtered-marc21.parquet\n",
      "SUCCESS: Created pod-processing-outputs/dartmouth_dartmouth_filtered-marc21.parquet with 3855421 dartmouth records (100.0% success rate)\n",
      "  Note: FLDR (leader) field is included by default in marctable output\n"
     ]
    }
   ],
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import re\n",
    "from pymarc import Record, MARCReader\n",
    "\n",
    "# Setup logging for MARC conversion\n",
    "log_dir = 'pod-processing-outputs/logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, 'marc2parquet.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_institution_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extract institution name from filename patterns\"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    \n",
    "    # For files from pod-processing-outputs/final/ like \"harvard_updates-001.mrc\"\n",
    "    if '_' in base:\n",
    "        return base.split('_')[0]\n",
    "    \n",
    "    # Pattern: institution-date-descriptor-format.ext\n",
    "    match = re.match(r'^([a-z]+)-[\\d\\-]+-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Pattern: institution-descriptor.ext\n",
    "    match = re.match(r'^([a-z]+)-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Default: use the first word\n",
    "    return base.split('-')[0].split('.')[0]\n",
    "\n",
    "def safe_read_marc_file_with_recovery(file_path: str, temp_output: str) -> Tuple[int, Dict]:\n",
    "    \"\"\"Read MARC file with maximum error recovery and minimal validation\"\"\"\n",
    "    total_records = 0\n",
    "    valid_records = 0\n",
    "    report = {\"total_attempted\": 0, \"parsed\": 0, \"errors\": 0}\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file, open(temp_output, 'wb') as outfile:\n",
    "            reader = MARCReader(file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            \n",
    "            for record_number, record in enumerate(reader, 1):\n",
    "                total_records += 1\n",
    "                \n",
    "                if record is None:\n",
    "                    report[\"errors\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    outfile.write(record.as_marc())\n",
    "                    valid_records += 1\n",
    "                except Exception as e:\n",
    "                    report[\"errors\"] += 1\n",
    "                    logger.warning(f\"Error writing record {record_number}: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "        \n",
    "    report[\"total_attempted\"] = total_records\n",
    "    report[\"parsed\"] = valid_records\n",
    "    \n",
    "    if total_records > 0:\n",
    "        report[\"success_rate\"] = (valid_records / total_records) * 100\n",
    "    else:\n",
    "        report[\"success_rate\"] = 0\n",
    "        \n",
    "    return valid_records, report\n",
    "\n",
    "def get_institution_specific_marc_files() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get all institution-specific MARC files from processed outputs\"\"\"\n",
    "    institution_file_pairs = []\n",
    "    \n",
    "    # PRIMARY: Look for processed MARC files in the final output directory\n",
    "    final_dir = 'pod-processing-outputs/final'\n",
    "    \n",
    "    if os.path.exists(final_dir):\n",
    "        # Get all .mrc files from the final directory\n",
    "        final_marc_files = glob.glob(os.path.join(final_dir, '*.mrc'))\n",
    "        \n",
    "        for file in final_marc_files:\n",
    "            # Extract institution from filename (e.g., \"harvard_updates-001.mrc\" -> \"harvard\")\n",
    "            institution = extract_institution_from_filename(file)\n",
    "            institution_file_pairs.append((institution, file))\n",
    "            \n",
    "        print(f\"Found {len(final_marc_files)} processed MARC files in {final_dir}\")\n",
    "    \n",
    "    # SECONDARY: Check the export directory for the latest export package\n",
    "    export_dir = 'pod-processing-outputs/export'\n",
    "    if os.path.exists(export_dir) and not institution_file_pairs:\n",
    "        # Find the most recent export package\n",
    "        export_packages = glob.glob(os.path.join(export_dir, 'marc_export_*'))\n",
    "        if export_packages:\n",
    "            latest_export = sorted(export_packages)[-1]  # Get most recent by timestamp\n",
    "            export_marc_files = glob.glob(os.path.join(latest_export, '*.mrc'))\n",
    "            \n",
    "            for file in export_marc_files:\n",
    "                # Skip non-MARC files\n",
    "                if file.endswith('.txt'):\n",
    "                    continue\n",
    "                institution = extract_institution_from_filename(file)\n",
    "                institution_file_pairs.append((institution, file))\n",
    "            \n",
    "            print(f\"Found {len(export_marc_files)} MARC files in latest export: {latest_export}\")\n",
    "    \n",
    "    # FALLBACK: If no processed files found, check for raw files\n",
    "    if not institution_file_pairs:\n",
    "        print(\"No processed files found in pod-processing-outputs/final or export directories\")\n",
    "        print(\"Falling back to raw MARC files in pod_*/file directories\")\n",
    "        \n",
    "        # Look for marc files in institution directories\n",
    "        institution_dirs = glob.glob(\"pod_*/file\")\n",
    "        \n",
    "        for institution_dir in institution_dirs:\n",
    "            institution = institution_dir.split('/')[0].replace('pod_', '')\n",
    "            \n",
    "            # Look for .mrc files only (no XML)\n",
    "            mrc_files = glob.glob(f\"{institution_dir}/**/*.mrc\", recursive=True)\n",
    "            for file in mrc_files:\n",
    "                institution_file_pairs.append((institution, file))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_pairs = list(set(institution_file_pairs))\n",
    "    unique_pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    print(f\"\\nTotal institution-specific MARC files to process: {len(unique_pairs)}\")\n",
    "    for institution, file in unique_pairs:\n",
    "        print(f\"  - {institution}: {file}\")\n",
    "    \n",
    "    return unique_pairs\n",
    "\n",
    "def process_file_with_recovery(file: str, institution: str) -> bool:\n",
    "    \"\"\"Process a MARC file with maximum error recovery\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('pod-processing-outputs', exist_ok=True)\n",
    "        \n",
    "        # Create a temporary file for processing\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join('pod-processing-outputs', \n",
    "                                   f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "        \n",
    "        # Process MARC file\n",
    "        written_count, report = safe_read_marc_file_with_recovery(file, temp_file)\n",
    "        \n",
    "        # Proceed if we have at least some records\n",
    "        if written_count == 0:\n",
    "            error_msg = f\"No records could be processed from {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Run marctable command - FLDR is included by default\n",
    "        marctable_cmd = f'marctable parquet {temp_file} {output_file}'\n",
    "        marctable_msg = f\"Running marctable: {marctable_cmd}\"\n",
    "        logger.info(marctable_msg)\n",
    "        print(marctable_msg)\n",
    "        exit_status = os.system(marctable_cmd)\n",
    "        \n",
    "        if exit_status != 0:\n",
    "            error_msg = f\"marctable command failed for {institution} file {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        else:\n",
    "            success_msg = f\"SUCCESS: Created {output_file} with {written_count} {institution} records ({report.get('success_rate', 0):.1f}% success rate)\"\n",
    "            logger.info(success_msg)\n",
    "            print(success_msg)\n",
    "            print(f\"  Note: FLDR (leader) field is included by default in marctable output\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error processing {institution} file {file}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if 'temp_file' in locals() and temp_file and os.path.exists(temp_file):\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error for {temp_file}: {str(e)}\")\n",
    "\n",
    "def marc2parquet_institution_specific(force_reprocess=False):\n",
    "    \"\"\"\n",
    "    Convert institution-specific MARC to Parquet with maximum error recovery\n",
    "    \n",
    "    Args:\n",
    "        force_reprocess: If True, reprocess even if parquet files already exist\n",
    "    \"\"\"\n",
    "    # Check if previous processing has been done\n",
    "    if not os.path.exists('pod-processing-outputs/final'):\n",
    "        print(\"WARNING: No processed files found in pod-processing-outputs/final/\")\n",
    "        print(\"Consider running ivyplus-updated-marc-pyspark.ipynb first for better results\")\n",
    "    \n",
    "    institution_file_pairs = get_institution_specific_marc_files()\n",
    "    \n",
    "    if not institution_file_pairs:\n",
    "        error_msg = \"No institution-specific MARC files found to process\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "    \n",
    "    results = []\n",
    "    institution_summary = {}\n",
    "    \n",
    "    for institution, file in institution_file_pairs:\n",
    "        if institution not in institution_summary:\n",
    "            institution_summary[institution] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "        \n",
    "        institution_summary[institution][\"total\"] += 1\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join('pod-processing-outputs', \n",
    "                                   f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "        # Skip if already processed unless force_reprocess is True\n",
    "        if not force_reprocess and os.path.exists(output_file):\n",
    "            skip_msg = f\"Skipping already processed {institution} file {file}\"\n",
    "            logger.info(skip_msg)\n",
    "            print(skip_msg)\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "            results.append(True)\n",
    "            continue\n",
    "            \n",
    "        result = process_file_with_recovery(file, institution)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result:\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "        else:\n",
    "            institution_summary[institution][\"failed\"] += 1\n",
    "    \n",
    "    # Print summary by institution\n",
    "    print(\"\\n=== Institution Processing Summary ===\")\n",
    "    for institution, stats in institution_summary.items():\n",
    "        print(f\"{institution.upper()}: Processed {stats['total']} files - {stats['success']} succeeded, {stats['failed']} failed\")\n",
    "    \n",
    "    # Overall success rate\n",
    "    total_success = sum(results)\n",
    "    total_files = len(results)\n",
    "    if total_files > 0:\n",
    "        print(f\"\\nOverall: Successfully processed {total_success} of {total_files} files ({total_success/total_files*100:.1f}%)\")\n",
    "        return total_success == total_files\n",
    "    else:\n",
    "        print(\"\\nNo files were processed\")\n",
    "        return False\n",
    "\n",
    "# Check if conversion is needed or if we can skip directly to processing\n",
    "print(\"Checking for existing parquet files...\")\n",
    "existing_parquet = glob.glob(\"pod-processing-outputs/*_marc21.parquet\")\n",
    "if existing_parquet:\n",
    "    print(f\"Found {len(existing_parquet)} existing parquet files\")\n",
    "    print(\"You can skip to the next cell unless you want to reprocess\")\n",
    "else:\n",
    "    print(\"No parquet files found. Running conversion...\")\n",
    "    marc2parquet_institution_specific()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing with Optimized Functions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, size, array_contains, collect_set\n",
    "from functools import reduce\n",
    "\n",
    "# Find all institution parquet files in pod-processing-outputs directory\n",
    "import glob\n",
    "import os\n",
    "\n",
    "parquet_files = glob.glob(\"pod-processing-outputs/*_marc21.parquet\")\n",
    "\n",
    "print(f\"Found {len(parquet_files)} institution-specific parquet files to process\")\n",
    "\n",
    "# Check if we have any files to process\n",
    "if not parquet_files:\n",
    "    print(\"ERROR: No parquet files found!\")\n",
    "    print(\"Please run the previous cell to convert MARC files to Parquet format.\")\n",
    "    raise ValueError(\"No parquet files found in pod-processing-outputs/\")\n",
    "\n",
    "print(\"Files to process:\")\n",
    "for f in parquet_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Process each institution's data with optimized functions\n",
    "all_dfs = []\n",
    "total_records_by_institution = {}\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    # Extract institution name from filename\n",
    "    filename = os.path.basename(parquet_file)\n",
    "    institution = filename.split('_')[0]\n",
    "    \n",
    "    print(f\"\\nProcessing {institution} from {parquet_file}\")\n",
    "    \n",
    "    # Read and process the parquet file\n",
    "    df = spark.read.parquet(parquet_file)\n",
    "    processed_df = process_institution_optimized(df, institution)\n",
    "    \n",
    "    # Get statistics without caching individual DataFrames\n",
    "    total_records = processed_df.count()\n",
    "    total_records_by_institution[institution] = total_records\n",
    "    \n",
    "    if total_records > 0:\n",
    "        # Select only needed columns and add to list\n",
    "        selected_df = processed_df.select(\n",
    "            \"F001\", \"source\", \"match_key\", \"id_list\", \n",
    "            \"is_valid_match_key\", \"match_key_message\"\n",
    "        )\n",
    "        all_dfs.append(selected_df)\n",
    "        \n",
    "        # Quick validation check (sample-based for efficiency)\n",
    "        sample_validation = processed_df.select(\"is_valid_match_key\").sample(\n",
    "            fraction=min(10000/total_records, 1.0), seed=42\n",
    "        ).groupBy(\"is_valid_match_key\").count().collect()\n",
    "        \n",
    "        valid_sample = next((r[\"count\"] for r in sample_validation if r[\"is_valid_match_key\"]), 0)\n",
    "        total_sample = sum(r[\"count\"] for r in sample_validation)\n",
    "        \n",
    "        if total_sample > 0:\n",
    "            est_valid_pct = (valid_sample / total_sample) * 100\n",
    "            print(f\"  - {total_records:,} records processed\")\n",
    "            print(f\"  - Estimated {est_valid_pct:.1f}% valid match keys (based on sample)\")\n",
    "    else:\n",
    "        print(f\"  - WARNING: No records found in {parquet_file}\")\n",
    "\n",
    "# Check if we have any DataFrames to union\n",
    "if not all_dfs:\n",
    "    print(\"ERROR: No DataFrames to combine!\")\n",
    "    raise ValueError(\"No valid institution data found to process\")\n",
    "\n",
    "# Efficient union of all DataFrames\n",
    "print(\"\\nCombining all institution data into a single DataFrame...\")\n",
    "all_df = reduce(lambda df1, df2: df1.union(df2), all_dfs)\n",
    "\n",
    "# Cache the combined DataFrame (this is where caching makes most sense)\n",
    "all_df.cache()\n",
    "\n",
    "# Force computation and get actual count\n",
    "print(\"Materializing combined DataFrame...\")\n",
    "total_keys = all_df.count()\n",
    "print(f\"Total records across all institutions: {total_keys:,}\")\n",
    "\n",
    "# Define the matching key array - COMBINED APPROACH\n",
    "print(\"\\nüîÑ Using COMBINED matching approach: ISBN/LCCN AND match_key\")\n",
    "all_df = all_df.withColumn(\"key_array\",\n",
    "    F.array_distinct(\n",
    "        F.concat(\n",
    "            F.col(\"id_list\"),  # ISBN/LCCN identifiers\n",
    "            F.array(F.col(\"match_key\"))  # Always include match_key\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Efficient validation counting in a single pass\n",
    "if total_keys > 0:\n",
    "    print(\"\\nAnalyzing match key quality...\")\n",
    "    validation_stats = all_df.groupBy(\"is_valid_match_key\").count().collect()\n",
    "    stats_dict = {row[\"is_valid_match_key\"]: row[\"count\"] for row in validation_stats}\n",
    "    \n",
    "    valid_keys = stats_dict.get(True, 0)\n",
    "    invalid_keys = stats_dict.get(False, 0)\n",
    "    \n",
    "    print(f\"\\nMatch key validation results:\")\n",
    "    print(f\"  ‚Ä¢ Valid match keys: {valid_keys:,} ({valid_keys/total_keys*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Invalid match keys: {invalid_keys:,} ({invalid_keys/total_keys*100:.1f}%)\")\n",
    "    \n",
    "    if invalid_keys > 0:\n",
    "        print(\"\\nInvalid match key reasons:\")\n",
    "        all_df.filter(col(\"is_valid_match_key\") == False) \\\n",
    "            .groupBy(\"match_key_message\") \\\n",
    "            .count() \\\n",
    "            .orderBy(col(\"count\").desc()) \\\n",
    "            .show(10, False)\n",
    "\n",
    "# Explode the key_array\n",
    "all_df_exploded = all_df.withColumn(\"key\", explode(\"key_array\"))\n",
    "\n",
    "# Print key array statistics\n",
    "print(\"\\nüìä Key array statistics:\")\n",
    "key_array_sizes = all_df.select(\n",
    "    F.size(\"key_array\").alias(\"num_keys\")\n",
    ").groupBy(\"num_keys\").count().orderBy(\"num_keys\")\n",
    "print(\"Distribution of number of keys per record:\")\n",
    "key_array_sizes.show()\n",
    "\n",
    "# Unpersist the cached combined DataFrame to free memory\n",
    "all_df.unpersist()\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete. Ready for uniqueness analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness Analysis and Overlap Detection\n",
    "from pyspark.sql.functions import collect_set, array_contains, size, col\n",
    "\n",
    "# Group by key and collect sources where that key appears\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# Broadcast the grouped DataFrame for more efficient joins\n",
    "grouped_broadcast = F.broadcast(grouped)\n",
    "\n",
    "# Find Penn records that exist in OTHER libraries\n",
    "# A Penn record is NOT unique if it exists in ANY other library\n",
    "penn_keys_in_other_libs = grouped_broadcast.filter(\n",
    "    (array_contains(col(\"sources\"), \"penn\")) & \n",
    "    (F.size(col(\"sources\")) > 1)  # Penn + at least one other library\n",
    ").select(\"key\")\n",
    "\n",
    "# Get Penn records that are truly unique to Penn\n",
    "# Start with all Penn records\n",
    "all_penn_exploded = all_df_exploded.filter(col(\"source\") == \"penn\")\n",
    "\n",
    "# Anti-join to remove Penn records found in other libraries\n",
    "unique_penn_exploded = all_penn_exploded.join(\n",
    "    penn_keys_in_other_libs,  # No need to re-broadcast\n",
    "    on=\"key\", \n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Deduplicate by Penn's F001 (not match_key) to get unique Penn records\n",
    "unique_penn = unique_penn_exploded.drop(\"key\").dropDuplicates([\"F001\"])\n",
    "\n",
    "# Cache the unique Penn records for better performance\n",
    "unique_penn.cache()\n",
    "\n",
    "# Calculate statistics efficiently\n",
    "unique_penn_count = unique_penn.count()  # Force cache materialization\n",
    "\n",
    "# Get total Penn records from the deduplicated exploded DataFrame\n",
    "total_penn = all_penn_exploded.select(\"F001\").distinct().count()\n",
    "\n",
    "print(f\"\\n=== Analysis Results ===\")\n",
    "print(f\"Total Penn records: {total_penn:,}\")\n",
    "print(f\"Unique Penn records: {unique_penn_count:,}\")\n",
    "\n",
    "# Add robust checking for division by zero\n",
    "if total_penn > 0:\n",
    "    print(f\"Uniqueness rate: {unique_penn_count/total_penn*100:.1f}%\")\n",
    "    print(f\"Overlap rate: {(total_penn - unique_penn_count)/total_penn*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Uniqueness rate: N/A (no Penn records found)\")\n",
    "\n",
    "# For analysis, let's also see overlap statistics\n",
    "print(\"\\n=== Overlap Analysis ===\")\n",
    "\n",
    "# More efficient: get Penn overlap stats without re-filtering\n",
    "penn_keys = grouped.filter(array_contains(col(\"sources\"), \"penn\")).cache()\n",
    "\n",
    "penn_overlap_stats = penn_keys \\\n",
    "    .withColumn(\"num_libraries\", F.size(col(\"sources\"))) \\\n",
    "    .groupBy(\"num_libraries\").count() \\\n",
    "    .orderBy(\"num_libraries\")\n",
    "\n",
    "print(\"Distribution of Penn records by number of libraries holding them:\")\n",
    "penn_overlap_stats.show()\n",
    "\n",
    "# Save results with consistent paths using pod-processing-outputs directory\n",
    "output_dir = \"pod-processing-outputs\"\n",
    "\n",
    "# Save unique Penn records\n",
    "unique_penn.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "\n",
    "# Save detailed overlap information for analysis\n",
    "# Note: Using cached penn_keys for efficiency\n",
    "penn_with_overlap_info = all_penn_exploded.join(\n",
    "    penn_keys.select(\"key\", \"sources\", F.size(\"sources\").alias(\"num_libraries\")),\n",
    "    on=\"key\",\n",
    "    how=\"left\"\n",
    ").drop(\"key\")\n",
    "\n",
    "penn_with_overlap_info.write.mode(\"overwrite\").parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "\n",
    "# Save validation statistics for analysis\n",
    "validation_stats = all_df.select(\"F001\", \"match_key\", \"is_valid_match_key\", \"match_key_message\", \"id_list\") \\\n",
    "    .filter(col(\"source\") == \"penn\")\n",
    "\n",
    "validation_stats.write.mode(\"overwrite\").parquet(f\"{output_dir}/match_key_validation_stats.parquet\")\n",
    "\n",
    "# Unpersist cached DataFrames to free memory\n",
    "penn_keys.unpersist()\n",
    "unique_penn.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source Validation (Updated: July 2025)\n",
    "# Validates Penn MARC data sources and ensures current data is used\n",
    "# Requires explicit confirmation for legacy data usage\n",
    "\n",
    "# Use Leader field FLDR to make a print set from unique penn and non-print\n",
    "from pyspark.sql.functions import col, substring, when, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Add this after the imports\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"pod-processing-outputs\"\n",
    "\n",
    "# Load the unique Penn dataset if not already loaded\n",
    "if 'unique_penn' not in locals() or unique_penn is None:\n",
    "    print(\"Loading unique Penn dataset...\")\n",
    "    unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "else:\n",
    "    print(\"Using existing unique_penn DataFrame\")\n",
    "\n",
    "# CRITICAL: Verify Penn data currency before processing\n",
    "def verify_penn_data_source(matching_files):\n",
    "    \"\"\"\n",
    "    Verify the Penn data source and warn if using outdated data\n",
    "    \"\"\"\n",
    "    if not matching_files:\n",
    "        return None\n",
    "        \n",
    "    selected_file = matching_files[0]\n",
    "    file_info = {\n",
    "        'path': selected_file,\n",
    "        'filename': os.path.basename(selected_file),\n",
    "        'is_legacy': 'penn-2022-07-20' in selected_file,\n",
    "        'is_processed': 'pod-processing-outputs' in selected_file\n",
    "    }\n",
    "    \n",
    "    # Extract date from filename if possible\n",
    "    date_pattern = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "    date_match = re.search(date_pattern, file_info['filename'])\n",
    "    if date_match:\n",
    "        file_info['data_date'] = date_match.group(1)\n",
    "    else:\n",
    "        file_info['data_date'] = 'unknown'\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Load full Penn records - prioritize most recent processed data\n",
    "penn_full_paths = [\n",
    "    # PRIMARY: Penn parquet files from current processing pipeline\n",
    "    \"pod-processing-outputs/penn_*updates*marc21.parquet\",\n",
    "    \n",
    "    # SECONDARY: Any Penn parquet files in processing outputs\n",
    "    \"pod-processing-outputs/penn_*.parquet\",\n",
    "    \n",
    "    # TERTIARY: Check for raw Penn parquet files (less preferred)\n",
    "    \"pod_penn/file/**/*.parquet\"\n",
    "    \n",
    "    # REMOVED: Legacy path - DO NOT USE unless absolutely necessary\n",
    "    # \"/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet\"\n",
    "]\n",
    "\n",
    "# Add data source verification\n",
    "penn_full = None\n",
    "selected_source = None\n",
    "\n",
    "print(\"\\n=== PENN DATA SOURCE VERIFICATION ===\")\n",
    "for path_pattern in penn_full_paths:\n",
    "    try:\n",
    "        matching_files = glob.glob(path_pattern, recursive=True)\n",
    "        if matching_files:\n",
    "            # Sort files by modification time to get most recent\n",
    "            matching_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "            \n",
    "            source_info = verify_penn_data_source(matching_files)\n",
    "            if source_info:\n",
    "                print(f\"\\nFound Penn records at: {source_info['path']}\")\n",
    "                print(f\"  - Source type: {'Processed updates' if source_info['is_processed'] else 'Raw data'}\")\n",
    "                print(f\"  - Data date: {source_info['data_date']}\")\n",
    "                \n",
    "                # Warn if data appears old\n",
    "                if source_info['data_date'] != 'unknown':\n",
    "                    try:\n",
    "                        data_date = datetime.strptime(source_info['data_date'], '%Y-%m-%d')\n",
    "                        days_old = (datetime.now() - data_date).days\n",
    "                        if days_old > 365:\n",
    "                            print(f\"  ‚ö†Ô∏è  WARNING: Data is {days_old} days old!\")\n",
    "                            print(f\"  ‚ö†Ô∏è  Results may not reflect current Penn holdings\")\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Load the data\n",
    "                penn_full = spark.read.parquet(source_info['path'])\n",
    "                selected_source = source_info\n",
    "                \n",
    "                # Verify record count and sample for currency check\n",
    "                record_count = penn_full.count()\n",
    "                print(f\"  - Total records: {record_count:,}\")\n",
    "                \n",
    "                # Sample check for recent cataloging activity\n",
    "                if 'F005' in penn_full.columns:\n",
    "                    recent_updates = penn_full.filter(\n",
    "                        col(\"F005\").rlike(\"202[4-5]\")\n",
    "                    ).count()\n",
    "                    recent_percentage = (recent_updates / record_count * 100) if record_count > 0 else 0\n",
    "                    print(f\"  - Recently updated records (2024-2025): {recent_updates:,} ({recent_percentage:.1f}%)\")\n",
    "                    \n",
    "                    if recent_percentage < 5:\n",
    "                        print(f\"  ‚ö†Ô∏è  WARNING: Only {recent_percentage:.1f}% of records updated recently\")\n",
    "                        print(f\"  ‚ö†Ô∏è  Data may be significantly outdated\")\n",
    "                \n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {path_pattern}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Final fallback with strong warning\n",
    "if penn_full is None:\n",
    "    print(\"\\n‚ö†Ô∏è  CRITICAL WARNING: No current Penn data found!\")\n",
    "    print(\"As a last resort, checking for legacy data...\")\n",
    "    \n",
    "    legacy_path = \"/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet\"\n",
    "    if os.path.exists(legacy_path):\n",
    "        response = input(\"\\nüö® Found 2022 Penn data. This is SEVERELY OUTDATED. Use anyway? (yes/no): \")\n",
    "        if response.lower() == 'yes':\n",
    "            penn_full = spark.read.parquet(legacy_path)\n",
    "            selected_source = {'is_legacy': True, 'filename': 'penn-2022-07-20-full-marc21.parquet'}\n",
    "            print(\"‚ö†Ô∏è  Using 2022 data - results will NOT reflect current Penn holdings!\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No Penn full MARC records found and user declined legacy data\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not find Penn full MARC records!\")\n",
    "        print(\"Please ensure Penn MARC data has been converted to Parquet format.\")\n",
    "        print(\"Run the previous cells to process MARC files first.\")\n",
    "        raise FileNotFoundError(\"Penn full MARC records not found\")\n",
    "\n",
    "print(\"\\n=== PROCEEDING WITH ANALYSIS ===\")\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"‚ö†Ô∏è  USING OUTDATED DATA - RESULTS MAY BE INACCURATE\")\n",
    "\n",
    "# OPTIMIZATION: Use join instead of SQL IN clause for better performance\n",
    "unique_penn_ids = unique_penn.select(\"F001\").distinct()\n",
    "unique_penn_full = penn_full.join(unique_penn_ids, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# OPTIMIZATION: Apply all transformations in a single chain\n",
    "unique_penn_with_material_type = (unique_penn_full\n",
    "    # Filter out 533 fields first\n",
    "    .filter(col(\"F533\").isNull())\n",
    "    # Add material type columns\n",
    "    .withColumn(\"record_type\", substring(col(\"FLDR\"), 7, 1))\n",
    "    .withColumn(\"bib_level\", substring(col(\"FLDR\"), 8, 1))\n",
    "    .withColumn(\"combined_type\", concat(col(\"record_type\"), col(\"bib_level\")))\n",
    "    .withColumn(\"material_category\", \n",
    "        when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"m\")), \"print_book\")\n",
    "        .when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"s\")), \"print_serial\")\n",
    "        .when((col(\"record_type\") == \"c\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_music\")\n",
    "        .when((col(\"record_type\") == \"e\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_maps\")\n",
    "        .when(col(\"record_type\") == \"m\", \"electronic_resource\")\n",
    "        .when(col(\"record_type\").isin(\"g\", \"k\"), \"visual_material\")\n",
    "        .when(col(\"record_type\") == \"i\", \"audio_material\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "    .withColumn(\"is_print\", \n",
    "        col(\"material_category\").isin(\"print_book\", \"print_serial\", \"print_music\", \"print_maps\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache before multiple operations\n",
    "unique_penn_with_material_type.cache()\n",
    "\n",
    "# OPTIMIZATION: Get all statistics in one pass\n",
    "print(\"\\n=== Material Type Distribution ===\")\n",
    "material_stats = unique_penn_with_material_type.groupBy(\"material_category\", \"is_print\").count().collect()\n",
    "\n",
    "# Process statistics\n",
    "material_counts_dict = {}\n",
    "print_count = 0\n",
    "non_print_count = 0\n",
    "\n",
    "for row in material_stats:\n",
    "    material_counts_dict[row[\"material_category\"]] = row[\"count\"]\n",
    "    if row[\"is_print\"]:\n",
    "        print_count += row[\"count\"]\n",
    "    else:\n",
    "        non_print_count += row[\"count\"]\n",
    "\n",
    "total_unique = print_count + non_print_count\n",
    "\n",
    "# Display material distribution\n",
    "for category, count in sorted(material_counts_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{category}: {count:,}\")\n",
    "\n",
    "# Filter for print materials only\n",
    "print_only_df = unique_penn_with_material_type.filter(col(\"is_print\") == True)\n",
    "\n",
    "# Add metadata if we have source information\n",
    "if selected_source:\n",
    "    print_only_df_with_metadata = print_only_df.withColumn(\n",
    "        \"processing_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    ).withColumn(\n",
    "        \"source_file\", lit(selected_source.get('filename', 'unknown'))\n",
    "    ).withColumn(\n",
    "        \"data_currency_warning\", \n",
    "        lit(\"OUTDATED - 2022 data\" if selected_source.get('is_legacy') else \"Current\")\n",
    "    )\n",
    "else:\n",
    "    print_only_df_with_metadata = print_only_df\n",
    "\n",
    "# Save datasets\n",
    "unique_penn_with_material_type.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn_full_no_533.parquet\")\n",
    "print_only_df_with_metadata.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\n=== Print Material Analysis ===\")\n",
    "print(f\"Total unique Penn records: {total_unique:,}\")\n",
    "\n",
    "if total_unique > 0:\n",
    "    print(f\"Print materials: {print_count:,} ({print_count/total_unique*100:.1f}%)\")\n",
    "    print(f\"Non-print materials: {non_print_count:,} ({non_print_count/total_unique*100:.1f}%)\")\n",
    "    \n",
    "    # Show print categories breakdown\n",
    "    print(\"\\n=== Print Material Categories ===\")\n",
    "    print_categories = [\"print_book\", \"print_serial\", \"print_music\", \"print_maps\"]\n",
    "    for category in print_categories:\n",
    "        if category in material_counts_dict:\n",
    "            count = material_counts_dict[category]\n",
    "            print(f\"{category}: {count:,} ({count/print_count*100:.1f}% of print materials)\")\n",
    "else:\n",
    "    print(\"No unique Penn records found to analyze\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "unique_penn_with_material_type.unpersist()\n",
    "\n",
    "# Final warning if using outdated data\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üö® CRITICAL WARNING: Analysis completed using 2022 Penn data\")\n",
    "    print(\"üö® Results do NOT reflect current Penn holdings\")\n",
    "    print(\"üö® Recommended: Re-run with current Penn MARC export\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling and Final Analysis\n",
    "from pyspark.sql.functions import rand, col\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "# Define output directory if not already defined\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"pod-processing-outputs\"\n",
    "\n",
    "# Load print materials dataset if not already loaded\n",
    "if 'print_only_df' not in locals() or print_only_df is None:\n",
    "    print(\"Loading print materials dataset...\")\n",
    "    print_only_df_raw = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "    \n",
    "    # Check if metadata columns exist and drop them for sampling\n",
    "    metadata_cols = [\"processing_date\", \"source_file\", \"data_currency_warning\"]\n",
    "    existing_metadata_cols = [col for col in metadata_cols if col in print_only_df_raw.columns]\n",
    "    \n",
    "    if existing_metadata_cols:\n",
    "        print(f\"Dropping metadata columns: {existing_metadata_cols}\")\n",
    "        print_only_df = print_only_df_raw.drop(*existing_metadata_cols)\n",
    "    else:\n",
    "        print_only_df = print_only_df_raw\n",
    "else:\n",
    "    print(\"Using existing print_only_df DataFrame\")\n",
    "\n",
    "# Load or compute necessary statistics if not available\n",
    "if 'total_penn' not in locals() or 'unique_penn_count' not in locals():\n",
    "    print(\"Loading required statistics...\")\n",
    "    # Load from saved parquet files\n",
    "    if 'unique_penn' not in locals():\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "    unique_penn_count = unique_penn.count()\n",
    "    \n",
    "    # Load Penn overlap analysis to get total Penn records\n",
    "    penn_overlap = spark.read.parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "    total_penn = penn_overlap.select(\"F001\").distinct().count()\n",
    "\n",
    "# Compute print statistics if not available\n",
    "if 'print_count' not in locals() or 'material_counts_dict' not in locals():\n",
    "    print(\"Computing material type statistics...\")\n",
    "    # Check for material_category column\n",
    "    if 'material_category' not in print_only_df.columns:\n",
    "        print(\"ERROR: material_category column not found in print_only_df\")\n",
    "        raise ValueError(\"Missing required column: material_category\")\n",
    "    \n",
    "    material_stats = print_only_df.groupBy(\"material_category\").count().collect()\n",
    "    material_counts_dict = {row[\"material_category\"]: row[\"count\"] for row in material_stats}\n",
    "    print_count = sum(material_counts_dict.values())\n",
    "\n",
    "# Define sampling function with improved stratification\n",
    "def create_stratified_sample(df, strata_column, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with improved randomization.\n",
    "    Uses multiple passes to ensure representation of all strata.\n",
    "    \"\"\"\n",
    "    print(f\"Creating stratified sample based on {strata_column}...\")\n",
    "    \n",
    "    # Verify the strata column exists\n",
    "    if strata_column not in df.columns:\n",
    "        print(f\"ERROR: Column '{strata_column}' not found in DataFrame\")\n",
    "        print(f\"Available columns: {df.columns}\")\n",
    "        raise ValueError(f\"Missing required column: {strata_column}\")\n",
    "    \n",
    "    # Get counts by strata for weighting\n",
    "    strata_counts = df.groupBy(strata_column).count().collect()\n",
    "    total_records = df.count()\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"WARNING: No records to sample from!\")\n",
    "        return df\n",
    "    \n",
    "    strata_map = {row[strata_column]: row[\"count\"] for row in strata_counts}\n",
    "    print(f\"Strata distribution:\")\n",
    "    for strata, count in sorted(strata_map.items()):\n",
    "        print(f\"  - {strata}: {count:,} records ({count/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate proportional sample sizes with minimum threshold\n",
    "    min_per_strata = 5  # Ensure at least a few records from each stratum\n",
    "    sample_fractions = {}\n",
    "    \n",
    "    for strata, count in strata_map.items():\n",
    "        # Proportional sampling with minimum threshold\n",
    "        if count > 0:\n",
    "            # Calculate proportional share but ensure at least min_per_strata\n",
    "            prop_size = max(\n",
    "                min_per_strata,\n",
    "                int((count / total_records) * sample_size)\n",
    "            )\n",
    "            \n",
    "            # Don't sample more than we have\n",
    "            prop_size = min(prop_size, count)\n",
    "            \n",
    "            # Calculate fraction\n",
    "            sample_fractions[strata] = prop_size / count\n",
    "    \n",
    "    # First pass: Stratified sampling\n",
    "    sampled_df = df.sampleBy(strata_column, fractions=sample_fractions, seed=42)\n",
    "    \n",
    "    # Check if we need a second pass to reach target size\n",
    "    current_size = sampled_df.count()\n",
    "    print(f\"First pass sample size: {current_size}\")\n",
    "    \n",
    "    if current_size < sample_size and current_size < total_records:\n",
    "        # Second pass: Sample from under-represented strata\n",
    "        remaining = min(sample_size - current_size, total_records - current_size)\n",
    "        print(f\"Need {remaining} more records to reach target sample size\")\n",
    "        \n",
    "        # Get records not in first sample\n",
    "        sampled_ids = sampled_df.select(\"F001\").distinct()\n",
    "        remaining_df = df.join(sampled_ids, on=\"F001\", how=\"left_anti\")\n",
    "        \n",
    "        remaining_count = remaining_df.count()\n",
    "        if remaining_count > 0:\n",
    "            # Simple random sample from remaining records\n",
    "            additional_sample = remaining_df.orderBy(rand(seed=43)).limit(remaining)\n",
    "            \n",
    "            # Union the samples\n",
    "            sampled_df = sampled_df.union(additional_sample)\n",
    "            print(f\"Added {min(remaining, remaining_count)} additional records\")\n",
    "    \n",
    "    final_size = sampled_df.count()\n",
    "    print(f\"Final sample size: {final_size}\")\n",
    "    \n",
    "    # Check distribution in final sample\n",
    "    sample_distribution = sampled_df.groupBy(strata_column).count().collect()\n",
    "    print(f\"\\nSample distribution by {strata_column}:\")\n",
    "    sample_dict = {row[strata_column]: row[\"count\"] for row in sample_distribution}\n",
    "    \n",
    "    for strata_val in sorted(strata_map.keys()):\n",
    "        original_count = strata_map.get(strata_val, 0)\n",
    "        sample_count = sample_dict.get(strata_val, 0)\n",
    "        if original_count > 0 and final_size > 0:\n",
    "            print(f\"  - {strata_val}: {sample_count} ({sample_count/final_size*100:.2f}% of sample vs {original_count/total_records*100:.2f}% of population)\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Create a stratified sample by material category\n",
    "sample_df = create_stratified_sample(print_only_df, \"material_category\", sample_size=1000)\n",
    "\n",
    "# Cache the sample for better performance\n",
    "sample_df.cache()\n",
    "\n",
    "# Save the sample for API validation\n",
    "sample_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/statistical_sample_for_api_no_hsp.parquet\")\n",
    "\n",
    "# Convert to CSV for easier human review\n",
    "# Select key fields for the CSV\n",
    "sample_for_csv = sample_df.select(\n",
    "    \"F001\", \"F020\", \"F010\", \"F245\", \"F250\", \"F260\", \"material_category\"\n",
    ")\n",
    "\n",
    "# Save as CSV (single file for easier review)\n",
    "sample_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/statistical_sample_for_api_no_hsp.csv\")\n",
    "\n",
    "# Generate final summary statistics in JSON format\n",
    "summary_stats = {\n",
    "    \"processing_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_penn_records\": int(total_penn),\n",
    "    \"unique_penn_records\": int(unique_penn_count),\n",
    "    \"uniqueness_rate\": float(unique_penn_count/total_penn) if total_penn > 0 else 0.0,\n",
    "    \"print_materials\": int(print_count),\n",
    "    \"print_materials_percentage\": float(print_count/unique_penn_count) if unique_penn_count > 0 else 0.0,\n",
    "    \"sample_size\": int(sample_df.count()),\n",
    "    \"material_categories\": {}\n",
    "}\n",
    "\n",
    "# Add material categories to summary\n",
    "for category, count in sorted(material_counts_dict.items()):\n",
    "    summary_stats[\"material_categories\"][category] = {\n",
    "        \"count\": int(count),\n",
    "        \"percentage\": float(count/print_count*100) if print_count > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# Write summary to JSON file\n",
    "with open(f\"{output_dir}/sample_summary_no_hsp.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "# Unpersist the cached sample\n",
    "sample_df.unpersist()\n",
    "\n",
    "print(\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"Results saved to {output_dir}/\")\n",
    "print(\"\\nFinal outputs:\")\n",
    "print(f\"  - unique_penn.parquet: All unique Penn records\")\n",
    "print(f\"  - physical_books_no_533.parquet: Unique Penn physical books\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\")\n",
    "print(f\"  - sample_summary_no_hsp.json: Summary statistics\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(f\"  - Total Penn records: {summary_stats['total_penn_records']:,}\")\n",
    "print(f\"  - Unique Penn records: {summary_stats['unique_penn_records']:,}\")\n",
    "print(f\"  - Uniqueness rate: {summary_stats['uniqueness_rate']*100:.1f}%\")\n",
    "print(f\"  - Print materials: {summary_stats['print_materials']:,}\")\n",
    "print(f\"  - Print materials percentage: {summary_stats['print_materials_percentage']:.1f}%\")\n",
    "print(f\"  - Sample size: {summary_stats['sample_size']:,}\")\n",
    "\n",
    "# Display material category breakdown\n",
    "if material_counts_dict:\n",
    "    print(\"\\nüìö Material Category Breakdown:\")\n",
    "    for category, info in sorted(summary_stats[\"material_categories\"].items()):\n",
    "        print(f\"  - {category}: {info['count']:,} ({info['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Cell - Run this to free all resources\n",
    "def cleanup_spark_resources():\n",
    "    \"\"\"Clean up all cached DataFrames and temporary views\"\"\"\n",
    "    try:\n",
    "        # Get all cached DataFrames\n",
    "        for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "            rdd.unpersist()\n",
    "        \n",
    "        # Drop all temporary views\n",
    "        for view in spark.catalog.listTables():\n",
    "            if view.isTemporary:\n",
    "                spark.catalog.dropTempView(view.name)\n",
    "        \n",
    "        print(\"‚úÖ All Spark resources cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
    "\n",
    "cleanup_spark_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
