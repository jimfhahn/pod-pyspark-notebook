{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivy Plus MARC Analysis\n",
    "\n",
    "This notebook processes MARC data from Ivy Plus libraries to identify unique items held by Penn that are not held by other institutions in the consortium.\n",
    "\n",
    "## Enhanced Normalization and Matching\n",
    "\n",
    "The matching process has been improved with multiple levels of matching and enhanced field extraction:\n",
    "\n",
    "### 1. **Multi-Level Matching Strategy**\n",
    "   - **Strict Match Keys**: Precise title, edition, and year matching\n",
    "   - **Fuzzy Match Keys**: Broader matching with aggressive normalization for catching variations\n",
    "\n",
    "### 2. **Enhanced Identifier Extraction**\n",
    "   - **OCLC Numbers**: Handles all variants (ocm, ocn, on prefixes) and leading zeros\n",
    "   - **ISBN Core**: Extracts the core ISBN for matching different formats of the same work\n",
    "   - **Publication Year**: Now checks both F260 and F264 fields (many modern records use F264)\n",
    "   - **LCCN**: Standardized to handle different formats and prefixes\n",
    "\n",
    "### 3. **Special Handling**\n",
    "   - **Multi-Volume Detection**: Identifies and properly handles multi-volume sets to prevent false positives\n",
    "   - **Smart Title Normalization**: Preserves important distinctions while removing true noise\n",
    "\n",
    "### 4. **Match Key Validation**\n",
    "   - Each match key is validated for quality to detect potential issues\n",
    "   - Short or generic match keys are flagged\n",
    "   - Match key quality metrics are saved for analysis\n",
    "   - Distribution statistics for different match types\n",
    "\n",
    "### 5. **Field Selection**\n",
    "   - Leader (FLDR) is included for record type identification\n",
    "   - Core bibliographic fields (F001, F010, F020, F245, F250, F260, F264, F035) are used\n",
    "   - F264 added for modern publication data\n",
    "   - F035 for OCLC number extraction\n",
    "\n",
    "This approach provides:\n",
    "-  multiple match levels\n",
    "-  enhanced OCLC and ISBN extraction\n",
    "-  multi-volume detection\n",
    "-  improved modern record support with F264 field processing\n",
    "\n",
    "## Initial Load - Institution-specific Processing\n",
    "Converts MARC to Parquet format for faster processing, maintaining institution-specific separation. This step ensures that each institution's MARC files are converted to separate Parquet files for consistent downstream processing.\n",
    "\n",
    "The conversion includes the leader field (FLDR) for each record. The leader contains important information about the record structure, material type, and bibliographic level.\n",
    "\n",
    "## Subsequent Runs\n",
    "If you have already run the notebook, you may rely on caches and append for new data if available and necessary.\n",
    "\n",
    "\n",
    "## HIGH MEMORY REQUIREMENT\n",
    "\n",
    "**This notebook is configured for a high-RAM server environment with the following specifications:**\n",
    "\n",
    "- **260GB driver memory allocation** (requires ~300GB total system RAM)\n",
    "- **12 cores** for parallel processing\n",
    "- Optimized for a **Linode 300GB server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "print(\"Python executable:\", sys.executable)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "print(\"PYSPARK_PYTHON:\", os.environ[\"PYSPARK_PYTHON\"])\n",
    "print(\"PYSPARK_DRIVER_PYTHON:\", os.environ[\"PYSPARK_DRIVER_PYTHON\"])\n",
    "!{sys.executable} -m pip install pymarc poetry fuzzywuzzy python-Levenshtein langdetect pyspark marctable==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for your PySpark server\n",
    "# Update these paths to match your server's directory structure\n",
    "input_dir = \"/home/jovyan/work/July-2025-PODParquet/initial_parquet\"  # Where your parquet files are located\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"  # Where to save the results\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {input_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Clean up any existing Spark sessions\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        time.sleep(2)  # Give it time to clean up\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear environment variables that might conflict\n",
    "for key in list(os.environ.keys()):\n",
    "    if 'SPARK' in key or 'JAVA' in key or 'PYSPARK' in key:\n",
    "        del os.environ[key]\n",
    "\n",
    "# Set JAVA_HOME explicitly\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "# Create temp directory\n",
    "os.makedirs('/tmp/spark-temp', exist_ok=True)\n",
    "\n",
    "# Create Spark session with all configurations at once\n",
    "# Since we know 200GB works from your test, we'll use that\n",
    "print(\"Creating Spark session with full configuration...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PodProcessing-Stable\") \\\n",
    "    .master(\"local[12]\") \\\n",
    "    .config(\"spark.driver.memory\", \"260g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"200g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"30m\") \\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"5min\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"20g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session initialized with 200GB memory and optimized settings!\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Test it works\n",
    "print(\"\\nTesting Spark with a simple operation...\")\n",
    "test_df = spark.range(100).selectExpr(\"id\", \"id * 2 as doubled\")\n",
    "test_df.show(5)\n",
    "\n",
    "# Verify key configurations\n",
    "print(\"\\n📋 Key configurations:\")\n",
    "print(f\"  - Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  - Max result size: {spark.conf.get('spark.driver.maxResultSize')}\")\n",
    "print(f\"  - Memory fraction: {spark.conf.get('spark.memory.fraction')}\")\n",
    "print(f\"  - Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "print(\"\\n✅ Spark session ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL Functions - ENHANCED VERSION 2.0\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Helper function to handle fields that might be strings or arrays\n",
    "def handle_field_as_string(col_name):\n",
    "    \"\"\"\n",
    "    Safely extract string value whether the field is a string or array.\n",
    "    This version handles mixed types properly.\n",
    "    \"\"\"\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        F.when(\n",
    "            F.size(F.col(col_name)) >= 0,\n",
    "            F.col(col_name).getItem(0)\n",
    "        ).otherwise(\n",
    "            F.col(col_name)\n",
    "        )\n",
    "    ).cast(\"string\")\n",
    "\n",
    "def extract_oclc_number_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Extract OCLC numbers from F035 field with ALL common patterns\n",
    "    Handles ocm, ocn, on prefixes and leading zeros\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"oclc_number\",\n",
    "        F.when(F.col(\"F035\").isNotNull() & (F.size(F.col(\"F035\")) > 0),\n",
    "            F.regexp_extract(\n",
    "                F.concat_ws(\" \", F.col(\"F035\")),\n",
    "                \"\\\\(OCoLC\\\\)(?:ocm|ocn|on)?0*([0-9]+)\",  # Handles prefixes AND leading zeros\n",
    "                1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extract_publication_year_enhanced(df):\n",
    "    \"\"\"\n",
    "    NEW: Check BOTH F260 and F264 for publication year\n",
    "    Many newer records use F264 instead of F260\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"pub_year\",\n",
    "        F.coalesce(\n",
    "            # First try F260\n",
    "            F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F260\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "            ),\n",
    "            # Then try F264 if F260 doesn't exist or is empty\n",
    "            F.when(F.col(\"F264\").isNotNull() & (F.size(F.col(\"F264\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F264\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"pub_decade\",\n",
    "        F.when(F.col(\"pub_year\").isNotNull(),\n",
    "            F.concat(F.substring(F.col(\"pub_year\"), 1, 3), F.lit(\"0s\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def identify_multivolume(df):\n",
    "    \"\"\"\n",
    "    NEW: Detect multi-volume works for special handling\n",
    "    Prevents false uniqueness for sets where libraries hold different volumes\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_multivolume\",\n",
    "        F.col(\"F245\").rlike(\"(?i)(v\\\\.|vol\\\\.|volume|pt\\\\.|part|tome|band|book)\\\\s*[0-9IVX]\") |\n",
    "        F.col(\"F245\").rlike(\"(?i)\\\\[?[0-9]+(st|nd|rd|th)\\\\s+(v\\\\.|vol|edition)\")\n",
    "    ).withColumn(\"base_title_for_multivolume\",\n",
    "        F.when(F.col(\"is_multivolume\"),\n",
    "            # Strip volume indicators for matching\n",
    "            F.regexp_replace(\n",
    "                F.col(\"F245\"),\n",
    "                \"(?i)[,;:]?\\\\s*(v\\\\.|vol\\\\.|volume|pt\\\\.|part|book)\\\\s*[0-9IVX]+.*$\",\n",
    "                \"\"\n",
    "            )\n",
    "        ).otherwise(F.col(\"F245\"))\n",
    "    )\n",
    "\n",
    "def normalize_isbn_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Better ISBN normalization with core extraction\n",
    "    Handles both ISBN-10 and ISBN-13 for better work-level matching\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"normalized_isbn\",\n",
    "        # F020 is array\n",
    "        F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0),\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"F020\").getItem(0), \"([0-9X-]+)\", 1),\n",
    "                \"[^0-9X]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"isbn_core\",\n",
    "        # Extract the core ISBN (ignoring check digit and prefix)\n",
    "        F.when(F.length(F.col(\"normalized_isbn\")) == 10,\n",
    "            F.substring(F.col(\"normalized_isbn\"), 1, 9)  # ISBN-10 core\n",
    "        ).when(F.length(F.col(\"normalized_isbn\")) == 13,\n",
    "            F.substring(F.col(\"normalized_isbn\"), 4, 9)  # ISBN-13 core (skip 978/979 prefix)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_smart_title_key(df):\n",
    "    \"\"\"\n",
    "    NEW: Smarter title normalization that preserves important distinctions\n",
    "    Less aggressive than fuzzy matching but catches more variations\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"title_normalized\",\n",
    "        # Remove only truly noise elements, keep important structure\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.lower(F.trim(F.col(\"F245\"))),\n",
    "                \"^(the|a|an|le|la|los|las|el|die|der|das|den|det)\\\\s+\", \"\"\n",
    "            ),\n",
    "            \"[\\\\[\\\\]\\\\(\\\\)/]\", \"\"  # Remove only brackets and slashes, keep colons/semicolons\n",
    "        )\n",
    "    ).withColumn(\"title_first_significant\",\n",
    "        # First 5 significant words for better matching\n",
    "        F.array_join(\n",
    "            F.slice(\n",
    "                F.split(F.col(\"title_normalized\"), \"\\\\s+\"),\n",
    "                1, 5\n",
    "            ),\n",
    "            \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_match_key_spark_improved(df):\n",
    "    \"\"\"\n",
    "    IMPROVED: Create better match keys using enhanced functions\n",
    "    \"\"\"\n",
    "    # Apply all the enhanced transformations first\n",
    "    df = df.transform(extract_publication_year_enhanced)\n",
    "    df = df.transform(identify_multivolume)\n",
    "    df = df.transform(create_smart_title_key)\n",
    "    \n",
    "    return df.withColumn(\"match_key\", \n",
    "        F.concat_ws(\"_\",\n",
    "            # Use base title for multivolume works\n",
    "            F.when(F.col(\"is_multivolume\"),\n",
    "                F.regexp_replace(F.col(\"base_title_for_multivolume\"), \"[^a-z0-9\\\\s]\", \"\")\n",
    "            ).otherwise(\n",
    "                F.regexp_replace(F.col(\"title_normalized\"), \"[^a-z0-9\\\\s]\", \"\")\n",
    "            ),\n",
    "            \n",
    "            # Normalize edition (F250 is array)\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F250\").getItem(0)), \n",
    "                    \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed|edition)\", \"$1 ed\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Use enhanced year extraction\n",
    "            F.coalesce(F.col(\"pub_year\"), F.lit(\"\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Redirect old function to enhanced version for backward compatibility\n",
    "def extract_oclc_number(df):\n",
    "    \"\"\"\n",
    "    Redirect to enhanced version\n",
    "    \"\"\"\n",
    "    return extract_oclc_number_enhanced(df)\n",
    "\n",
    "# Keep the original create_match_key_spark for backward compatibility\n",
    "def create_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Create match keys - now uses improved version\n",
    "    \"\"\"\n",
    "    return create_match_key_spark_improved(df)\n",
    "\n",
    "def create_fuzzy_match_key(df):\n",
    "    \"\"\"\n",
    "    Create FUZZY match keys for broader matching (catches more duplicates)\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"fuzzy_match_key\",\n",
    "        F.concat_ws(\"_\",\n",
    "            # More aggressive title normalization - remove ALL non-alphanumeric\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.trim(F.col(\"F245\"))),\n",
    "                        \"^(the|a|an|le|la|el|los|las|die|der|das|den|det)\\\\s+\", \"\"\n",
    "                    ),\n",
    "                    \"[^a-z0-9]\", \"\"  # Remove ALL punctuation and spaces\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Just extract edition number, ignore format\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F250\").getItem(0), \"(\\\\d+)\", 1)\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Year range (decade) instead of exact year\n",
    "            F.when(F.col(\"pub_year\").isNotNull(),\n",
    "                F.col(\"pub_decade\")\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_work_level_key(df):\n",
    "    \"\"\"\n",
    "    Create work-level match key (title + author only)\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"work_key\",\n",
    "        F.concat_ws(\"_\",\n",
    "            # Normalized title only\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F245\")),\n",
    "                    \"[^a-z0-9]\", \"\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Add author if available (F100 for personal, F110 for corporate)\n",
    "            F.when(F.col(\"F100\").isNotNull(),\n",
    "                F.regexp_replace(F.lower(F.col(\"F100\")), \"[^a-z]\", \"\")\n",
    "            ).when(F.col(\"F110\").isNotNull(),\n",
    "                F.regexp_replace(F.lower(F.col(\"F110\")), \"[^a-z]\", \"\")\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def normalize_isbn_for_matching(df):\n",
    "    \"\"\"\n",
    "    Enhanced ISBN normalization - redirects to enhanced version\n",
    "    \"\"\"\n",
    "    return normalize_isbn_enhanced(df)\n",
    "\n",
    "def normalize_ids_spark(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Normalize ISBN and LCCN using improved functions\n",
    "    \"\"\"\n",
    "    return df.transform(normalize_isbn_enhanced) \\\n",
    "        .withColumn(\"normalized_lccn\", \n",
    "            F.when(F.col(\"F010\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.trim(F.col(\"F010\")),\n",
    "                    \"[^a-zA-Z0-9-]\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "def add_id_list_spark_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Create comprehensive id_list including ISBN core\n",
    "    FIXED: Use concat to properly combine arrays\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"id_list\",\n",
    "        F.array_remove(\n",
    "            F.array_distinct(\n",
    "                F.concat(\n",
    "                    # Standard identifiers\n",
    "                    F.when(F.col(\"normalized_isbn\").isNotNull() & (F.col(\"normalized_isbn\") != \"\"), \n",
    "                        F.array(F.col(\"normalized_isbn\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"isbn_core\").isNotNull() & (F.col(\"isbn_core\") != \"\"), \n",
    "                        F.array(F.col(\"isbn_core\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"normalized_lccn\").isNotNull() & (F.col(\"normalized_lccn\") != \"\"), \n",
    "                        F.array(F.col(\"normalized_lccn\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"oclc_number\").isNotNull() & (F.col(\"oclc_number\") != \"\"), \n",
    "                        F.array(F.col(\"oclc_number\"))).otherwise(F.array()),\n",
    "                    # Match keys\n",
    "                    F.when(F.col(\"match_key\").isNotNull() & (F.col(\"match_key\") != \"\"), \n",
    "                        F.array(F.col(\"match_key\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"fuzzy_match_key\").isNotNull() & (F.col(\"fuzzy_match_key\") != \"\"), \n",
    "                        F.array(F.col(\"fuzzy_match_key\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"work_key\").isNotNull() & (F.col(\"work_key\") != \"\"), \n",
    "                        F.array(F.col(\"work_key\"))).otherwise(F.array())\n",
    "                )\n",
    "            ),\n",
    "            \"\"  # Remove empty strings\n",
    "        )\n",
    "    )\n",
    "\n",
    "def validate_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Validate match keys using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_valid_match_key\",\n",
    "        (F.length(F.col(\"match_key\")) >= 5) &\n",
    "        (~F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"))\n",
    "    ).withColumn(\"match_key_message\",\n",
    "        F.when(F.length(F.col(\"match_key\")) < 5, \"Match key too short\")\n",
    "         .when(F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"), \"Generic match key\")\n",
    "         .otherwise(\"Valid match key\")\n",
    "    )\n",
    "\n",
    "def process_institution_optimized(df, institution_name):\n",
    "    \"\"\"\n",
    "    ENHANCED: Apply all enhanced optimizations to an institution's DataFrame\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"source\", F.lit(institution_name))\n",
    "        .transform(extract_oclc_number_enhanced)  # ENHANCED OCLC\n",
    "        .transform(extract_publication_year_enhanced)  # NEW: F264 support\n",
    "        .transform(identify_multivolume)  # NEW: Multi-volume detection\n",
    "        .transform(normalize_ids_spark)   # Enhanced with ISBN core\n",
    "        .transform(create_match_key_spark_improved)  # IMPROVED match key\n",
    "        .transform(create_fuzzy_match_key)  # Keep existing fuzzy\n",
    "        .transform(create_work_level_key)   # Keep existing work-level\n",
    "        .transform(add_id_list_spark_enhanced)  # Enhanced with ISBN core\n",
    "        .transform(validate_match_key_spark)\n",
    "    )\n",
    "\n",
    "print(\"✅ ENHANCED Spark SQL functions loaded - VERSION 2.0\")\n",
    "print(\"✅ Major improvements:\")\n",
    "print(\"  - OCLC extraction handles all variants (ocm, ocn, on prefixes + leading zeros)\")\n",
    "print(\"  - Publication year checks both F260 and F264\")\n",
    "print(\"  - Multi-volume work detection prevents false positives\")\n",
    "print(\"  - ISBN core extraction for better work-level matching\")\n",
    "print(\"  - Smarter title normalization preserves important distinctions\")\n",
    "print(\"  - Backward compatible with existing code\")\n",
    "print(\"✅ FIXED: id_list generation now properly uses F.concat() to combine arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import re\n",
    "from pymarc import Record, MARCReader\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# First, let's verify marctable installation\n",
    "print(\"=== Checking marctable installation ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if marctable can be imported\n",
    "try:\n",
    "    import marctable\n",
    "    print(f\"✅ marctable module can be imported\")\n",
    "    print(f\"   Module location: {marctable.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Cannot import marctable: {e}\")\n",
    "\n",
    "# Check if marctable command is available in PATH\n",
    "try:\n",
    "    result = subprocess.run(['which', 'marctable'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ marctable command found at: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(\"❌ marctable command not found in PATH\")\n",
    "        \n",
    "        # Try to find it in common locations\n",
    "        possible_paths = [\n",
    "            os.path.join(sys.prefix, 'bin', 'marctable'),\n",
    "            os.path.join(os.path.expanduser('~'), '.local', 'bin', 'marctable'),\n",
    "            '/usr/local/bin/marctable',\n",
    "            '/opt/conda/bin/marctable'\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"   Found marctable at: {path}\")\n",
    "                # Add to PATH for this session\n",
    "                os.environ['PATH'] = os.path.dirname(path) + ':' + os.environ.get('PATH', '')\n",
    "                print(f\"   Added {os.path.dirname(path)} to PATH\")\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for marctable: {e}\")\n",
    "\n",
    "# Setup logging for MARC conversion\n",
    "log_dir = f'{output_dir}/logs'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, 'marc2parquet.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_file_with_recovery(file: str, institution: str) -> bool:\n",
    "    \"\"\"Process a MARC file with maximum error recovery\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create a temporary file for processing\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        # Remove _filtered from the output name to avoid double naming\n",
    "        clean_base = base.replace('_filtered', '')\n",
    "        output_file = os.path.join(output_dir, \n",
    "                           f\"{institution}_{clean_base.replace('.mrc', '')}-marc21.parquet\")\n",
    "        \n",
    "        # Process MARC file\n",
    "        written_count, report = safe_read_marc_file_with_recovery(file, temp_file)\n",
    "        \n",
    "        # Proceed if we have at least some records\n",
    "        if written_count == 0:\n",
    "            error_msg = f\"No records could be processed from {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Check temp file exists and has content\n",
    "        if not os.path.exists(temp_file):\n",
    "            error_msg = f\"Temporary file {temp_file} does not exist!\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "            \n",
    "        temp_file_size = os.path.getsize(temp_file)\n",
    "        print(f\"  - Temp file size: {temp_file_size:,} bytes\")\n",
    "        \n",
    "        # Try different ways to run marctable\n",
    "        marctable_cmd = None\n",
    "        \n",
    "        # Method 1: Try direct command\n",
    "        if subprocess.run(['which', 'marctable'], capture_output=True).returncode == 0:\n",
    "            marctable_cmd = ['marctable', 'parquet', temp_file, output_file]\n",
    "        # Method 2: Try with python -m\n",
    "        else:\n",
    "            marctable_cmd = [sys.executable, '-m', 'marctable', 'parquet', temp_file, output_file]\n",
    "        \n",
    "        print(f\"Running command: {' '.join(marctable_cmd)}\")\n",
    "        \n",
    "        # Run marctable command with better error capture\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                marctable_cmd,\n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            if result.stdout:\n",
    "                print(f\"  - stdout: {result.stdout}\")\n",
    "            if result.stderr:\n",
    "                print(f\"  - stderr: {result.stderr}\")\n",
    "                \n",
    "            # Check if output file was created\n",
    "            if os.path.exists(output_file):\n",
    "                output_size = os.path.getsize(output_file)\n",
    "                success_msg = f\"SUCCESS: Created {output_file} ({output_size:,} bytes) with {written_count} {institution} records\"\n",
    "                logger.info(success_msg)\n",
    "                print(success_msg)\n",
    "                return True\n",
    "            else:\n",
    "                error_msg = f\"Output file {output_file} was not created\"\n",
    "                logger.error(error_msg)\n",
    "                print(f\"ERROR: {error_msg}\")\n",
    "                return False\n",
    "                \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            error_msg = f\"marctable command failed with exit code {e.returncode}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            print(f\"  - stdout: {e.stdout}\")\n",
    "            print(f\"  - stderr: {e.stderr}\")\n",
    "            return False\n",
    "        except FileNotFoundError:\n",
    "            error_msg = \"marctable command not found\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error processing {institution} file {file}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if 'temp_file' in locals() and temp_file and os.path.exists(temp_file):\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error for {temp_file}: {str(e)}\")\n",
    "\n",
    "def safe_read_marc_file_with_recovery(input_file: str, output_file: str) -> Tuple[int, Dict]:\n",
    "    \"\"\"\n",
    "    Read MARC file with aggressive error recovery, \n",
    "    capturing as many records as possible\n",
    "    \"\"\"\n",
    "    written_count = 0\n",
    "    error_count = 0\n",
    "    recovered_count = 0\n",
    "    error_types = {}\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'rb') as fh:\n",
    "            # Try to read the entire file into memory for better recovery\n",
    "            file_content = fh.read()\n",
    "            \n",
    "        # First pass: try standard reading\n",
    "        try:\n",
    "            reader = MARCReader(file_content, to_unicode=True, force_utf8=True, \n",
    "                              hide_utf8_warnings=True, utf8_handling='ignore')\n",
    "            \n",
    "            with open(output_file, 'wb') as out:\n",
    "                for record in reader:\n",
    "                    if record is not None:\n",
    "                        try:\n",
    "                            out.write(record.as_marc())\n",
    "                            written_count += 1\n",
    "                        except Exception as e:\n",
    "                            error_count += 1\n",
    "                            error_type = type(e).__name__\n",
    "                            error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "                            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Standard reading failed: {str(e)}. Attempting recovery...\")\n",
    "            \n",
    "            # Second pass: chunk-based recovery\n",
    "            chunk_size = 2048  # Start with 2KB chunks\n",
    "            position = 0\n",
    "            \n",
    "            with open(output_file, 'wb') as out:\n",
    "                while position < len(file_content):\n",
    "                    # Find next record start (0x1D)\n",
    "                    start = file_content.find(b'\\x1d', position)\n",
    "                    if start == -1:\n",
    "                        break\n",
    "                    \n",
    "                    # Find end of this record (next 0x1D or end of file)\n",
    "                    end = file_content.find(b'\\x1d', start + 1)\n",
    "                    if end == -1:\n",
    "                        end = len(file_content)\n",
    "                    \n",
    "                    # Try to parse this chunk as a record\n",
    "                    try:\n",
    "                        chunk = file_content[start:end + 1]\n",
    "                        record = Record(data=chunk, to_unicode=True, force_utf8=True)\n",
    "                        if record and hasattr(record, 'leader'):\n",
    "                            out.write(record.as_marc())\n",
    "                            written_count += 1\n",
    "                            recovered_count += 1\n",
    "                    except:\n",
    "                        error_count += 1\n",
    "                    \n",
    "                    position = end + 1\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error reading file: {str(e)}\")\n",
    "        error_count += 1\n",
    "        error_types['Critical'] = 1\n",
    "    \n",
    "    # Calculate success rate\n",
    "    total_attempted = written_count + error_count\n",
    "    success_rate = (written_count / total_attempted * 100) if total_attempted > 0 else 0\n",
    "    \n",
    "    report = {\n",
    "        'written': written_count,\n",
    "        'errors': error_count,\n",
    "        'recovered': recovered_count,\n",
    "        'success_rate': success_rate,\n",
    "        'error_types': error_types\n",
    "    }\n",
    "    \n",
    "    return written_count, report\n",
    "\n",
    "def detect_encoding(file_path: str, sample_size: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Detect the encoding of a MARC file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sample = f.read(sample_size)\n",
    "        \n",
    "        # First try to detect using chardet\n",
    "        import chardet\n",
    "        result = chardet.detect(sample)\n",
    "        confidence = result.get('confidence', 0)\n",
    "        \n",
    "        if confidence > 0.7:\n",
    "            return result['encoding']\n",
    "        \n",
    "        # Fallback: check for MARC-8 indicators\n",
    "        if b'\\x1b' in sample:  # ESC character often indicates MARC-8\n",
    "            return 'MARC-8'\n",
    "        \n",
    "        # Default to UTF-8\n",
    "        return 'utf-8'\n",
    "    except:\n",
    "        return 'utf-8'\n",
    "\n",
    "def convert_marc_to_parquet_batch(marc_files: List[str], output_dir: str, \n",
    "                                 batch_name: str = \"batch\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert multiple MARC files to Parquet format in batch\n",
    "    \"\"\"\n",
    "    success_files = []\n",
    "    \n",
    "    for i, marc_file in enumerate(marc_files):\n",
    "        try:\n",
    "            # Create output filename\n",
    "            base_name = os.path.basename(marc_file).replace('.mrc', '')\n",
    "            output_file = os.path.join(output_dir, f\"{batch_name}_{base_name}.parquet\")\n",
    "            \n",
    "            # Process with recovery\n",
    "            logger.info(f\"Processing {marc_file} ({i+1}/{len(marc_files)})\")\n",
    "            temp_file = f\"/tmp/temp_{batch_name}_{i}.mrc\"\n",
    "            \n",
    "            written, report = safe_read_marc_file_with_recovery(marc_file, temp_file)\n",
    "            \n",
    "            if written > 0:\n",
    "                # Convert to parquet using marctable\n",
    "                cmd = f\"marctable parquet {temp_file} {output_file}\"\n",
    "                result = os.system(cmd)\n",
    "                \n",
    "                if result == 0:\n",
    "                    success_files.append(output_file)\n",
    "                    logger.info(f\"Successfully converted {marc_file} -> {output_file}\")\n",
    "                else:\n",
    "                    logger.error(f\"marctable failed for {marc_file}\")\n",
    "                \n",
    "                # Cleanup temp file\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.remove(temp_file)\n",
    "            else:\n",
    "                logger.error(f\"No records recovered from {marc_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {marc_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return success_files\n",
    "\n",
    "def process_institution_marc_files(institution: str, files: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Process all MARC files for a single institution\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {institution.upper()} - {len(files)} files\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_success = True\n",
    "    \n",
    "    for file in files:\n",
    "        success = process_file_with_recovery(file, institution)\n",
    "        all_success = all_success and success\n",
    "    \n",
    "    return all_success\n",
    "\n",
    "print(\"✅ All MARC processing functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process MARC files to Parquet for each institution - BATCH PROCESSING VERSION\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Update input_dir to point to your specific directory\n",
    "input_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/final\"\n",
    "print(f\"Using input directory: {input_dir}\")\n",
    "\n",
    "print(\"=== PROCESSING MARC FILES TO PARQUET - BATCH MODE ===\")\n",
    "print(\"This will convert MARC files from each institution to Parquet format\")\n",
    "print(\"⚠️  WARNING: Source .mrc files will be DELETED immediately after conversion to save space!\\n\")\n",
    "\n",
    "# Check initial disk space\n",
    "import subprocess\n",
    "df_output = subprocess.run(\"df -h /\", shell=True, capture_output=True, text=True)\n",
    "print(\"📊 Initial disk usage:\")\n",
    "print(df_output.stdout)\n",
    "\n",
    "# Define institutions in order of file size (smallest first to maximize success)\n",
    "# This order processes smaller files first to free up space gradually\n",
    "institutions_by_size = [\n",
    "    ('harvard', 110.0),\n",
    "]\n",
    "\n",
    "# Dictionary to track processing results\n",
    "processing_results = {}\n",
    "deleted_files = []\n",
    "space_freed = 0\n",
    "cumulative_space_freed = 0\n",
    "\n",
    "# Process in batches\n",
    "print(\"\\n📋 Processing order (smallest to largest):\")\n",
    "for inst, size in institutions_by_size:\n",
    "    print(f\"  - {inst}: {size:.1f} GB\")\n",
    "\n",
    "print(\"\\nStarting batch processing...\\n\")\n",
    "\n",
    "for institution, expected_size_gb in institutions_by_size:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {institution.upper()} (expected ~{expected_size_gb} GB)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check current disk space\n",
    "    df_result = subprocess.run(\"df -h / | tail -1\", shell=True, capture_output=True, text=True)\n",
    "    available_space = df_result.stdout.split()[3]\n",
    "    print(f\"📊 Current available space: {available_space}\")\n",
    "    \n",
    "    # Look for the MARC file\n",
    "    marc_file = f\"{input_dir}/{institution}_filtered.mrc\"\n",
    "    \n",
    "    if not os.path.exists(marc_file):\n",
    "        print(f\"⚠️  No MARC file found for {institution}\")\n",
    "        processing_results[institution] = {\"status\": \"no_files\", \"count\": 0}\n",
    "        continue\n",
    "    \n",
    "    # Get the actual file size\n",
    "    file_size = os.path.getsize(marc_file)\n",
    "    print(f\"Found MARC file: {os.path.basename(marc_file)} ({file_size/1024/1024/1024:.2f} GB)\")\n",
    "    \n",
    "    # Check if we have enough space for conversion (need at least 50% of file size for safety)\n",
    "    df_bytes_result = subprocess.run(\"df / | tail -1\", shell=True, capture_output=True, text=True)\n",
    "    available_bytes = int(df_bytes_result.stdout.split()[3]) * 1024  # Convert KB to bytes\n",
    "    \n",
    "    required_space = file_size * 0.5  # Need 50% of file size for conversion\n",
    "    if available_bytes < required_space:\n",
    "        print(f\"⚠️  WARNING: May not have enough space!\")\n",
    "        print(f\"  - Available: {available_bytes/1024/1024/1024:.2f} GB\")\n",
    "        print(f\"  - Required: {required_space/1024/1024/1024:.2f} GB\")\n",
    "        # Non-interactive mode: proceed automatically\n",
    "        print(\"🤖 Non-interactive mode: proceeding despite low space warning.\")\n",
    "        # If you prefer to skip instead of proceed, uncomment the next lines:\n",
    "        # processing_results[institution] = {\"status\": \"skipped_low_space\", \"count\": 0}\n",
    "        # print(\"Skipping this file due to low disk space.\")\n",
    "        # continue\n",
    "    \n",
    "    marc_files = [marc_file]\n",
    "    \n",
    "    # Process the institution's file\n",
    "    try:\n",
    "        success = process_institution_marc_files(institution, marc_files)\n",
    "        \n",
    "        if success:\n",
    "            # Check what was created\n",
    "            output_pattern = f\"{output_dir}/{institution}_*marc21.parquet\"\n",
    "            created_files = glob.glob(output_pattern)\n",
    "            \n",
    "            if created_files:\n",
    "                print(f\"\\n✅ Successfully created {len(created_files)} Parquet file(s) for {institution}\")\n",
    "                total_parquet_size = 0\n",
    "                for cf in created_files:\n",
    "                    parquet_size = os.path.getsize(cf)\n",
    "                    total_parquet_size += parquet_size\n",
    "                    print(f\"  - {os.path.basename(cf)} ({parquet_size/1024/1024:.2f} MB)\")\n",
    "                \n",
    "                # Show compression ratio\n",
    "                compression_ratio = (1 - total_parquet_size/file_size) * 100\n",
    "                print(f\"  📊 Compression: {compression_ratio:.1f}% reduction\")\n",
    "                \n",
    "                # DELETE THE SOURCE MARC FILE IMMEDIATELY\n",
    "                try:\n",
    "                    os.remove(marc_file)\n",
    "                    deleted_files.append(marc_file)\n",
    "                    space_freed = file_size\n",
    "                    cumulative_space_freed += space_freed\n",
    "                    print(f\"  🗑️  Deleted source file: {os.path.basename(marc_file)}\")\n",
    "                    print(f\"  💾 Freed: {space_freed/1024/1024/1024:.2f} GB\")\n",
    "                    print(f\"  📊 Total freed so far: {cumulative_space_freed/1024/1024/1024:.2f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️  WARNING: Could not delete source file: {e}\")\n",
    "                \n",
    "                processing_results[institution] = {\n",
    "                    \"status\": \"success\",\n",
    "                    \"count\": len(created_files),\n",
    "                    \"files\": created_files,\n",
    "                    \"source_deleted\": marc_file in deleted_files,\n",
    "                    \"space_freed_gb\": space_freed/1024/1024/1024\n",
    "                }\n",
    "                \n",
    "                # Show updated disk space\n",
    "                df_after = subprocess.run(\"df -h / | tail -1\", shell=True, capture_output=True, text=True)\n",
    "                print(f\"\\n📊 Disk space after {institution}:\")\n",
    "                print(f\"   {df_after.stdout}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"⚠️  No Parquet files were created for {institution}\")\n",
    "                print(f\"  💾 Keeping source file: {os.path.basename(marc_file)}\")\n",
    "                processing_results[institution] = {\"status\": \"no_output\", \"count\": 0}\n",
    "        else:\n",
    "            print(f\"❌ Failed to process {institution} MARC files\")\n",
    "            print(f\"  💾 Keeping source file due to error: {os.path.basename(marc_file)}\")\n",
    "            processing_results[institution] = {\"status\": \"failed\", \"count\": 0}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {institution}: {str(e)}\")\n",
    "        print(f\"  💾 Keeping source file due to error: {os.path.basename(marc_file)}\")\n",
    "        processing_results[institution] = {\"status\": \"error\", \"count\": 0, \"error\": str(e)}\n",
    "\n",
    "# Final summary report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful = sum(1 for r in processing_results.values() if r[\"status\"] == \"success\")\n",
    "failed = sum(1 for r in processing_results.values() if r[\"status\"] in [\"failed\", \"error\"])\n",
    "skipped = sum(1 for r in processing_results.values() if r[\"status\"] == \"skipped\")\n",
    "no_files = sum(1 for r in processing_results.values() if r[\"status\"] == \"no_files\")\n",
    "\n",
    "print(f\"Total institutions: {len(institutions_by_size)}\")\n",
    "print(f\"✅ Successful: {successful}\")\n",
    "print(f\"❌ Failed: {failed}\")\n",
    "print(f\"⏭️  Skipped: {skipped}\")\n",
    "print(f\"📭 No files: {no_files}\")\n",
    "\n",
    "print(\"\\nDetailed results:\")\n",
    "for inst, result in sorted(processing_results.items()):\n",
    "    status_emoji = \"✅\" if result[\"status\"] == \"success\" else \"❌\" if result[\"status\"] in [\"failed\", \"error\"] else \"⏭️\" if result[\"status\"] == \"skipped\" else \"📭\"\n",
    "    deleted_emoji = \"🗑️\" if result.get(\"source_deleted\", False) else \"💾\"\n",
    "    space_info = f\" (freed {result.get('space_freed_gb', 0):.1f} GB)\" if result.get('space_freed_gb') else \"\"\n",
    "    print(f\"{status_emoji} {inst}: {result['status']} {deleted_emoji}{space_info}\")\n",
    "\n",
    "# Space savings report\n",
    "print(f\"\\n💾 SPACE SAVINGS:\")\n",
    "print(f\"  - Deleted {len(deleted_files)} source files\")\n",
    "print(f\"  - Total space freed: {cumulative_space_freed/1024/1024/1024:.2f} GB\")\n",
    "\n",
    "# Final disk check\n",
    "print(\"\\n📊 Final disk usage:\")\n",
    "df_final = subprocess.run(\"df -h /\", shell=True, capture_output=True, text=True)\n",
    "print(df_final.stdout)\n",
    "\n",
    "# List all created Parquet files\n",
    "print(f\"\\n=== All Parquet files in {output_dir} ===\")\n",
    "all_parquet = glob.glob(f\"{output_dir}/*.parquet\")\n",
    "print(f\"Total Parquet files: {len(all_parquet)}\")\n",
    "total_parquet_size = 0\n",
    "for pf in sorted(all_parquet):\n",
    "    file_size = os.path.getsize(pf)\n",
    "    total_parquet_size += file_size\n",
    "    print(f\"  - {os.path.basename(pf)} ({file_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal Parquet storage: {total_parquet_size/1024/1024/1024:.2f} GB\")\n",
    "print(f\"Original MARC size: ~167 GB\")\n",
    "print(f\"Compression ratio: {(1 - total_parquet_size/(167*1024*1024*1024))*100:.1f}%\")\n",
    "\n",
    "print(\"\\n✅ Batch MARC to Parquet conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Processing with Memory-Optimized Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before re-processing\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing - Memory-Optimized with Batch Processing\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=== STARTING MAIN PROCESSING ===\")\n",
    "print(\"This will process all institution parquet files and create the exploded dataset\\n\")\n",
    "\n",
    "# Get all institution parquet files\n",
    "parquet_files = glob.glob(f\"{input_dir}/*.parquet\")\n",
    "print(f\"Found {len(parquet_files)} institution parquet files to process\")\n",
    "\n",
    "# Process each institution and save to temp directory\n",
    "temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "processed_institutions = []\n",
    "\n",
    "for file_path in parquet_files:\n",
    "    # Extract institution name from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    institution = filename.split('_')[0]\n",
    "    \n",
    "    print(f\"\\nProcessing {institution}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read institution data\n",
    "        df = spark.read.parquet(file_path)\n",
    "        # Skip counting - just process\n",
    "        # record_count = df.count()\n",
    "        # print(f\"  - Records: {record_count:,}\")\n",
    "        \n",
    "        # Apply all enhanced processing\n",
    "        processed_df = process_institution_optimized(df, institution)\n",
    "        \n",
    "        # Save processed data\n",
    "        temp_path = f\"{temp_output_dir}/{institution}_processed.parquet\"\n",
    "        processed_df.write.mode(\"overwrite\").parquet(temp_path)\n",
    "        \n",
    "        processed_institutions.append((institution, temp_path))\n",
    "        print(f\"  ✅ Saved to {temp_path}\")\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {institution}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Processed {len(processed_institutions)} institutions\")\n",
    "\n",
    "# Now create the exploded dataset by reading all processed files\n",
    "print(\"\\n=== CREATING EXPLODED DATASET ===\")\n",
    "print(\"This creates a row for each identifier/key in each record...\")\n",
    "\n",
    "# Read all processed institution files\n",
    "processed_paths = [path for _, path in processed_institutions]\n",
    "all_df = spark.read.parquet(*processed_paths)\n",
    "\n",
    "# Create exploded dataset with id_list as key_array\n",
    "all_df_with_key_array = all_df.withColumn(\"key_array\", F.col(\"id_list\"))\n",
    "\n",
    "# Explode the key_array to create one row per key\n",
    "all_df_exploded = all_df_with_key_array.select(\n",
    "    \"F001\", \"source\", \"match_key\", \"is_valid_match_key\",\n",
    "    F.explode(\"key_array\").alias(\"key\")\n",
    ").filter(F.col(\"key\").isNotNull())\n",
    "\n",
    "# Persist exploded dataset for downstream cells\n",
    "exploded_path = f\"{output_dir}/all_df_exploded.parquet\"\n",
    "all_df_exploded.write.mode(\"overwrite\").parquet(exploded_path)\n",
    "print(f\"\\n✅ Saved exploded dataset to: {exploded_path}\")\n",
    "\n",
    "# Cache for immediate use\n",
    "all_df_exploded.cache()\n",
    "print(f\"📌 Dataset cached in memory for subsequent analysis\")\n",
    "\n",
    "print(f\"\\n💡 Ready for uniqueness analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn Overlap Analysis: compute unique_penn and overlap summary\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "print(\"=== PENN OVERLAP ANALYSIS ===\")\n",
    "exploded_path = f\"{output_dir}/all_df_exploded.parquet\"\n",
    "all_df_exploded = spark.read.parquet(exploded_path)\n",
    "print(f\"Loaded exploded dataset: {exploded_path}\")\n",
    "\n",
    "# Classify identifier types based on value patterns\n",
    "id_classified = all_df_exploded.withColumn(\n",
    "    \"id_type\",\n",
    "    F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), F.lit(\"ISBN\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), F.lit(\"OCLC\"))\n",
    "     .when(F.col(\"key\").contains(\"_\"), F.lit(\"MatchKey\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[A-Za-z0-9-]{5,}$\"), F.lit(\"LCCN\"))\n",
    "     .otherwise(F.lit(\"OTHER\"))\n",
    ")\n",
    "standard_types = [\"ISBN\", \"OCLC\", \"LCCN\", \"MatchKey\"]\n",
    "df_std = id_classified.filter(F.col(\"id_type\").isin(standard_types))\n",
    "\n",
    "# Compute cross-institution collisions on standard ids\n",
    "id_counts = df_std.groupBy(\"key\").agg(F.countDistinct(\"source\").alias(\"id_count\"))\n",
    "multi_inst_ids = id_counts.filter(F.col(\"id_count\") > 1).select(\"key\").distinct()\n",
    "print(f\"Standard IDs shared across institutions: {multi_inst_ids.count():,}\")\n",
    "\n",
    "# Compute Penn records that have no standard-id collisions with any non-Penn source\n",
    "penn_df = all_df_exploded.filter(F.col(\"source\") == F.lit(\"penn\"))\n",
    "penn_std = penn_df.join(df_std.select(\"key\").distinct(), on=\"key\", how=\"left_semi\")\n",
    "penn_unique_by_std = penn_std.join(multi_inst_ids, on=\"key\", how=\"left_anti\").select(\"F001\").distinct()\n",
    "\n",
    "# Materialize unique_penn only if not already present\n",
    "unique_penn_path = f\"{output_dir}/unique_penn.parquet\"\n",
    "if not os.path.exists(unique_penn_path):\n",
    "    penn_full = spark.read.parquet(f\"{output_dir}/penn_penn-marc21.parquet\")\n",
    "    unique_penn = penn_full.join(penn_unique_by_std, on=\"F001\", how=\"inner\")\n",
    "    unique_penn.write.mode(\"overwrite\").parquet(unique_penn_path)\n",
    "    print(f\"✅ Saved unique_penn to: {unique_penn_path}\")\n",
    "else:\n",
    "    print(f\"ℹ️ unique_penn already exists at {unique_penn_path}; skipping write\")\n",
    "\n",
    "# Overlap analysis summary per id key (always write for diagnostics)\n",
    "overlap_summary = df_std.groupBy(\"key\").agg(\n",
    "    F.collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"total_occurrences\"),\n",
    "    F.countDistinct(\"source\").alias(\"inst_count\")\n",
    ")\n",
    "penn_overlap = penn_df.join(overlap_summary, on=\"key\", how=\"left\")\n",
    "penn_overlap_path = f\"{output_dir}/penn_overlap_analysis.parquet\"\n",
    "penn_overlap.write.mode(\"overwrite\").parquet(penn_overlap_path)\n",
    "print(f\"✅ Saved penn_overlap_analysis to: {penn_overlap_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative Uniqueness Filtering (revised) - SAFE VERSION\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "print(\"=== CONSERVATIVE UNIQUENESS FILTERING (REVISED) ===\")\n",
    "\n",
    "# Check if September 13 outputs already exist\n",
    "conservative_path = f\"{output_dir}/conservative_unique_penn.parquet\"\n",
    "conservative_filtered_path = f\"{output_dir}/conservative_unique_penn_filtered.parquet\"\n",
    "\n",
    "# Check what we have from September 13\n",
    "for path in [conservative_path, conservative_filtered_path]:\n",
    "    if os.path.exists(path):\n",
    "        success_file = os.path.join(path, \"_SUCCESS\")\n",
    "        if os.path.exists(success_file):\n",
    "            print(f\"✅ {os.path.basename(path)} exists from September 13 - DATA IS SAFE\")\n",
    "            # Get file count to verify\n",
    "            import glob\n",
    "            part_files = glob.glob(os.path.join(path, \"part-*.parquet\"))\n",
    "            print(f\"   Contains {len(part_files)} parquet part files\")\n",
    "\n",
    "# Ask if you want to reprocess or just verify\n",
    "response = input(\"\\nDo you want to REPROCESS (overwrite) or just VERIFY existing data? (reprocess/verify): \").lower()\n",
    "\n",
    "if response == 'verify':\n",
    "    print(\"\\n✅ Verifying existing September 13 data...\")\n",
    "    \n",
    "    # Quick verification without full count\n",
    "    for path in [conservative_path, conservative_filtered_path]:\n",
    "        if os.path.exists(path):\n",
    "            df = spark.read.parquet(path)\n",
    "            print(f\"\\n{os.path.basename(path)}:\")\n",
    "            print(f\"  - Schema verified: {len(df.columns)} columns\")\n",
    "            print(f\"  - Sample record check: {df.limit(1).count() == 1}\")\n",
    "            \n",
    "            # Show schema\n",
    "            print(\"  - Columns:\", ', '.join(df.columns[:10]), \"...\")\n",
    "    \n",
    "    print(\"\\n✅ September 13 data is intact and ready to use!\")\n",
    "    \n",
    "elif response == 'reprocess':\n",
    "    print(\"\\nReprocessing from scratch...\")\n",
    "    \n",
    "    # Your existing logic WITHOUT the counts\n",
    "    exploded_path = f\"{output_dir}/all_df_exploded.parquet\"\n",
    "    if os.path.exists(exploded_path):\n",
    "        all_df_exploded = spark.read.parquet(exploded_path)\n",
    "        print(f\"Loaded exploded dataset from {exploded_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ Exploded dataset not found on disk. Need to recreate.\")\n",
    "        # You'll need to recreate all_df_exploded here\n",
    "    \n",
    "    # Rest of your processing logic...\n",
    "    id_classified = all_df_exploded.withColumn(\n",
    "        \"id_type\",\n",
    "        F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), F.lit(\"ISBN\"))\n",
    "         .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), F.lit(\"OCLC\"))\n",
    "         .when(F.col(\"key\").contains(\"_\"), F.lit(\"MatchKey\"))\n",
    "         .when(F.col(\"key\").rlike(\"^[A-Za-z0-9-]{5,}$\"), F.lit(\"LCCN\"))\n",
    "         .otherwise(F.lit(\"OTHER\"))\n",
    "    )\n",
    "    \n",
    "    # Continue with processing...\n",
    "    standard_types = [\"ISBN\", \"OCLC\", \"LCCN\", \"MatchKey\"]\n",
    "    std_ids = id_classified.filter(F.col(\"id_type\").isin(standard_types))\n",
    "    \n",
    "    # Multi-institution collisions on standard ids\n",
    "    key_inst_count = std_ids.groupBy(\"key\").agg(F.countDistinct(\"source\").alias(\"inst_count\"))\n",
    "    multi_inst_keys = key_inst_count.filter(F.col(\"inst_count\") > 1).select(\"key\").distinct()\n",
    "    \n",
    "    # Penn unique by standard ids\n",
    "    penn_rows = id_classified.filter(F.col(\"source\") == F.lit(\"penn\"))\n",
    "    penn_std = penn_rows.join(std_ids.select(\"key\").distinct(), on=\"key\", how=\"left_semi\")\n",
    "    penn_unique_keys = penn_std.join(multi_inst_keys, on=\"key\", how=\"left_anti\").select(\"F001\").distinct()\n",
    "    \n",
    "    # Cache for one count only\n",
    "    penn_unique_keys.cache()\n",
    "    print(f\"Penn records unique by standard ids: {penn_unique_keys.count():,}\")\n",
    "    penn_unique_keys.unpersist()\n",
    "    \n",
    "    # Rest of processing...\n",
    "    penn_full = spark.read.parquet(f\"{input_dir}/penn_penn-marc21.parquet\")\n",
    "    conservative_unique_penn = penn_full.join(penn_unique_keys, on=\"F001\", how=\"inner\")\n",
    "    \n",
    "    # Apply filters\n",
    "    has_533 = (F.col(\"F533\").isNotNull() & (F.size(F.col(\"F533\")) > 0))\n",
    "    conservative_unique_penn_no533 = conservative_unique_penn.filter(~has_533)\n",
    "    \n",
    "    # HSP filtering\n",
    "    possible_hsp_paths = [\n",
    "        os.path.join(output_dir, \"hsp_removed_mmsid.txt\"),\n",
    "        os.path.join(os.getcwd(), \"hsp_removed_mmsid.txt\"),\n",
    "        \"hsp_removed_mmsid.txt\",\n",
    "    ]\n",
    "    \n",
    "    hsp_path_found = next((p for p in possible_hsp_paths if os.path.exists(p)), None)\n",
    "    if hsp_path_found:\n",
    "        hsp_mmsids = spark.createDataFrame(\n",
    "            [(line.strip(),) for line in open(hsp_path_found, \"r\", encoding=\"utf-8\").read().splitlines() if line.strip()],\n",
    "            schema=[\"F001\"]\n",
    "        )\n",
    "        conservative_filtered = conservative_unique_penn_no533.join(hsp_mmsids, on=\"F001\", how=\"left_anti\")\n",
    "        print(f\"Applied HSP exclusion filter from: {hsp_path_found}\")\n",
    "    else:\n",
    "        conservative_filtered = conservative_unique_penn_no533\n",
    "        print(\"HSP exclusion list not found; skipping HSP filter\")\n",
    "    \n",
    "    # Write WITHOUT counts in the print statements\n",
    "    print(f\"\\nWriting conservative_unique_penn to: {conservative_path}\")\n",
    "    conservative_unique_penn.write.mode(\"overwrite\").parquet(conservative_path)\n",
    "    print(f\"✅ Write complete\")\n",
    "    \n",
    "    print(f\"\\nWriting conservative_unique_penn_filtered to: {conservative_filtered_path}\")  \n",
    "    conservative_filtered.write.mode(\"overwrite\").parquet(conservative_filtered_path)\n",
    "    print(f\"✅ Write complete\")\n",
    "    \n",
    "    print(\"\\n✅ Reprocessing complete! Use the verification option to check record counts.\")\n",
    "else:\n",
    "    print(\"\\n✅ No changes made. Your September 13 data remains intact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative Uniqueness Analysis - OPTIMIZED VERSION\n",
    "# Disable broadcast joins to prevent timeout errors with large datasets\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\")  # 20 minutes (if broadcast is re-enabled)\n",
    "\n",
    "from pyspark.sql.functions import col, size, array_contains, collect_set, count, when\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=== CONSERVATIVE UNIQUENESS ANALYSIS (OPTIMIZED) ===\")\n",
    "print(\"Analyzing POD dataset for unique Penn records\")\n",
    "print(\"Applying stricter criteria to identify truly unique records\\n\")\n",
    "print(\"⚠️ This version avoids expensive count operations to prevent hangs\\n\")\n",
    "\n",
    "# Check if we have the exploded dataset on disk\n",
    "exploded_path = f\"{output_dir}/all_df_exploded.parquet\"\n",
    "if not os.path.exists(exploded_path):\n",
    "    print(\"❌ Exploded dataset not found. Please run the main processing cell first.\")\n",
    "    raise FileNotFoundError(f\"Missing required file: {exploded_path}\")\n",
    "\n",
    "print(f\"✓ Loading exploded POD dataset from: {exploded_path}\")\n",
    "all_df_exploded = spark.read.parquet(exploded_path)\n",
    "\n",
    "# STEP 1: Classify identifier types based on key patterns (matching Penn Overlap Analysis)\n",
    "print(\"\\n[1/7] Classifying identifier types...\")\n",
    "id_classified = all_df_exploded.withColumn(\n",
    "    \"id_type\",\n",
    "    F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), F.lit(\"ISBN\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), F.lit(\"OCLC\"))\n",
    "     .when(F.col(\"key\").contains(\"_\"), F.lit(\"MatchKey\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[A-Za-z0-9-]{5,}$\"), F.lit(\"LCCN\"))\n",
    "     .otherwise(F.lit(\"OTHER\"))\n",
    ")\n",
    "\n",
    "# STEP 2: Filter to Penn records with standard ID types\n",
    "print(\"\\n[2/7] Filtering to Penn POD records with standard IDs...\")\n",
    "standard_types = ['OCLC', 'LCCN', 'ISBN', 'MatchKey']\n",
    "penn_rows = id_classified.filter(col(\"source\") == \"penn\")\n",
    "penn_std = penn_rows.filter(col(\"id_type\").isin(standard_types))\n",
    "penn_std_exploded_count = penn_std.count()\n",
    "penn_std_unique = penn_std.select(\"F001\").distinct()\n",
    "penn_std_count = penn_std_unique.count()\n",
    "print(f\"  → {penn_std_exploded_count:,} Penn identifier rows with standard IDs (exploded)\")\n",
    "print(f\"  → {penn_std_count:,} unique Penn POD records (F001s) with standard IDs\")\n",
    "\n",
    "# STEP 3: Group by key across ALL INSTITUTIONS to identify shared vs unique IDs\n",
    "print(\"\\n[3/7] Grouping by key across ALL institutions to detect sharing...\")\n",
    "print(\"  ⏳ This may take several minutes for large datasets...\")\n",
    "\n",
    "# CRITICAL: Group across ALL institutions (not just Penn) to detect sharing\n",
    "all_std = id_classified.filter(col(\"id_type\").isin(standard_types))\n",
    "grouped_standard = (\n",
    "    all_std\n",
    "    .groupBy(\"key\")\n",
    "    .agg(\n",
    "        collect_set(\"source\").alias(\"sources\"),\n",
    "        collect_set(\"F001\").alias(\"f001_values\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mark if shared across institutions (size > 1)\n",
    "grouped_standard = grouped_standard.withColumn(\n",
    "    \"is_shared\", \n",
    "    size(col(\"sources\")) > 1\n",
    ")\n",
    "\n",
    "print(\"  → Grouping complete\")\n",
    "\n",
    "# STEP 4: Join back to get sharing status for Penn records\n",
    "print(\"\\n[4/7] Joining sharing status back to Penn records...\")\n",
    "penn_with_standard_ids = penn_std.join(\n",
    "    grouped_standard.select(\"key\", \"is_shared\"),\n",
    "    on=\"key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# STEP 5: Filter to unique Penn records (not shared)\n",
    "print(\"\\n[5/7] Filtering to unique Penn POD records (not shared with other institutions)...\")\n",
    "penn_unique_keys = penn_with_standard_ids.filter(col(\"is_shared\") == False)\n",
    "\n",
    "# Get distinct F001s from unique records\n",
    "conservative_unique_f001s = penn_unique_keys.select(\"F001\").distinct()\n",
    "unique_count = conservative_unique_f001s.count()\n",
    "print(f\"  → {unique_count:,} unique Penn POD F001s identified (truly unique to Penn)\")\n",
    "\n",
    "# STEP 6: Join back to full Penn dataset to get complete records\n",
    "print(\"\\n[6/7] Retrieving full records for unique F001s...\")\n",
    "penn_full = all_df_exploded.filter(col(\"source\") == \"penn\")\n",
    "conservative_unique_penn = penn_full.join(\n",
    "    conservative_unique_f001s,\n",
    "    on=\"F001\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "final_count = conservative_unique_penn.select(\"F001\").distinct().count()\n",
    "print(f\"  → {final_count:,} unique Penn POD records retrieved (verification)\")\n",
    "\n",
    "# STEP 7: Save results\n",
    "print(\"\\n[7/7] Saving results...\")\n",
    "conservative_unique_path = f\"{output_dir}/conservative_unique_penn.parquet\"\n",
    "conservative_unique_penn.write.mode(\"overwrite\").parquet(conservative_unique_path)\n",
    "print(f\"  ✓ Saved to: {conservative_unique_path}\")\n",
    "\n",
    "# Save just the F001 list for quick reference\n",
    "f001_list_path = f\"{output_dir}/conservative_unique_f001s.parquet\"\n",
    "conservative_unique_f001s.write.mode(\"overwrite\").parquet(f001_list_path)\n",
    "print(f\"  ✓ F001 list saved to: {f001_list_path}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSERVATIVE UNIQUENESS SUMMARY (POD DATA)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Penn POD records with standard IDs:   {penn_std_count:,}\")\n",
    "print(f\"Unique Penn POD F001s (not shared):   {unique_count:,}\")\n",
    "print(f\"Uniqueness rate:                      {(unique_count/penn_std_count*100):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display sample of unique records (recompute id_type for display)\n",
    "print(\"\\nSample of unique Penn POD records:\")\n",
    "conservative_unique_penn_display = conservative_unique_penn.withColumn(\n",
    "    \"id_type\",\n",
    "    F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), F.lit(\"ISBN\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), F.lit(\"OCLC\"))\n",
    "     .when(F.col(\"key\").contains(\"_\"), F.lit(\"MatchKey\"))\n",
    "     .when(F.col(\"key\").rlike(\"^[A-Za-z0-9-]{5,}$\"), F.lit(\"LCCN\"))\n",
    "     .otherwise(F.lit(\"OTHER\"))\n",
    ")\n",
    "conservative_unique_penn_display.select(\"F001\", \"key\", \"id_type\", \"source\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\n✅ Conservative uniqueness analysis complete!\")\n",
    "print(\"💡 Next: Apply format filters (no 533 fields) for final refinement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Filtering - Reproduction Removal and HSP Exclusion\n",
    "from pyspark.sql.functions import col, when, size, array_contains, collect_set, min\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=== ADDITIONAL FILTERING FOR ACCURATE UNIQUENESS ===\")\n",
    "print(\"Removing reproductions and HSP records while preserving all unique physical items\\n\")\n",
    "\n",
    "# Load the conservative unique records if not already loaded\n",
    "if 'conservative_unique' not in locals():\n",
    "    conservative_unique = spark.read.parquet(f\"{output_dir}/conservative_unique_penn.parquet\")\n",
    "\n",
    "# Get the count of conservative unique records as our baseline\n",
    "if 'conservative_unique_count' not in locals():\n",
    "    conservative_unique_count = conservative_unique.select(\"F001\").distinct().count()\n",
    "    print(f\"Conservative unique records loaded: {conservative_unique_count:,}\\n\")\n",
    "\n",
    "# Load the full Penn records with all fields\n",
    "penn_full = spark.read.parquet(f\"{input_dir}/penn_penn-marc21.parquet\")\n",
    "\n",
    "# IMPORTANT: Ensure we only get unique F001s from penn_full to avoid duplicates\n",
    "penn_full_unique = penn_full.dropDuplicates([\"F001\"])\n",
    "\n",
    "# Join to get full records for conservative unique items\n",
    "conservative_unique_full = conservative_unique.join(penn_full_unique, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Verify the join didn't create duplicates\n",
    "joined_count = conservative_unique_full.count()\n",
    "if joined_count != conservative_unique_count:\n",
    "    print(f\"⚠️  WARNING: Join created duplicates! Expected {conservative_unique_count:,}, got {joined_count:,}\")\n",
    "    print(\"Deduplicating by F001...\")\n",
    "    conservative_unique_full = conservative_unique_full.dropDuplicates([\"F001\"])\n",
    "    joined_count = conservative_unique_full.count()\n",
    "    print(f\"After deduplication: {joined_count:,} records\\n\")\n",
    "\n",
    "print(\"=== FILTERING STEPS ===\")\n",
    "print(\"Note: NOT deduplicating by ISBN or OCLC - we want to count ALL unique physical items\\n\")\n",
    "\n",
    "# Start with the full joined records\n",
    "conservative_filtered = conservative_unique_full\n",
    "\n",
    "print(\"📋 STEP 1: REMOVING REPRODUCTIONS (F533 FIELD)\")\n",
    "print(\"Filtering out records with reproduction notes...\")\n",
    "\n",
    "# Remove records with F533 (reproduction note)\n",
    "if \"F533\" in conservative_filtered.columns:\n",
    "    conservative_no_reproductions = conservative_filtered.filter(col(\"F533\").isNull())\n",
    "    no_repro_count = conservative_no_reproductions.count()\n",
    "    removed_by_f533 = joined_count - no_repro_count\n",
    "    \n",
    "    print(f\"  - Records before F533 filter: {joined_count:,}\")\n",
    "    print(f\"  - Records after F533 filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Removed by F533 filter: {removed_by_f533:,}\")\n",
    "else:\n",
    "    print(\"  - F533 field not found, skipping reproduction filter\")\n",
    "    conservative_no_reproductions = conservative_filtered\n",
    "    no_repro_count = joined_count\n",
    "    removed_by_f533 = 0\n",
    "\n",
    "print(\"\\n📋 STEP 2: REMOVING HSP (HISTORICAL SOCIETY OF PENNSYLVANIA) RECORDS\")\n",
    "print(\"Loading HSP exclusion list from file...\")\n",
    "\n",
    "# Load HSP F001 values from text file\n",
    "hsp_file_path = \"/home/jovyan/work/July-2025-PODParquet/hsp_removed_mmsid.txt\"\n",
    "try:\n",
    "    # Read the HSP F001 values from the text file\n",
    "    with open(hsp_file_path, 'r') as f:\n",
    "        hsp_f001_list = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"  - Loaded {len(hsp_f001_list):,} HSP F001 values from file\")\n",
    "    \n",
    "    # Convert to DataFrame for efficient joining\n",
    "    hsp_df = spark.createDataFrame([(f001,) for f001 in hsp_f001_list], [\"F001\"])\n",
    "    \n",
    "    # Remove HSP records using anti-join\n",
    "    conservative_no_hsp = conservative_no_reproductions.join(\n",
    "        hsp_df,\n",
    "        on=\"F001\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    \n",
    "    no_hsp_count = conservative_no_hsp.count()\n",
    "    removed_by_hsp = no_repro_count - no_hsp_count\n",
    "    \n",
    "    print(f\"  - Records before HSP filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Records after HSP filter: {no_hsp_count:,}\")\n",
    "    print(f\"  - Removed by HSP filter: {removed_by_hsp:,}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"  ⚠️  WARNING: HSP file not found at {hsp_file_path}\")\n",
    "    print(\"  Falling back to pattern-based HSP detection...\")\n",
    "    \n",
    "    # Fallback: Use pattern matching approach\n",
    "    hsp_patterns = [\n",
    "        \"HSP\",\n",
    "        \"Historical Society of Pennsylvania\",\n",
    "        \"Hist Soc Penn\",\n",
    "        \"Hist.Soc.Penn\"\n",
    "    ]\n",
    "    \n",
    "    # Create filter conditions for HSP detection\n",
    "    hsp_filter_conditions = F.lit(False)\n",
    "    \n",
    "    # Check F710 (corporate name added entry)\n",
    "    if \"F710\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F710\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Check F590 (local note)\n",
    "    if \"F590\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F590\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Check F500 (general note)\n",
    "    if \"F500\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F500\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Apply HSP filter\n",
    "    conservative_no_hsp = conservative_no_reproductions.filter(~hsp_filter_conditions)\n",
    "    no_hsp_count = conservative_no_hsp.count()\n",
    "    removed_by_hsp = no_repro_count - no_hsp_count\n",
    "    \n",
    "    print(f\"  - Records before HSP filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Records after HSP filter: {no_hsp_count:,}\")\n",
    "    print(f\"  - Removed by HSP filter: {removed_by_hsp:,}\")\n",
    "\n",
    "# Calculate final statistics\n",
    "print(\"\\n=== FINAL FILTERED UNIQUENESS SUMMARY ===\")\n",
    "print(f\"Conservative unique (starting point): {conservative_unique_count:,}\")\n",
    "print(f\"After removing reproductions: {no_repro_count:,}\")\n",
    "print(f\"After removing HSP records: {no_hsp_count:,}\")\n",
    "\n",
    "# Calculate filtering rate (what percentage of conservative unique records survived filtering)\n",
    "filtering_rate = (no_hsp_count / conservative_unique_count * 100) if conservative_unique_count > 0 else 0\n",
    "print(f\"\\nFiltering rate: {filtering_rate:.1f}% of conservative unique records retained\")\n",
    "\n",
    "print(f\"\\n📊 FILTERING IMPACT:\")\n",
    "print(f\"  - Reproductions removed: {removed_by_f533:,}\")\n",
    "print(f\"  - HSP records removed: {removed_by_hsp:,}\")\n",
    "print(f\"  - Total filtered out: {conservative_unique_count - no_hsp_count:,}\")\n",
    "print(f\"\\n✅ PRESERVED: All unique physical items (no ISBN/OCLC deduplication)\")\n",
    "\n",
    "# Save the final filtered dataset\n",
    "conservative_no_hsp.select(\"F001\").write.mode(\"overwrite\").parquet(\n",
    "    f\"{output_dir}/conservative_unique_penn_filtered.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Additional filtering complete!\")\n",
    "print(f\"Final filtered results saved to: {output_dir}/conservative_unique_penn_filtered.parquet\")\n",
    "print(f\"\\n💡 This dataset preserves ALL unique physical items Penn owns\")\n",
    "print(f\"   Each physical copy/item is counted separately as intended\")\n",
    "\n",
    "# Update the unique_penn variable for downstream processing\n",
    "unique_penn = conservative_no_hsp.select(\"F001\")\n",
    "unique_penn_count = no_hsp_count\n",
    "\n",
    "# Store the baseline for use in next cell\n",
    "total_penn = conservative_unique_count  # This is the correct baseline for the rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Spark cache before Material Type Analysis\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Material Type Analysis - CORRECTED VERSION\n",
    "from pyspark.sql.functions import col, substring, when, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "print(\"=== MATERIAL TYPE ANALYSIS (CORRECTED) ===\")\n",
    "\n",
    "# Load the CORRECT filtered unique Penn F001s (not the exploded dataset!)\n",
    "print(\"\\n📂 Loading the correct filtered unique Penn F001s...\")\n",
    "\n",
    "# Check which filtered dataset is available (most filtered to least)\n",
    "filtered_paths = [\n",
    "    f\"{output_dir}/conservative_unique_penn_filtered_no_f035_hsp.parquet\",\n",
    "    f\"{output_dir}/conservative_unique_penn_filtered.parquet\",\n",
    "    f\"{output_dir}/conservative_unique_penn.parquet\"\n",
    "]\n",
    "\n",
    "unique_penn_f001s = None\n",
    "for path in filtered_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✅ Found filtered dataset: {os.path.basename(path)}\")\n",
    "        # Load just the F001s (this should be the filtered unique records)\n",
    "        temp_df = spark.read.parquet(path)\n",
    "        \n",
    "        # If this has full MARC data, extract just F001s\n",
    "        if \"F001\" in temp_df.columns:\n",
    "            unique_penn_f001s = temp_df.select(\"F001\").distinct()\n",
    "        else:\n",
    "            # This might already be just F001s\n",
    "            unique_penn_f001s = temp_df\n",
    "        break\n",
    "\n",
    "if unique_penn_f001s is None:\n",
    "    raise FileNotFoundError(\"No filtered unique Penn dataset found! Please run the filtering cells first.\")\n",
    "\n",
    "# Count the ACTUAL unique Penn records\n",
    "unique_penn_count = unique_penn_f001s.count()\n",
    "print(f\"📊 Filtered unique Penn F001s: {unique_penn_count:,}\")\n",
    "\n",
    "# Verify this is reasonable (should be < 5,362,031 after HSP filtering)\n",
    "if unique_penn_count > 5_362_031:\n",
    "    print(\"❌ ERROR: Count is too high! This appears to be the exploded dataset.\")\n",
    "    print(\"   Expected: < 5,362,031 (after HSP filtering)\")\n",
    "    print(\"   Got: {unique_penn_count:,}\")\n",
    "    raise ValueError(\"Using wrong dataset - this is the exploded dataset, not unique F001s\")\n",
    "\n",
    "# Load the FULL Penn MARC records (the original dataset with all fields)\n",
    "penn_full_path = f\"{input_dir}/penn_penn-marc21.parquet\"\n",
    "penn_full = spark.read.parquet(penn_full_path)\n",
    "print(f\"\\n✅ Loaded full Penn MARC records: {penn_full.count():,} total records\")\n",
    "\n",
    "# Check for Leader field\n",
    "if \"FLDR\" in penn_full.columns:\n",
    "    LEADER_FIELD = \"FLDR\"\n",
    "elif \"LDR\" in penn_full.columns:\n",
    "    LEADER_FIELD = \"LDR\"\n",
    "else:\n",
    "    raise ValueError(\"No Leader field found!\")\n",
    "\n",
    "print(f\"✅ Using Leader field: {LEADER_FIELD}\")\n",
    "\n",
    "# JOIN to get only the filtered unique records with full MARC data\n",
    "print(f\"\\n🔄 Joining {unique_penn_count:,} unique F001s with full MARC records...\")\n",
    "unique_penn_full = penn_full.join(unique_penn_f001s, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Verify the join\n",
    "joined_count = unique_penn_full.count()\n",
    "print(f\"✅ Joined records: {joined_count:,}\")\n",
    "\n",
    "if joined_count != unique_penn_count:\n",
    "    print(f\"⚠️ WARNING: Expected {unique_penn_count:,} but got {joined_count:,}\")\n",
    "    if joined_count > unique_penn_count:\n",
    "        print(\"   There might be duplicate F001s in penn_full. Deduplicating...\")\n",
    "        unique_penn_full = unique_penn_full.dropDuplicates([\"F001\"])\n",
    "        joined_count = unique_penn_full.count()\n",
    "        print(f\"   After dedup: {joined_count:,} records\")\n",
    "\n",
    "# Apply material type categorization\n",
    "unique_penn_with_material_type = (unique_penn_full\n",
    "    .withColumn(\"record_type\", substring(col(LEADER_FIELD), 7, 1))\n",
    "    .withColumn(\"bib_level\", substring(col(LEADER_FIELD), 8, 1))\n",
    "    .withColumn(\"material_category\", \n",
    "        when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"m\")), \"print_book\")\n",
    "        .when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"s\")), \"print_serial\")\n",
    "        .when((col(\"record_type\") == \"c\"), \"print_music\")\n",
    "        .when((col(\"record_type\") == \"e\"), \"print_maps\")\n",
    "        .when(col(\"record_type\") == \"m\", \"electronic_resource\")\n",
    "        .when(col(\"record_type\").isin(\"g\", \"k\"), \"visual_material\")\n",
    "        .when(col(\"record_type\") == \"i\", \"audio_material\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "    .withColumn(\"is_print\", \n",
    "        col(\"material_category\").isin(\"print_book\", \"print_serial\", \"print_music\", \"print_maps\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache for analysis\n",
    "unique_penn_with_material_type.cache()\n",
    "\n",
    "# Get statistics\n",
    "print(\"\\n📊 Material Type Distribution for FILTERED UNIQUE Penn records:\")\n",
    "material_stats = unique_penn_with_material_type.groupBy(\"material_category\", \"is_print\").count().collect()\n",
    "\n",
    "material_counts_dict = {}\n",
    "print_count = 0\n",
    "non_print_count = 0\n",
    "\n",
    "for row in material_stats:\n",
    "    material_counts_dict[row[\"material_category\"]] = row[\"count\"]\n",
    "    if row[\"is_print\"]:\n",
    "        print_count += row[\"count\"]\n",
    "    else:\n",
    "        non_print_count += row[\"count\"]\n",
    "\n",
    "total_unique = print_count + non_print_count\n",
    "\n",
    "# Display distribution\n",
    "for category, count in sorted(material_counts_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {category}: {count:,}\")\n",
    "\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"  Total FILTERED UNIQUE Penn records analyzed: {total_unique:,}\")\n",
    "print(f\"  Print materials: {print_count:,} ({print_count/total_unique*100:.1f}%)\")\n",
    "print(f\"  Non-print materials: {non_print_count:,} ({non_print_count/total_unique*100:.1f}%)\")\n",
    "\n",
    "# Save the correctly filtered datasets\n",
    "print(\"\\n💾 Saving corrected datasets...\")\n",
    "unique_penn_with_material_type.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn_with_material_type.parquet\")\n",
    "\n",
    "print_only_df = unique_penn_with_material_type.filter(col(\"is_print\") == True)\n",
    "print_only_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "print(f\"\\n✅ Material type analysis complete!\")\n",
    "print(f\"   Analyzed {total_unique:,} UNIQUE Penn records\")\n",
    "print(f\"   Starting from {unique_penn_count:,} filtered F001s\")\n",
    "print(f\"   (NOT the 30M exploded dataset!)\")\n",
    "\n",
    "# Unpersist\n",
    "unique_penn_with_material_type.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache and adjust broadcast settings to prevent timeout\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Disable broadcast joins temporarily to prevent timeout\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Increase broadcast timeout\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\")  # 20 minutes\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, lower, trim, concat_ws, when, regexp_extract, size, array_join, slice, split\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "from datetime import datetime\n",
    "import builtins\n",
    "\n",
    "# Define the create_match_key_spark function if not already defined\n",
    "if 'create_match_key_spark' not in globals():\n",
    "    def create_match_key_spark(df):\n",
    "        \"\"\"\n",
    "        Create match keys for the dataframe\n",
    "        \"\"\"\n",
    "        # Check if required columns exist\n",
    "        required_cols = ['F245', 'F250', 'F260', 'F264']\n",
    "        existing_cols = [col for col in required_cols if col in df.columns]\n",
    "        \n",
    "        # Extract publication year from F260 or F264\n",
    "        df = df.withColumn(\"pub_year\",\n",
    "            F.coalesce(\n",
    "                F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0),\n",
    "                    F.regexp_extract(F.col(\"F260\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "                ) if \"F260\" in df.columns else F.lit(None),\n",
    "                F.when(F.col(\"F264\").isNotNull() & (F.size(F.col(\"F264\")) > 0),\n",
    "                    F.regexp_extract(F.col(\"F264\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "                ) if \"F264\" in df.columns else F.lit(None)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Normalize title\n",
    "        df = df.withColumn(\"title_normalized\",\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.trim(F.col(\"F245\"))),\n",
    "                        \"^(the|a|an|le|la|los|las|el|die|der|das|den|det)\\\\s+\", \"\"\n",
    "                    ),\n",
    "                    \"[^a-z0-9\\\\s]\", \"\"\n",
    "                )\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "        \n",
    "        # Create match key\n",
    "        df = df.withColumn(\"match_key\", \n",
    "            F.concat_ws(\"_\",\n",
    "                F.col(\"title_normalized\"),\n",
    "                F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(\"F250\").getItem(0)), \n",
    "                        \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed|edition)\", \"$1 ed\"\n",
    "                    )\n",
    "                ).otherwise(\"\") if \"F250\" in df.columns else F.lit(\"\"),\n",
    "                F.coalesce(F.col(\"pub_year\"), F.lit(\"\"))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Define output directory if not already defined\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load print materials dataset if not already loaded\n",
    "if 'print_only_df' not in locals() or print_only_df is None:\n",
    "    print(\"Loading print materials dataset...\")\n",
    "    print_only_df_raw = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "    metadata_cols = [\"processing_date\", \"source_file\", \"data_currency_warning\"]\n",
    "    existing_metadata_cols = [col for col in metadata_cols if col in print_only_df_raw.columns]\n",
    "    if existing_metadata_cols:\n",
    "        print(f\"Dropping metadata columns: {existing_metadata_cols}\")\n",
    "        print_only_df = print_only_df_raw.drop(*existing_metadata_cols)\n",
    "    else:\n",
    "        print_only_df = print_only_df_raw\n",
    "else:\n",
    "    print(\"Using existing print_only_df DataFrame\")\n",
    "\n",
    "# Add match keys to the full print dataset using the existing function\n",
    "print_only_df_with_keys = create_match_key_spark(print_only_df)\n",
    "\n",
    "# Verify match keys were created\n",
    "match_key_stats = print_only_df_with_keys.select(\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.sum(F.when(F.col(\"match_key\").isNotNull() & (F.col(\"match_key\") != \"\"), 1).otherwise(0)).alias(\"records_with_match_key\"),\n",
    "    F.avg(F.length(\"match_key\")).alias(\"avg_match_key_length\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nMatch key generation results:\")\n",
    "print(f\"  - Total records: {match_key_stats['total_records']:,}\")\n",
    "print(f\"  - Records with match key: {match_key_stats['records_with_match_key']:,} ({match_key_stats['records_with_match_key']/match_key_stats['total_records']*100:.1f}%)\")\n",
    "print(f\"  - Average match key length: {match_key_stats['avg_match_key_length']:.1f} characters\")\n",
    "\n",
    "# Cache the full dataset for performance\n",
    "print_only_df_with_keys.cache()\n",
    "\n",
    "# Save the full print dataset with match keys\n",
    "print_only_df_with_keys.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "print(\"\\n✅ Match keys added to full print dataset (no sampling)\")\n",
    "print(f\"Results saved to {output_dir}/physical_books_no_533.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced RDA filtering with OCLC numbers included for HathiTrust checking\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import builtins\n",
    "\n",
    "# Define the extract_oclc_number_enhanced function if not already defined\n",
    "if 'extract_oclc_number_enhanced' not in globals():\n",
    "    def extract_oclc_number_enhanced(df):\n",
    "        \"\"\"\n",
    "        ENHANCED: Extract OCLC numbers from F035 field with ALL common patterns\n",
    "        Handles ocm, ocn, on prefixes and leading zeros\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"oclc_number\",\n",
    "            F.when(F.col(\"F035\").isNotNull() & (F.size(F.col(\"F035\")) > 0),\n",
    "                F.regexp_extract(\n",
    "                    F.concat_ws(\" \", F.col(\"F035\")),\n",
    "                    \"\\\\(OCoLC\\\\)(?:ocm|ocn|on)?0*([0-9]+)\",  # Handles prefixes AND leading zeros\n",
    "                    1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(\"=== EXPORTING PHYSICAL BOOKS TO EXCEL WITH ENHANCED RDA FILTERING ===\")\n",
    "print(\"Reading from existing parquet file and applying comprehensive RDA filtering...\\n\")\n",
    "\n",
    "# Read the existing physical_books_no_533.parquet\n",
    "physical_books_df = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "# Check if OCLC numbers already exist (from previous processing)\n",
    "if \"oclc_number\" in physical_books_df.columns:\n",
    "    print(\"✅ OCLC numbers already present in dataset\")\n",
    "else:\n",
    "    print(\"📋 Extracting OCLC numbers...\")\n",
    "    physical_books_df = extract_oclc_number_enhanced(physical_books_df)\n",
    "\n",
    "# Get initial count\n",
    "initial_count = physical_books_df.count()\n",
    "print(f\"Initial records: {initial_count:,}\")\n",
    "\n",
    "# Apply comprehensive RDA filtering\n",
    "print(\"\\n📋 APPLYING COMPREHENSIVE RDA FILTERING\")\n",
    "print(\"Checking multiple RDA fields with false positive prevention...\")\n",
    "\n",
    "# Create electronic filter function\n",
    "def create_electronic_filter(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive filter for electronic resources while avoiding false positives\n",
    "    \"\"\"\n",
    "    # Primary electronic indicators (high confidence)\n",
    "    primary_electronic = (\n",
    "        # 337 $a = computer (media type)\n",
    "        (F.col(\"F337\").isNotNull() & \n",
    "         F.array_contains(\n",
    "             F.transform(F.col(\"F337\"), lambda x: F.lower(x)), \n",
    "             F.lit(\"computer\")\n",
    "         )) |\n",
    "        # 338 $a = online resource or other computer carriers\n",
    "        (F.col(\"F338\").isNotNull() & \n",
    "         (F.array_contains(F.transform(F.col(\"F338\"), lambda x: F.lower(x)), F.lit(\"online\")) |\n",
    "          F.array_contains(F.transform(F.col(\"F338\"), lambda x: F.lower(x)), F.lit(\"computer disc\")) |\n",
    "          F.array_contains(F.transform(F.col(\"F338\"), lambda x: F.lower(x)), F.lit(\"computer chip\")) |\n",
    "          F.array_contains(F.transform(F.col(\"F338\"), lambda x: F.lower(x)), F.lit(\"computer tape\"))\n",
    "         ))\n",
    "    )\n",
    "    \n",
    "    # Check for electronic format in 300 field (physical description)\n",
    "    electronic_300 = F.lit(False)\n",
    "    if \"F300\" in df.columns:\n",
    "        electronic_300 = (\n",
    "            F.col(\"F300\").isNotNull() & \n",
    "            F.array_contains(\n",
    "                F.transform(F.col(\"F300\"), lambda x: F.lower(x)), \n",
    "                F.lit(\"online resource\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Combine filters with OR logic\n",
    "    return primary_electronic | electronic_300\n",
    "\n",
    "# Apply the comprehensive filter\n",
    "electronic_filter = create_electronic_filter(physical_books_df)\n",
    "physical_books_filtered = physical_books_df.filter(~electronic_filter)\n",
    "\n",
    "# Count after filtering\n",
    "filtered_count = physical_books_filtered.count()\n",
    "removed_by_rda = initial_count - filtered_count\n",
    "\n",
    "print(f\"\\nEnhanced RDA filtering results:\")\n",
    "print(f\"  - Records before RDA filter: {initial_count:,}\")\n",
    "print(f\"  - Records after RDA filter: {filtered_count:,}\")\n",
    "print(f\"  - Removed by RDA filter: {removed_by_rda:,}\")\n",
    "\n",
    "# EXTRACT OCLC NUMBERS for the filtered dataset (if not already present)\n",
    "print(\"\\n📋 Checking OCLC numbers for HathiTrust checking...\")\n",
    "physical_books_with_oclc = physical_books_filtered\n",
    "\n",
    "# Check if oclc_number already exists, if not extract it\n",
    "if \"oclc_number\" not in physical_books_with_oclc.columns:\n",
    "    print(\"  - Extracting OCLC numbers from F035 field...\")\n",
    "    physical_books_with_oclc = extract_oclc_number_enhanced(physical_books_with_oclc)\n",
    "\n",
    "# Count records with OCLC numbers\n",
    "oclc_count = physical_books_with_oclc.filter(\n",
    "    F.col(\"oclc_number\").isNotNull() & (F.col(\"oclc_number\") != \"\")\n",
    ").count()\n",
    "print(f\"  - Records with OCLC numbers: {oclc_count:,} ({oclc_count/filtered_count*100:.1f}%)\")\n",
    "\n",
    "# Prepare data for Excel export INCLUDING OCLC NUMBERS\n",
    "print(\"\\n📋 Preparing data for Excel export with OCLC numbers...\")\n",
    "physical_books_excel = physical_books_with_oclc.select(\n",
    "    \"F001\",\n",
    "    # OCLC number - ADDED FOR HATHITRUST\n",
    "    F.when(F.col(\"oclc_number\").isNotNull(), F.col(\"oclc_number\")).otherwise(\"\").alias(\"OCLC\"),\n",
    "    # F020 is an array - get first ISBN if available\n",
    "    F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0), \n",
    "           F.col(\"F020\").getItem(0)).otherwise(\"\").alias(\"ISBN\"),\n",
    "    F.col(\"F010\").alias(\"LCCN\"),\n",
    "    F.col(\"F245\").alias(\"Title\"),\n",
    "    # F250 is an array - get first edition statement if available\n",
    "    F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0), \n",
    "           F.col(\"F250\").getItem(0)).otherwise(\"\").alias(\"Edition\"),\n",
    "    # F260 is an array - get first publication info if available\n",
    "    F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0), \n",
    "           F.col(\"F260\").getItem(0)).otherwise(\"\").alias(\"Publication\"),\n",
    "    # Also check F264 for publication info if F260 is empty\n",
    "    F.when(\n",
    "        (F.col(\"F260\").isNull() | (F.size(F.col(\"F260\")) == 0)) & \n",
    "        F.col(\"F264\").isNotNull() & (F.size(F.col(\"F264\")) > 0),\n",
    "        F.col(\"F264\").getItem(0)\n",
    "    ).otherwise(\"\").alias(\"Publication_264\"),\n",
    "    \"material_category\",\n",
    "    \"match_key\"\n",
    ")\n",
    "\n",
    "# Check the size before converting to pandas\n",
    "print(f\"\\n⚠️  Dataset has {filtered_count:,} records - checking Excel limits...\")\n",
    "\n",
    "# Excel has a limit of 1,048,576 rows\n",
    "EXCEL_ROW_LIMIT = 1048576\n",
    "\n",
    "if filtered_count > EXCEL_ROW_LIMIT:\n",
    "    print(f\"❌ Dataset too large for single Excel file ({filtered_count:,} > {EXCEL_ROW_LIMIT:,})\")\n",
    "    print(\"\\n📋 OPTIONS:\")\n",
    "    \n",
    "    # Option 1: Sample the data\n",
    "    print(\"\\n1️⃣ OPTION 1: Create a sample Excel file (first 1M records)\")\n",
    "    sample_df = physical_books_excel.limit(EXCEL_ROW_LIMIT - 1)  # -1 for header\n",
    "    pandas_df_sample = sample_df.toPandas()\n",
    "    \n",
    "    # Combine Publication columns\n",
    "    if 'Publication_264' in pandas_df_sample.columns:\n",
    "        pandas_df_sample['Publication'] = pandas_df_sample.apply(\n",
    "            lambda row: row['Publication'] if row['Publication'] else row['Publication_264'], \n",
    "            axis=1\n",
    "        )\n",
    "        pandas_df_sample = pandas_df_sample.drop('Publication_264', axis=1)\n",
    "    \n",
    "    # Save sample to Excel\n",
    "    excel_sample_path = f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc_SAMPLE.xlsx\"\n",
    "    print(f\"   Writing sample to: {excel_sample_path}\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_sample_path, engine='openpyxl') as writer:\n",
    "        pandas_df_sample.to_excel(writer, sheet_name='Physical Books Sample', index=False)\n",
    "        \n",
    "        # Auto-adjust column widths\n",
    "        worksheet = writer.sheets['Physical Books Sample']\n",
    "        for column in pandas_df_sample:\n",
    "            column_length = builtins.max(pandas_df_sample[column].astype(str).map(len).max(), len(column))\n",
    "            col_idx = pandas_df_sample.columns.get_loc(column)\n",
    "            if col_idx < 26:  # Only handle first 26 columns (A-Z)\n",
    "                worksheet.column_dimensions[chr(65 + col_idx)].width = builtins.min(column_length + 2, 50)\n",
    "    \n",
    "    print(f\"   ✅ Sample Excel created with {len(pandas_df_sample):,} records\")\n",
    "    \n",
    "    # Option 2: Save as CSV (no row limit)\n",
    "    print(\"\\n2️⃣ OPTION 2: Save full dataset as CSV (no row limit)\")\n",
    "    csv_path = f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc.csv\"\n",
    "    \n",
    "    # Convert to pandas in chunks to avoid memory issues\n",
    "    print(f\"   Converting to CSV: {csv_path}\")\n",
    "    pandas_df_full = physical_books_excel.toPandas()\n",
    "    \n",
    "    # Combine Publication columns\n",
    "    if 'Publication_264' in pandas_df_full.columns:\n",
    "        pandas_df_full['Publication'] = pandas_df_full.apply(\n",
    "            lambda row: row['Publication'] if row['Publication'] else row['Publication_264'], \n",
    "            axis=1\n",
    "        )\n",
    "        pandas_df_full = pandas_df_full.drop('Publication_264', axis=1)\n",
    "    \n",
    "    pandas_df_full.to_csv(csv_path, index=False)\n",
    "    print(f\"   ✅ CSV created with all {len(pandas_df_full):,} records\")\n",
    "    \n",
    "    # Always save the parquet version\n",
    "    print(\"\\n3️⃣ OPTION 3: Full dataset saved as Parquet (recommended for large data)\")\n",
    "    parquet_path = f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc.parquet\"\n",
    "    physical_books_with_oclc.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    print(f\"   ✅ Parquet saved: {parquet_path}\")\n",
    "    \n",
    "    print(f\"\\n📊 SUMMARY:\")\n",
    "    print(f\"   - Total records: {filtered_count:,}\")\n",
    "    print(f\"   - Records with OCLC: {oclc_count:,}\")\n",
    "    print(f\"   - Electronic resources removed: {removed_by_rda:,}\")\n",
    "    print(f\"   - Sample Excel: {excel_sample_path} (first 1M records)\")\n",
    "    print(f\"   - Full CSV: {csv_path} (all records)\")\n",
    "    print(f\"   - Full Parquet: {parquet_path} (all records)\")\n",
    "    \n",
    "else:\n",
    "    # Dataset fits in Excel - proceed normally\n",
    "    print(f\"✅ Dataset fits in Excel ({filtered_count:,} < {EXCEL_ROW_LIMIT:,})\")\n",
    "    \n",
    "    # Convert to Pandas DataFrame\n",
    "    print(\"\\nConverting to Pandas DataFrame...\")\n",
    "    pandas_df = physical_books_excel.toPandas()\n",
    "    \n",
    "    # Combine Publication and Publication_264 if needed\n",
    "    if 'Publication_264' in pandas_df.columns:\n",
    "        pandas_df['Publication'] = pandas_df.apply(\n",
    "            lambda row: row['Publication'] if row['Publication'] else row['Publication_264'], \n",
    "            axis=1\n",
    "        )\n",
    "        pandas_df = pandas_df.drop('Publication_264', axis=1)\n",
    "    \n",
    "    # Save to Excel\n",
    "    excel_path = f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc.xlsx\"\n",
    "    print(f\"\\nWriting to Excel file: {excel_path}\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        pandas_df.to_excel(writer, sheet_name='Physical Books', index=False)\n",
    "        \n",
    "        # Auto-adjust column widths\n",
    "        worksheet = writer.sheets['Physical Books']\n",
    "        for column in pandas_df:\n",
    "            column_length = builtins.max(pandas_df[column].astype(str).map(len).max(), len(column))\n",
    "            col_idx = pandas_df.columns.get_loc(column)\n",
    "            if col_idx < 26:  # Only handle first 26 columns (A-Z)\n",
    "                worksheet.column_dimensions[chr(65 + col_idx)].width = builtins.min(column_length + 2, 50)\n",
    "    \n",
    "    print(f\"\\n✅ Enhanced Excel export complete with OCLC numbers!\")\n",
    "    print(f\"   - Output file: {excel_path}\")\n",
    "    print(f\"   - Records exported: {filtered_count:,}\")\n",
    "    print(f\"   - Records with OCLC: {oclc_count:,}\")\n",
    "    print(f\"   - Electronic resources removed: {removed_by_rda:,}\")\n",
    "    \n",
    "    # Also save a parquet version\n",
    "    if removed_by_rda > 0 or \"oclc_number\" not in physical_books_filtered.columns:\n",
    "        print(f\"\\n📋 Saving enhanced RDA-filtered data with OCLC to parquet...\")\n",
    "        physical_books_with_oclc.write.mode(\"overwrite\").parquet(\n",
    "            f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc.parquet\"\n",
    "        )\n",
    "        print(f\"   - Updated parquet: {output_dir}/physical_books_no_533_no_electronic_with_oclc.parquet\")\n",
    "\n",
    "# Show sample of records with OCLC numbers (works for both cases)\n",
    "print(\"\\n📋 Sample records with OCLC numbers:\")\n",
    "sample_preview = physical_books_excel.filter(F.col(\"OCLC\") != \"\").limit(5).toPandas()\n",
    "if len(sample_preview) > 0:\n",
    "    print(sample_preview[['F001', 'OCLC', 'ISBN', 'Title']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No records with OCLC numbers found in sample\")\n",
    "\n",
    "print(\"\\n📋 FINAL OUTPUT INCLUDES:\")\n",
    "print(\"✅ F001 (MMS ID)\")\n",
    "print(\"✅ OCLC (for HathiTrust checking)\")\n",
    "print(\"✅ ISBN\")\n",
    "print(\"✅ LCCN\")\n",
    "print(\"✅ Title\")\n",
    "print(\"✅ Edition\")\n",
    "print(\"✅ Publication\")\n",
    "print(\"✅ Material Category\")\n",
    "print(\"✅ Match Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety: clear Spark cache if Spark is available (prevents stale cached DataFrames)\n",
    "try:\n",
    "    _ = spark\n",
    "    spark.catalog.clearCache()\n",
    "    print(\"🧹 Spark cache cleared\")\n",
    "except NameError:\n",
    "    # Spark not in this kernel/session\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipping the Harvard API check in favor of BD and Alma APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvard API prep: build worklist using prior Harvard results as exclusion (no API calls yet)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARVARD API PREP — EXCLUDE PRIOR RESULTS (NO NETWORK CALLS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import csv as _csv\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Control flags\n",
    "FORCE_FULL_RUN = False                 # If True, ignore exclusions and process all current records\n",
    "USE_BASELINE_FOR_DELTA = False         # If True, also subtract a July baseline; default False per user request\n",
    "\n",
    "# Paths\n",
    "EXACT_BASE_DIR = '/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs'\n",
    "EXACT_CURRENT_PATH = os.path.join(EXACT_BASE_DIR, 'physical_books_no_533_no_electronic_with_oclc.csv')\n",
    "EXACT_BASELINE_PATH = os.path.join(EXACT_BASE_DIR, 'physical_books_no_533_with_match_keys.csv')\n",
    "\n",
    "# Helper: ensure F001 exists and standard columns are present\n",
    "\n",
    "def _normalize_id_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    if 'f001' in cols:\n",
    "        df['F001'] = df[cols['f001']].astype(str).str.strip()\n",
    "        return df\n",
    "    for candidate in ['mmsid', 'mms_id', 'mms id', 'id']:\n",
    "        if candidate in cols:\n",
    "            df['F001'] = df[cols[candidate]].astype(str).str.strip()\n",
    "            return df\n",
    "    if 'F001' not in df.columns:\n",
    "        df['F001'] = ''\n",
    "    return df\n",
    "\n",
    "\n",
    "def _coerce_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _normalize_id_column(df)\n",
    "    for col in ['OCLC', 'ISBN', 'Title']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "    for col in ['F001', 'OCLC', 'ISBN']:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def _read_any_table(path: str, usecols=None) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in ['.csv', '.txt', '.tsv']:\n",
    "        sep = ',' if ext == '.csv' else '\\t'\n",
    "        if usecols and isinstance(usecols, (list, set)):\n",
    "            wanted = {c.strip().lower() for c in usecols}\n",
    "            usecols_callable = lambda c: c is not None and c.strip().lower() in wanted\n",
    "        else:\n",
    "            usecols_callable = usecols\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                path, sep=sep, usecols=usecols_callable, dtype=str, low_memory=False,\n",
    "                quotechar='\"', escapechar='\\\\', engine='c', on_bad_lines='skip'\n",
    "            )\n",
    "        except Exception:\n",
    "            return pd.read_csv(\n",
    "                path, sep=sep, dtype=str, low_memory=False,\n",
    "                quotechar='\"', escapechar='\\\\', engine='c', on_bad_lines='skip'\n",
    "            )\n",
    "    if ext in ['.parquet']:\n",
    "        return pd.read_parquet(path, columns=usecols if usecols else None)\n",
    "    if ext in ['.xlsx', '.xls']:\n",
    "        return pd.read_excel(path, usecols=usecols, dtype=str)\n",
    "    raise ValueError(f\"Unsupported file type: {path}\")\n",
    "\n",
    "\n",
    "def _count_physical_lines(path: str, max_lines: int | None = None) -> int:\n",
    "    cnt = 0\n",
    "    with open(path, 'rb') as f:\n",
    "        for i, _ in enumerate(f, 1):\n",
    "            if max_lines and i >= max_lines:\n",
    "                return i\n",
    "        cnt = i if 'i' in locals() else 0\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def load_current_dataset(base_dir: str) -> pd.DataFrame:\n",
    "    p = EXACT_CURRENT_PATH\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(p)\n",
    "    print(f\"📄 Current dataset: {p}\")\n",
    "    print(f\"   (physical lines ≈ {_count_physical_lines(p):,})\")\n",
    "    df = _read_any_table(p, usecols=['F001', 'OCLC', 'ISBN', 'Title'])\n",
    "    df = _coerce_columns(df)\n",
    "    df = df[['F001', 'OCLC', 'ISBN', 'Title']].dropna(subset=['F001'])\n",
    "    print(f\"   → Loaded {len(df):,} rows (logical records)\")\n",
    "    print(f\"   → Unique MMS IDs: {df['F001'].nunique():,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_baseline_dataset(base_dir: str) -> pd.DataFrame:\n",
    "    p = EXACT_BASELINE_PATH\n",
    "    if not os.path.exists(p):\n",
    "        return pd.DataFrame({'F001': []})\n",
    "    print(f\"📄 Baseline dataset: {p}\")\n",
    "    print(f\"   (physical lines ≈ {_count_physical_lines(p):,})\")\n",
    "    df = _read_any_table(p, usecols={'F001', 'mmsid', 'mms_id', 'mms id', 'id'})\n",
    "    df = _coerce_columns(df)\n",
    "    df = df[['F001']].dropna(subset=['F001']).drop_duplicates()\n",
    "    print(f\"   → Unique MMS IDs (baseline): {df['F001'].nunique():,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _add_ids_from_json_file(path: str, already: set):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # Flexible extraction\n",
    "        def collect(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                # Common fields: 'F001', 'mms_id', 'id'\n",
    "                for k in ['F001','mms_id','mmsid','id']:\n",
    "                    if k in obj and obj[k]:\n",
    "                        already.add(str(obj[k]).strip())\n",
    "                for v in obj.values():\n",
    "                    collect(v)\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    collect(item)\n",
    "        collect(data)\n",
    "        print(f\"   → Added IDs from {os.path.basename(path)} (total now {len(already):,})\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ! Failed to parse {os.path.basename(path)}: {e}\")\n",
    "\n",
    "\n",
    "def _add_ids_from_pickle(path: str, already: set):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        # Try common shapes: list[dict], dict[str, Any], set/list[str]\n",
    "        if isinstance(obj, (set, list)):\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict):\n",
    "                    val = item.get('F001') or item.get('mms_id') or item.get('id')\n",
    "                    if val:\n",
    "                        already.add(str(val).strip())\n",
    "                elif isinstance(item, (str, int)):\n",
    "                    already.add(str(item).strip())\n",
    "        elif isinstance(obj, dict):\n",
    "            for v in obj.values():\n",
    "                if isinstance(v, (list, set)):\n",
    "                    for item in v:\n",
    "                        if isinstance(item, dict):\n",
    "                            val = item.get('F001') or item.get('mms_id') or item.get('id')\n",
    "                            if val:\n",
    "                                already.add(str(val).strip())\n",
    "                        elif isinstance(item, (str, int)):\n",
    "                            already.add(str(item).strip())\n",
    "        print(f\"   → Added IDs from {os.path.basename(path)} (total now {len(already):,})\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ! Failed to parse pickle {os.path.basename(path)}: {e}\")\n",
    "\n",
    "\n",
    "def load_already_checked_ids(base_dir: str) -> set:\n",
    "    already = set()\n",
    "    # Known Harvard outputs to use as exclusion sources\n",
    "    files = [\n",
    "        os.path.join(base_dir, 'harvard_api_full_checkpoint.json'),\n",
    "        os.path.join(base_dir, 'harvard_api_full_results.csv'),\n",
    "        os.path.join(base_dir, 'harvard_api_full_results.parquet'),\n",
    "        os.path.join(base_dir, 'harvard_api_results_complete_*.csv'),\n",
    "        os.path.join(base_dir, 'harvard_check_results_verified_fixed.json'),\n",
    "        os.path.join(base_dir, 'harvard_check_results_verified.json'),\n",
    "        os.path.join(base_dir, 'harvard_check_results.json'),\n",
    "        os.path.join(base_dir, 'harvard_check_checkpoint_v3.pkl'),\n",
    "    ]\n",
    "\n",
    "    for p in files:\n",
    "        if '*' in p:\n",
    "            for csv_path in glob.glob(p):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, dtype=str, low_memory=False)\n",
    "                    df = _normalize_id_column(df)\n",
    "                    ids = set(df['F001'].astype(str).str.strip())\n",
    "                    already |= ids\n",
    "                    print(f\"   → Prior results exclude: {len(ids):,} from {os.path.basename(csv_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ! Failed to read {os.path.basename(csv_path)}: {e}\")\n",
    "            continue\n",
    "        if not os.path.exists(p):\n",
    "            continue\n",
    "        ext = os.path.splitext(p)[1].lower()\n",
    "        if ext == '.json':\n",
    "            _add_ids_from_json_file(p, already)\n",
    "        elif ext == '.csv':\n",
    "            try:\n",
    "                df = pd.read_csv(p, dtype=str, low_memory=False)\n",
    "                df = _normalize_id_column(df)\n",
    "                ids = set(df['F001'].astype(str).str.strip())\n",
    "                already |= ids\n",
    "                print(f\"   → Prior results exclude: {len(ids):,} from {os.path.basename(p)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ! Failed to read {os.path.basename(p)}: {e}\")\n",
    "        elif ext == '.parquet':\n",
    "            try:\n",
    "                df = pd.read_parquet(p, columns=None)\n",
    "                df = _normalize_id_column(df)\n",
    "                ids = set(df['F001'].astype(str).str.strip())\n",
    "                already |= ids\n",
    "                print(f\"   → Prior results exclude: {len(ids):,} from {os.path.basename(p)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ! Failed to read {os.path.basename(p)}: {e}\")\n",
    "        elif ext == '.pkl':\n",
    "            _add_ids_from_pickle(p, already)\n",
    "    return already\n",
    "\n",
    "# 1) Load current inputs\n",
    "current_df = load_current_dataset(EXACT_BASE_DIR)\n",
    "current_ids = set(current_df['F001'].astype(str))\n",
    "\n",
    "# 2) Build exclusion set from prior Harvard results\n",
    "print(\"\\nGathering already-processed MMS IDs from prior Harvard outputs…\")\n",
    "already_checked = load_already_checked_ids(EXACT_BASE_DIR)\n",
    "print(f\"   Total already-checked IDs: {len(already_checked):,}\")\n",
    "\n",
    "# 3) Optional: load baseline if requested (off by default)\n",
    "baseline_ids = set()\n",
    "if USE_BASELINE_FOR_DELTA and not FORCE_FULL_RUN:\n",
    "    baseline_df = load_baseline_dataset(EXACT_BASE_DIR)\n",
    "    baseline_ids = set(baseline_df['F001'].astype(str)) if len(baseline_df) else set()\n",
    "\n",
    "# 4) Compute worklist according to flags\n",
    "if FORCE_FULL_RUN:\n",
    "    work_ids = current_ids\n",
    "    mode = 'full-run (ignore prior results/baseline)'\n",
    "elif USE_BASELINE_FOR_DELTA and baseline_ids:\n",
    "    # subtract both July baseline and prior results\n",
    "    work_ids = (current_ids - baseline_ids) - already_checked\n",
    "    mode = 'delta vs July baseline minus prior results'\n",
    "else:\n",
    "    # default: October-only style — subtract prior results only\n",
    "    work_ids = current_ids - already_checked\n",
    "    mode = 'current minus prior Harvard results (October-only)'\n",
    "\n",
    "print(\"\\n📊 Worklist composition:\")\n",
    "print(f\"   Current unique MMS IDs:  {len(current_ids):,}\")\n",
    "if USE_BASELINE_FOR_DELTA and baseline_ids:\n",
    "    print(f\"   Baseline unique MMS IDs: {len(baseline_ids):,}\")\n",
    "print(f\"   Already-checked IDs:     {len(already_checked):,}\")\n",
    "print(f\"   Mode:                    {mode}\")\n",
    "print(f\"   Final to-check:          {len(work_ids):,} IDs\")\n",
    "\n",
    "# 5) Persist worklist\n",
    "worklist_df = current_df[current_df['F001'].isin(work_ids)].copy()\n",
    "worklist_df = worklist_df[['F001', 'OCLC', 'ISBN', 'Title']].drop_duplicates('F001')\n",
    "\n",
    "print(\"\\n✅ HARVARD WORKLIST READY\")\n",
    "print(f\"   Final to-check count: {len(worklist_df):,} MMS IDs\")\n",
    "\n",
    "worklist_path = os.path.join(EXACT_BASE_DIR, 'harvard_api_worklist.csv')\n",
    "worklist_df.to_csv(worklist_path, index=False)\n",
    "print(f\"   Saved worklist CSV: {worklist_path}\")\n",
    "\n",
    "sample_path = os.path.join(EXACT_BASE_DIR, 'generated_api_sample.csv')\n",
    "worklist_df.sample(min(1000, len(worklist_df)), random_state=42).to_csv(sample_path, index=False)\n",
    "print(f\"   Saved sample:       {sample_path}\")\n",
    "\n",
    "manifest = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'mode': mode,\n",
    "    'current_count_unique': len(current_ids),\n",
    "    'baseline_count_unique': len(baseline_ids) if (USE_BASELINE_FOR_DELTA and baseline_ids) else None,\n",
    "    'already_checked_excluded': len(already_checked),\n",
    "    'final_to_check': len(worklist_df),\n",
    "    'inputs': {\n",
    "        'current_path': EXACT_CURRENT_PATH,\n",
    "        'baseline_path': EXACT_BASELINE_PATH if USE_BASELINE_FOR_DELTA else None,\n",
    "    },\n",
    "    'artifacts': {\n",
    "        'worklist_csv': 'harvard_api_worklist.csv',\n",
    "        'sample_csv': 'generated_api_sample.csv'\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(EXACT_BASE_DIR, 'harvard_api_prep_manifest.json'), 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"   Saved manifest:     {os.path.join(EXACT_BASE_DIR, 'harvard_api_prep_manifest.json')}\")\n",
    "\n",
    "print(\"\\n💡 Next: run the Harvard API against harvard_api_worklist.csv with checkpointing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Check that these Penn MMSIDs exist and are not held in HSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alma API check for Penn-unique items (using reconstructed worklist)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALMA API CHECK FOR PENN-UNIQUE ITEMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Configuration flags\n",
    "RUN_MODE = \"full\"  # \"demo\" or \"full\"\n",
    "CHECKPOINT_INTERVAL = 100  # Save progress every N records\n",
    "RATE_LIMIT_DELAY = 0.05  # Delay between API calls in seconds\n",
    "\n",
    "# Use the RECONSTRUCTED worklist as input (the correct filtered dataset)\n",
    "worklist_path = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/harvard_api_worklist_reconstructed.csv\"\n",
    "\n",
    "class AlmaAPIChecker:\n",
    "    \"\"\"Check if MMS IDs exist in Penn's Alma catalog using SRU API\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://upenn.alma.exlibrisgroup.com/view/sru/01UPENN_INST\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Penn-Library-Research/1.0',\n",
    "            'Accept': 'application/xml'\n",
    "        })\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = RATE_LIMIT_DELAY\n",
    "        \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Enforce rate limiting between API calls\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit_delay:\n",
    "            time.sleep(self.rate_limit_delay - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def check_mms_exists(self, mms_id):\n",
    "        \"\"\"\n",
    "        Check if an MMS ID exists in Alma using SRU\n",
    "        Returns: (exists, record_count, error_message)\n",
    "        \"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        try:\n",
    "            # Clean the MMS ID - ensure it's a string\n",
    "            mms_id_clean = str(mms_id).strip()\n",
    "            \n",
    "            # Construct SRU query for MMS ID\n",
    "            query = f'alma.mms_id={mms_id_clean}'\n",
    "            \n",
    "            params = {\n",
    "                'version': '1.2',\n",
    "                'operation': 'searchRetrieve',\n",
    "                'query': query,\n",
    "                'maximumRecords': '1',\n",
    "                'recordSchema': 'marcxml'\n",
    "            }\n",
    "            \n",
    "            response = self.session.get(self.base_url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Parse XML response\n",
    "                root = ET.fromstring(response.content)\n",
    "                \n",
    "                # Define namespace\n",
    "                ns = {\n",
    "                    'srw': 'http://www.loc.gov/zing/srw/',\n",
    "                    'marc': 'http://www.loc.gov/MARC21/slim'\n",
    "                }\n",
    "                \n",
    "                # Get number of records\n",
    "                num_records_elem = root.find('.//srw:numberOfRecords', ns)\n",
    "                if num_records_elem is not None:\n",
    "                    num_records = int(num_records_elem.text)\n",
    "                    exists = num_records > 0\n",
    "                    return exists, num_records, None\n",
    "                else:\n",
    "                    return False, 0, \"Could not parse response\"\n",
    "                    \n",
    "            else:\n",
    "                return False, 0, f\"HTTP {response.status_code}\"\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            return False, 0, \"Timeout\"\n",
    "        except ET.ParseError as e:\n",
    "            return False, 0, f\"XML parse error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, 0, str(e)\n",
    "    \n",
    "    def check_batch(self, mms_ids, checkpoint_file=None):\n",
    "        \"\"\"\n",
    "        Check a batch of MMS IDs with checkpoint support\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Load checkpoint if exists\n",
    "        start_idx = 0\n",
    "        if checkpoint_file and os.path.exists(checkpoint_file):\n",
    "            try:\n",
    "                with open(checkpoint_file, 'r') as f:\n",
    "                    checkpoint = json.load(f)\n",
    "                    start_idx = checkpoint.get('last_index', 0)\n",
    "                    results = checkpoint.get('results', [])\n",
    "                print(f\"   ↩️  Resuming from checkpoint at index {start_idx}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not load checkpoint: {e}\")\n",
    "        \n",
    "        # Process MMS IDs\n",
    "        total = len(mms_ids)\n",
    "        for i, mms_id in enumerate(mms_ids[start_idx:], start=start_idx):\n",
    "            exists, count, error = self.check_mms_exists(mms_id)\n",
    "            \n",
    "            result = {\n",
    "                'F001': str(mms_id),  # Ensure it's a string\n",
    "                'exists_in_alma': exists,\n",
    "                'record_count': count,\n",
    "                'error': error,\n",
    "                'checked_at': datetime.now().isoformat()\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Show progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                exists_count = sum(1 for r in results if r['exists_in_alma'])\n",
    "                print(f\"   Progress: {i+1}/{total} - Found in Alma: {exists_count}/{i+1} ({exists_count/(i+1)*100:.1f}%)\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if checkpoint_file and (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                checkpoint = {\n",
    "                    'last_index': i + 1,\n",
    "                    'results': results,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    json.dump(checkpoint, f, indent=2)\n",
    "                print(f\"   💾 Checkpoint saved at index {i+1}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"\\n🔍 Starting Alma SRU verification using reconstructed worklist...\")\n",
    "\n",
    "# Check if the reconstructed worklist exists\n",
    "if not os.path.exists(worklist_path):\n",
    "    print(f\"❌ Reconstructed worklist not found: {worklist_path}\")\n",
    "    print(\"   Looking for alternative files...\")\n",
    "    \n",
    "    # Try alternative locations\n",
    "    alternative_files = [\n",
    "        f\"{output_dir}/harvard_api_worklist_reconstructed.csv\",\n",
    "        f\"{output_dir}/harvard_api_worklist.csv\",\n",
    "        f\"{output_dir}/physical_books_no_533_no_electronic_with_oclc.csv\"\n",
    "    ]\n",
    "    \n",
    "    for alt_file in alternative_files:\n",
    "        if os.path.exists(alt_file):\n",
    "            worklist_path = alt_file\n",
    "            print(f\"   ✅ Using: {alt_file}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No suitable worklist file found!\")\n",
    "\n",
    "# Load the worklist CSV file\n",
    "print(f\"\\n📂 Loading reconstructed worklist from: {worklist_path}\")\n",
    "print(f\"   File size: {os.path.getsize(worklist_path) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Read CSV file with proper dtype specification\n",
    "df = pd.read_csv(worklist_path, dtype={'F001': str})\n",
    "\n",
    "# Ensure F001 is string type to avoid merge conflicts\n",
    "if 'F001' in df.columns:\n",
    "    df['F001'] = df['F001'].astype(str)\n",
    "    print(f\"✅ Loaded {len(df):,} Penn-unique items from reconstructed worklist\")\n",
    "    print(f\"   Columns available: {', '.join(df.columns[:10])}...\")\n",
    "    \n",
    "    # This should be the CORRECT number (not 30M!)\n",
    "    if len(df) > 10_000_000:\n",
    "        print(f\"\\n⚠️ WARNING: Dataset seems too large ({len(df):,} records)\")\n",
    "        print(\"   This might be the exploded dataset, not the filtered unique records!\")\n",
    "        response = input(\"Continue anyway? (yes/no): \")\n",
    "        if response.lower() != 'yes':\n",
    "            raise ValueError(\"Aborted - dataset appears to be incorrect\")\n",
    "else:\n",
    "    print(\"❌ F001 column not found in data!\")\n",
    "    raise ValueError(\"F001 column is required\")\n",
    "\n",
    "# Show sample of data to verify\n",
    "print(\"\\n📋 Sample of worklist data:\")\n",
    "print(df[['F001'] + [col for col in ['OCLC', 'ISBN', 'Title'] if col in df.columns]].head(3))\n",
    "\n",
    "# Determine how many records to check\n",
    "if RUN_MODE == \"demo\":\n",
    "    df_to_check = df.head(100)\n",
    "    print(f\"\\n🧪 DEMO MODE: Checking first 100 records only\")\n",
    "else:\n",
    "    df_to_check = df\n",
    "    print(f\"\\n🚀 FULL RUN: Checking all {len(df_to_check):,} records\")\n",
    "    print(f\"   This is the CORRECT filtered dataset, not the 30M exploded records\")\n",
    "\n",
    "# Get MMS IDs to check (ensure they're strings)\n",
    "mms_ids = df_to_check['F001'].astype(str).tolist()\n",
    "\n",
    "# Initialize API checker\n",
    "checker = AlmaAPIChecker()\n",
    "\n",
    "# Set checkpoint file\n",
    "checkpoint_file = f\"{output_dir}/alma_check_checkpoint.json\"\n",
    "\n",
    "print(f\"\\n📡 Checking {len(mms_ids):,} MMS IDs in Alma...\")\n",
    "print(f\"   Rate limit: {RATE_LIMIT_DELAY} seconds between calls\")\n",
    "print(f\"   Estimated time: {len(mms_ids) * RATE_LIMIT_DELAY / 60:.1f} minutes\")\n",
    "print(f\"   Estimated time: {len(mms_ids) * RATE_LIMIT_DELAY / 3600:.1f} hours\")\n",
    "\n",
    "# Check all MMS IDs\n",
    "start_time = time.time()\n",
    "results = checker.check_batch(mms_ids, checkpoint_file)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Convert results to DataFrame with consistent string types\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['F001'] = results_df['F001'].astype(str)\n",
    "\n",
    "# Ensure df_to_check also has F001 as string before merge\n",
    "df_to_check['F001'] = df_to_check['F001'].astype(str)\n",
    "\n",
    "# Merge with original data\n",
    "df_with_alma = df_to_check.merge(\n",
    "    results_df[['F001', 'exists_in_alma', 'record_count', 'error']],\n",
    "    on='F001',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "total_checked = len(results_df)\n",
    "exists_count = results_df['exists_in_alma'].sum()\n",
    "missing_count = total_checked - exists_count\n",
    "error_count = results_df['error'].notna().sum()\n",
    "\n",
    "print(f\"\\n📊 Alma Verification Results:\")\n",
    "print(f\"   Total checked: {total_checked:,}\")\n",
    "print(f\"   Exists in Alma: {exists_count:,} ({exists_count/total_checked*100:.1f}%)\")\n",
    "print(f\"   NOT in Alma: {missing_count:,} ({missing_count/total_checked*100:.1f}%)\")\n",
    "print(f\"   Errors: {error_count:,}\")\n",
    "print(f\"   Time taken: {elapsed/60:.1f} minutes ({elapsed/3600:.1f} hours)\")\n",
    "\n",
    "# Filter to only records that DO exist in Alma\n",
    "df_alma_verified = df_with_alma[df_with_alma['exists_in_alma'] == True].copy()\n",
    "\n",
    "print(f\"\\n✅ Records verified to exist in Alma: {len(df_alma_verified):,}\")\n",
    "print(f\"   These are Penn-unique and EXIST in Alma\")\n",
    "\n",
    "# Check for HSP holdings\n",
    "hsp_file = \"/home/jovyan/work/July-2025-PODParquet/hsp_removed_mmsid.txt\"\n",
    "if os.path.exists(hsp_file):\n",
    "    with open(hsp_file, 'r') as f:\n",
    "        hsp_mms_ids = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    df_alma_verified['is_hsp'] = df_alma_verified['F001'].isin(hsp_mms_ids)\n",
    "    non_hsp = df_alma_verified[df_alma_verified['is_hsp'] == False]\n",
    "    print(f\"   After removing HSP holdings: {len(non_hsp):,}\")\n",
    "    df_final = non_hsp\n",
    "else:\n",
    "    df_final = df_alma_verified\n",
    "    print(\"   (HSP check not applied - file not found)\")\n",
    "\n",
    "# Prepare data for Excel export\n",
    "# Select relevant columns if they exist\n",
    "export_cols = ['F001']\n",
    "optional_cols = ['OCLC', 'ISBN', 'Title', 'F020', 'F035', 'F245', 'F010', 'F250', 'F260', 'F264', \n",
    "                 'material_category', 'match_key', 'oclc_number', \n",
    "                 'exists_in_alma', 'is_hsp']\n",
    "\n",
    "for col in optional_cols:\n",
    "    if col in df_final.columns:\n",
    "        export_cols.append(col)\n",
    "\n",
    "df_export = df_final[export_cols].copy()\n",
    "\n",
    "# Add readable column names\n",
    "rename_map = {\n",
    "    'F001': 'MMS_ID',\n",
    "    'F020': 'ISBN',\n",
    "    'F035': 'System_Number',\n",
    "    'F245': 'Title',\n",
    "    'F010': 'LCCN',\n",
    "    'F250': 'Edition',\n",
    "    'F260': 'Publication',\n",
    "    'F264': 'Publication_264',\n",
    "    'oclc_number': 'OCLC'\n",
    "}\n",
    "\n",
    "df_export = df_export.rename(columns={k: v for k, v in rename_map.items() if k in df_export.columns})\n",
    "\n",
    "# Save results\n",
    "output_excel = f\"{output_dir}/physical_books_alma_verified_from_worklist.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "    # Full results with Alma status\n",
    "    df_with_alma.to_excel(writer, sheet_name='All Checked', index=False)\n",
    "    \n",
    "    # Only Alma-verified records\n",
    "    df_export.to_excel(writer, sheet_name='Alma Verified', index=False)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Total items in worklist',\n",
    "            'Checked in Alma',\n",
    "            'Exists in Alma',\n",
    "            'NOT in Alma',\n",
    "            'Final verified count (after HSP removal)'\n",
    "        ],\n",
    "        'Count': [\n",
    "            len(df),\n",
    "            total_checked,\n",
    "            exists_count,\n",
    "            missing_count,\n",
    "            len(df_final)\n",
    "        ],\n",
    "        'Percentage': [\n",
    "            100.0,\n",
    "            100.0,\n",
    "            exists_count/total_checked*100 if total_checked > 0 else 0,\n",
    "            missing_count/total_checked*100 if total_checked > 0 else 0,\n",
    "            len(df_final)/len(df)*100\n",
    "        ]\n",
    "    })\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Records NOT in Alma (for investigation)\n",
    "    df_missing = df_with_alma[df_with_alma['exists_in_alma'] == False]\n",
    "    if len(df_missing) > 0:\n",
    "        df_missing.to_excel(writer, sheet_name='Not In Alma', index=False)\n",
    "        print(f\"\\n⚠️  {len(df_missing):,} records NOT found in Alma saved to 'Not In Alma' sheet\")\n",
    "\n",
    "print(f\"\\n💾 Results saved to: {output_excel}\")\n",
    "print(f\"   Sheet 'Alma Verified' contains {len(df_final):,} records\")\n",
    "print(f\"   Input was the reconstructed worklist (NOT the 30M exploded dataset)\")\n",
    "\n",
    "# Save results to JSON for record keeping\n",
    "results_json = f\"{output_dir}/alma_check_results_from_worklist.json\"\n",
    "with open(results_json, 'w') as f:\n",
    "    json.dump({\n",
    "        'check_date': datetime.now().isoformat(),\n",
    "        'input_file': worklist_path,\n",
    "        'total_in_worklist': len(df),\n",
    "        'total_checked': total_checked,\n",
    "        'exists_in_alma': int(exists_count),\n",
    "        'not_in_alma': int(missing_count),\n",
    "        'errors': int(error_count),\n",
    "        'time_taken_minutes': elapsed/60,\n",
    "        'run_mode': RUN_MODE\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"📋 Check summary saved to: {results_json}\")\n",
    "\n",
    "# Clean up checkpoint file if successful\n",
    "if os.path.exists(checkpoint_file):\n",
    "    os.remove(checkpoint_file)\n",
    "    print(\"🧹 Checkpoint file cleaned up\")\n",
    "\n",
    "print(\"\\n✅ Alma verification complete!\")\n",
    "print(f\"   Next step: Run BorrowDirect check on the {len(df_final):,} Alma-verified records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install selenium if not already installed\n",
    "try:\n",
    "    import selenium\n",
    "    print(\"✅ Selenium is installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing selenium...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"selenium\"])\n",
    "    import selenium\n",
    "    print(\"✅ Selenium installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BorrowDirect Phase 1: Collect candidate BD record IDs (idempotent; safe to re-run)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BORROWDIRECT PHASE 1: COLLECT CANDIDATE RECORD IDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import os\n",
    "import builtins\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Configuration flags\n",
    "RUN_MODE = \"full\"  # kept for compatibility; demo removed\n",
    "CHECKPOINT_INTERVAL = 100  # Save progress every N records\n",
    "\n",
    "alma_verified_excel = f\"{output_dir}/physical_books_alma_verified_from_worklist.xlsx\"\n",
    "\n",
    "class BorrowDirectPhase1Collector:\n",
    "    \"\"\"Phase 1: Collect BorrowDirect record IDs for Penn items\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://borrowdirect.reshare.indexdata.com/api/v1\"\n",
    "        self.search_url = f\"{self.base_url}/search\"\n",
    "        self.rate_limit_delay = 0.5  # 2 requests per second\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'application/json',\n",
    "            'User-Agent': 'Penn-Library-Research/1.0'\n",
    "        })\n",
    "        self.last_request_time = 0\n",
    "        \n",
    "    def clean_isbn(self, isbn_field):\n",
    "        if pd.isna(isbn_field) or not isbn_field:\n",
    "            return None\n",
    "        isbn_match = re.search(r'(\\d{10,13})', str(isbn_field))\n",
    "        if isbn_match:\n",
    "            return isbn_match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def clean_title_for_search(self, title):\n",
    "        if pd.isna(title) or not title:\n",
    "            return None\n",
    "        title = re.sub(r'\\|[a-z]', ' ', str(title))\n",
    "        title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "        title = ' '.join(title.split())\n",
    "        words = title.split()[:5]\n",
    "        return ' '.join(words) if words else None\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit_delay:\n",
    "            time.sleep(self.rate_limit_delay - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def search_by_isbn(self, isbn):\n",
    "        if not isbn:\n",
    "            return []\n",
    "        self._rate_limit()\n",
    "        try:\n",
    "            params = {\n",
    "                'lookfor': isbn,\n",
    "                'type': 'ISN',\n",
    "                'field[]': ['id', 'title'],\n",
    "                'limit': 20\n",
    "            }\n",
    "            response = self.session.get(self.search_url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bd_ids = []\n",
    "                for record in data.get('records', []):\n",
    "                    if 'id' in record:\n",
    "                        bd_ids.append({\n",
    "                            'bd_id': record['id'],\n",
    "                            'bd_title': record.get('title', ''),\n",
    "                            'search_method': 'isbn'\n",
    "                        })\n",
    "                return bd_ids\n",
    "            else:\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching ISBN {isbn}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_by_title(self, title):\n",
    "        clean_title = self.clean_title_for_search(title)\n",
    "        if not clean_title:\n",
    "            return []\n",
    "        self._rate_limit()\n",
    "        try:\n",
    "            params = {\n",
    "                'lookfor': clean_title,\n",
    "                'type': 'AllFields',\n",
    "                'field[]': ['id', 'title'],\n",
    "                'limit': 20\n",
    "            }\n",
    "            response = self.session.get(self.search_url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bd_ids = []\n",
    "                for record in data.get('records', []):\n",
    "                    if 'id' in record:\n",
    "                        bd_ids.append({\n",
    "                            'bd_id': record['id'],\n",
    "                            'bd_title': record.get('title', ''),\n",
    "                            'search_method': 'title'\n",
    "                        })\n",
    "                return bd_ids\n",
    "            else:\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching title: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_candidates(self, record):\n",
    "        candidates = []\n",
    "        isbn = self.clean_isbn(record.get('ISBN'))\n",
    "        candidates += self.search_by_isbn(isbn)\n",
    "        title = record.get('Title')\n",
    "        title_cands = self.search_by_title(title)\n",
    "        existing_ids = {c['bd_id'] for c in candidates}\n",
    "        for tc in title_cands:\n",
    "            if tc['bd_id'] not in existing_ids:\n",
    "                candidates.append(tc)\n",
    "        return candidates\n",
    "\n",
    "print(\"\\n🔍 Phase 1: Collecting BorrowDirect candidate record IDs...\")\n",
    "\n",
    "if not os.path.exists(alma_verified_excel):\n",
    "    print(f\"❌ Required Excel not found: {alma_verified_excel}\\n   Please run Alma verification earlier in the notebook.\")\n",
    "else:\n",
    "    # Load Alma-verified Excel as authoritative input\n",
    "    original_df = pd.read_excel(\n",
    "        alma_verified_excel,\n",
    "        sheet_name='Alma Verified',\n",
    "        dtype={'OCLC': str, 'ISBN': str, 'F001': str}\n",
    "    )\n",
    "\n",
    "    # Idempotent merge from any prior Phase 1 results\n",
    "    phase1_file = f\"{output_dir}/bd_phase1_results_full.pkl\"\n",
    "    merged_file = f\"{output_dir}/bd_phase1_merged_results.pkl\"  # optional legacy\n",
    "\n",
    "    existing = {}\n",
    "    if os.path.exists(merged_file):\n",
    "        try:\n",
    "            with open(merged_file, 'rb') as f:\n",
    "                payload = pickle.load(f)\n",
    "                for r in payload.get('results', []):\n",
    "                    existing[r['F001']] = r\n",
    "            print(f\"📦 Existing Phase 1 merged results: {len(existing):,} records\")\n",
    "        except Exception as e:\n",
    "            print(\"  ! Failed to read merged phase 1 results:\", e)\n",
    "\n",
    "    results = [] if not existing else list(existing.values())\n",
    "\n",
    "    if 'F001' not in original_df.columns:\n",
    "        print(\"❌ Input is missing F001 column.\")\n",
    "    else:\n",
    "        base_ids = original_df['F001'].astype(str).tolist()\n",
    "        pending_ids = [mid for mid in base_ids if mid not in existing]\n",
    "        print(f\"  - Candidate MMS IDs: {len(base_ids):,}\")\n",
    "        print(f\"  - Already collected: {len(existing):,}\")\n",
    "        print(f\"  - Remaining to collect: {len(pending_ids):,}\")\n",
    "\n",
    "        collector = BorrowDirectPhase1Collector()\n",
    "        checkpoint_file = f\"{output_dir}/bd_phase1_checkpoint.pkl\"\n",
    "        start_idx = 0\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            try:\n",
    "                with open(checkpoint_file, 'rb') as f:\n",
    "                    cp = pickle.load(f)\n",
    "                    start_idx = cp.get('last_index', 0)\n",
    "                    prev = cp.get('results', [])\n",
    "                    results_map = {r['F001']: r for r in results}\n",
    "                    for r in prev:\n",
    "                        results_map[r['F001']] = r\n",
    "                    results = list(results_map.values())\n",
    "                print(f\"  ↩️  Resuming Phase 1 from checkpoint index {start_idx}\")\n",
    "            except Exception as e:\n",
    "                print(\"  ! Failed to load Phase 1 checkpoint:\", e)\n",
    "\n",
    "        found_count = 0\n",
    "        for i, mms_id in enumerate(pending_ids[start_idx:], start=start_idx):\n",
    "            rec = original_df.loc[original_df['F001'] == mms_id].iloc[0].to_dict()\n",
    "            cands = collector.collect_candidates(rec)\n",
    "            result = {\n",
    "                'F001': mms_id,\n",
    "                'num_candidates': len(cands),\n",
    "                'has_candidates': len(cands) > 0,\n",
    "                'candidates': cands,\n",
    "                'search_title': rec.get('Title')\n",
    "            }\n",
    "            results.append(result)\n",
    "            if cands:\n",
    "                found_count += 1\n",
    "\n",
    "            if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                try:\n",
    "                    with open(checkpoint_file, 'wb') as f:\n",
    "                        pickle.dump({'last_index': i + 1, 'results': results}, f)\n",
    "                    print(f\"   💾 Phase 1 checkpoint at {i + 1}\")\n",
    "                except Exception as e:\n",
    "                    print(\"   ! Failed to save Phase 1 checkpoint:\", e)\n",
    "\n",
    "        print(f\"\\n📊 Phase 1 collection completed for {len(pending_ids):,} new MMS IDs\")\n",
    "        records_with_candidates = sum(1 for r in results if r.get('num_candidates', 0) > 0)\n",
    "        print(f\"  - Records with BD candidates: {records_with_candidates} ({records_with_candidates/len(results)*100:.1f}%)\")\n",
    "\n",
    "        with open(phase1_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'results': results,\n",
    "                'original_df': original_df,\n",
    "                'run_mode': RUN_MODE,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "            }, f)\n",
    "        print(f\"  💾 Saved Phase 1 results: {phase1_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BorrowDirect Phase 2: Full Run with Selenium (delta-aware + inline Excel build)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BORROWDIRECT PHASE 2: FULL RUN WITH SELENIUM — DELTA + APPEND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from collections import deque\n",
    "import threading\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from difflib import SequenceMatcher\n",
    "import html\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: Class SeleniumBorrowDirectParser is defined in the previous Phase 2 cell and reused here.\n",
    "\n",
    "print(\"\\n🔎 Starting BorrowDirect Phase 2 in delta/append mode...\")\n",
    "print(\"📌 Behavior: processes only MMS IDs not already present in the merged Phase 2 results; then rebuilds the validated Excel.\")\n",
    "\n",
    "# Inputs (prefer merged; fall back to full)\n",
    "phase1_file_merged = f\"{output_dir}/bd_phase1_merged_results.pkl\"\n",
    "phase1_file_full = f\"{output_dir}/bd_phase1_results_full.pkl\"\n",
    "\n",
    "# Canonical/cached outputs\n",
    "phase2_merged_file = f\"{output_dir}/bd_phase2_merged_results.pkl\"\n",
    "checkpoint_file = f\"{output_dir}/bd_phase2_checkpoint.pkl\"\n",
    "\n",
    "# Downstream Excel paths\n",
    "alma_verified_excel = f\"{output_dir}/physical_books_NOT_harvard_alma_verified.xlsx\"\n",
    "bd_validated_excel = f\"{output_dir}/penn_unique_borrowdirect_validated.xlsx\"\n",
    "\n",
    "# Resolve Phase 1 input\n",
    "phase1_path = None\n",
    "if os.path.exists(phase1_file_merged):\n",
    "    phase1_path = phase1_file_merged\n",
    "elif os.path.exists(phase1_file_full):\n",
    "    phase1_path = phase1_file_full\n",
    "\n",
    "if not phase1_path:\n",
    "    print(f\"❌ Phase 1 results not found: {phase1_file_merged} or {phase1_file_full}\")\n",
    "else:\n",
    "    # Load Phase 1 records\n",
    "    with open(phase1_path, 'rb') as f:\n",
    "        phase1_data = pickle.load(f)\n",
    "    records = phase1_data.get('results', [])\n",
    "    records_with_candidates = [r for r in records if r.get('candidates', [])]\n",
    "\n",
    "    print(f\"📊 Loaded {len(records):,} records from Phase 1; with candidates: {len(records_with_candidates):,}\")\n",
    "\n",
    "    # Load existing merged Phase 2 results if available\n",
    "    existing_map = {}\n",
    "    existing_results = []\n",
    "    if os.path.exists(phase2_merged_file):\n",
    "        try:\n",
    "            with open(phase2_merged_file, 'rb') as f:\n",
    "                phase2_data = pickle.load(f)\n",
    "                existing_results = phase2_data.get('results', [])\n",
    "                existing_map = {r['F001']: r for r in existing_results}\n",
    "            print(f\"📦 Existing Phase 2 merged results found for {len(existing_map):,} MMS IDs\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Failed to read existing Phase 2 merged results:\", e)\n",
    "\n",
    "    # Build delta list\n",
    "    pending = [r for r in records_with_candidates if r['F001'] not in existing_map]\n",
    "    print(f\"  - Pending to process: {len(pending):,}\")\n",
    "\n",
    "    # Resume from checkpoint if available (applies to delta list)\n",
    "    start_idx = 0\n",
    "    new_results = []\n",
    "    successful_parses = 0\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            print(\"📂 Found checkpoint; resuming...\")\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                cp = pickle.load(f)\n",
    "                start_idx = cp.get('last_index', 0)\n",
    "                new_results = cp.get('results', [])\n",
    "                successful_parses = cp.get('successful_parses', 0)\n",
    "            print(f\"  ↩️ Resume index: {start_idx}\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Failed to load checkpoint:\", e)\n",
    "\n",
    "    # Configuration\n",
    "    RUN_MODE = \"delta\"  # process only pending\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    # If there's work to do, run Selenium parser\n",
    "    if pending:\n",
    "        parser = SeleniumBorrowDirectParser(headless=True)\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            total = len(pending)\n",
    "            for batch_start in range(start_idx, total, BATCH_SIZE):\n",
    "                batch_end = min(batch_start + BATCH_SIZE, total)\n",
    "                batch = pending[batch_start:batch_end]\n",
    "\n",
    "                print(f\"\\n📦 Processing batch {batch_start}-{batch_end} of {total}\")\n",
    "                batch_results, batch_parses = parser.process_batch(batch)\n",
    "                new_results.extend(batch_results)\n",
    "                successful_parses += batch_parses\n",
    "\n",
    "                # Save checkpoint for delta progress\n",
    "                cp = {\n",
    "                    'last_index': batch_end,\n",
    "                    'results': new_results,\n",
    "                    'successful_parses': successful_parses,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(cp, f)\n",
    "                print(f\"  💾 Checkpoint saved at {batch_end}\")\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 2 DELTA PROCESSING COMPLETE\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"⏱️ Processed {len(new_results)} new records in {elapsed/60:.1f} minutes; parses: {successful_parses}\")\n",
    "        finally:\n",
    "            parser.cleanup()\n",
    "\n",
    "        # Merge existing + new results keyed by F001\n",
    "        merged_map = {r['F001']: r for r in existing_results}\n",
    "        for r in new_results:\n",
    "            merged_map[r['F001']] = r\n",
    "        merged_results = list(merged_map.values())\n",
    "\n",
    "        # Save merged canonical results\n",
    "        with open(phase2_merged_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'results': merged_results,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'run_mode': RUN_MODE,\n",
    "                'successful_parses_total': successful_parses,\n",
    "            }, f)\n",
    "        print(f\"✅ Merged Phase 2 results saved: {phase2_merged_file} ({len(merged_results):,} records)\")\n",
    "\n",
    "        # Clean up checkpoint on success\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            os.remove(checkpoint_file)\n",
    "            print(\"🧹 Removed checkpoint file\")\n",
    "    else:\n",
    "        print(\"✅ No pending MMS IDs to process; using existing Phase 2 merged results\")\n",
    "        merged_results = existing_results\n",
    "\n",
    "    # Always (re)build the BorrowDirect validated Excel from merged results\n",
    "    if not os.path.exists(alma_verified_excel):\n",
    "        print(f\"❌ Original Excel not found: {alma_verified_excel}\\n   Run the Alma verification cell first.\")\n",
    "    else:\n",
    "        try:\n",
    "            df_original = pd.read_excel(\n",
    "                alma_verified_excel,\n",
    "                sheet_name='Alma Verified',\n",
    "                dtype={'OCLC': str, 'ISBN': str, 'F001': str}\n",
    "            )\n",
    "            # Build summary from merged Phase 2\n",
    "            bd_rows = []\n",
    "            for res in merged_results:\n",
    "                bd_rows.append({\n",
    "                    'F001': res['F001'],\n",
    "                    'found_in_bd': bool(res.get('found_in_bd', False)),\n",
    "                    'is_penn_only': bool(res.get('is_penn_only', False)),\n",
    "                    'num_candidates_checked': int(res.get('num_candidates_checked', 0)),\n",
    "                })\n",
    "            df_bd = pd.DataFrame(bd_rows)\n",
    "\n",
    "            df_final = df_original.merge(df_bd, on='F001', how='left')\n",
    "            df_final['found_in_bd'] = df_final['found_in_bd'].fillna(False)\n",
    "            df_final['is_penn_only'] = df_final['is_penn_only'].fillna(False)\n",
    "            df_final['num_candidates_checked'] = df_final['num_candidates_checked'].fillna(0)\n",
    "\n",
    "            total_records = len(df_final)\n",
    "            found_in_bd = int(df_final['found_in_bd'].sum())\n",
    "            penn_only = int(df_final['is_penn_only'].sum())\n",
    "            not_found = total_records - found_in_bd\n",
    "            available_elsewhere = found_in_bd - penn_only\n",
    "\n",
    "            print(\"\\n📊 BorrowDirect Validation Results (from merged Phase 2):\")\n",
    "            print(f\"  - Total records: {total_records:,}\")\n",
    "            print(f\"  - Found in BorrowDirect: {found_in_bd:,} ({found_in_bd/total_records*100:.1f}%)\")\n",
    "            print(f\"    • Penn-only: {penn_only:,}\")\n",
    "            print(f\"    • Available elsewhere: {available_elsewhere:,}\")\n",
    "            print(f\"  - NOT in BorrowDirect: {not_found:,} ({not_found/total_records*100:.1f}%)\")\n",
    "\n",
    "            with pd.ExcelWriter(bd_validated_excel, engine='openpyxl') as writer:\n",
    "                df_final.to_excel(writer, sheet_name='BD Validated', index=False)\n",
    "                summary_df = pd.DataFrame({\n",
    "                    'Metric': [\n",
    "                        'Total Records',\n",
    "                        'Found in BorrowDirect',\n",
    "                        'Penn-only in BD',\n",
    "                        'Available Elsewhere',\n",
    "                        'Not in BorrowDirect',\n",
    "                    ],\n",
    "                    'Count': [\n",
    "                        total_records,\n",
    "                        found_in_bd,\n",
    "                        penn_only,\n",
    "                        available_elsewhere,\n",
    "                        not_found,\n",
    "                    ],\n",
    "                    'Percentage': [\n",
    "                        100.0,\n",
    "                        found_in_bd/total_records*100,\n",
    "                        penn_only/total_records*100,\n",
    "                        available_elsewhere/total_records*100,\n",
    "                        not_found/total_records*100,\n",
    "                    ],\n",
    "                })\n",
    "                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "            print(f\"\\n💾 BorrowDirect validated Excel saved: {bd_validated_excel}\")\n",
    "            print(\"   This file is ready for the HathiTrust filtering step.\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Failed to build BD validated Excel:\", e)\n",
    "\n",
    "print(\"\\n✅ Phase 2 delta+append complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HathiTrust Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTERING BORROWDIRECT RESULTS FOR HATHITRUST CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure output_dir is available and valid\n",
    "if 'output_dir' not in globals():\n",
    "    output_dir = os.path.join(os.getcwd(), 'pod-processing-outputs')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the BorrowDirect validated results\n",
    "bd_results_path = f\"{output_dir}/penn_unique_borrowdirect_validated.xlsx\"\n",
    "\n",
    "if os.path.exists(bd_results_path):\n",
    "    # Load the BorrowDirect validated data\n",
    "    df_bd = pd.read_excel(\n",
    "        bd_results_path,\n",
    "        sheet_name='BD Validated',\n",
    "        dtype={'OCLC': str, 'F001': str, 'ISBN': str}\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Loaded {len(df_bd):,} BorrowDirect validated records\")\n",
    "\n",
    "    # Filter to truly Penn-unique items:\n",
    "    # - Not found in BorrowDirect OR\n",
    "    # - Found but Penn-only in BorrowDirect\n",
    "    if 'found_in_bd' in df_bd.columns and 'is_penn_only' in df_bd.columns:\n",
    "        df_penn_unique = df_bd[\n",
    "            (~df_bd['found_in_bd']) |\n",
    "            (df_bd['is_penn_only'])\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n\udcc8 Filtering results:\")\n",
    "        print(f\"  - Records before filtering: {len(df_bd):,}\")\n",
    "        print(f\"  - Records after filtering (truly Penn-unique): {len(df_penn_unique):,}\")\n",
    "        print(f\"  - Removed {len(df_bd) - len(df_penn_unique):,} records available at other institutions\")\n",
    "\n",
    "        # Optional: breakdown\n",
    "        if len(df_penn_unique) > 0:\n",
    "            not_found_count = int((~df_penn_unique['found_in_bd']).sum())\n",
    "            penn_only_count = int(df_penn_unique['is_penn_only'].sum())\n",
    "            print(f\"\\n\uddc2️ Breakdown of Penn-unique items:\")\n",
    "            print(f\"  - Not found in BorrowDirect: {not_found_count:,}\")\n",
    "            print(f\"  - Found but Penn-only: {penn_only_count:,}\")\n",
    "\n",
    "        # Save to a new Excel file for HathiTrust checking\n",
    "        hathitrust_input_path = f\"{output_dir}/physical_books_for_hathitrust_check.xlsx\"\n",
    "        df_penn_unique.to_excel(hathitrust_input_path, sheet_name='Penn Unique', index=False)\n",
    "\n",
    "        print(f\"\\n💾 Saved filtered data for HathiTrust checking: {hathitrust_input_path}\")\n",
    "        print(\"   This file contains only items truly unique to Penn.\")\n",
    "    else:\n",
    "        print(\"⚠️ Required columns 'found_in_bd' or 'is_penn_only' not found in the dataset.\")\n",
    "        print(\"Please ensure the BorrowDirect Phase 2 cell completed successfully.\")\n",
    "else:\n",
    "    print(f\"❌ BorrowDirect results not found: {bd_results_path}\")\n",
    "    print(\"Please run the BorrowDirect Phase 2 cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HathiTrust check Penn-unique items (Alma verified)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HATHITRUST CHECK FOR PENN-UNIQUE ITEMS NOT AT HARVARD (ALMA VERIFIED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Graceful tqdm import (avoid pip install in notebook)\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs):\n",
    "        return x\n",
    "\n",
    "# Ensure output_dir exists and is consistent (no hard-coded Linux path)\n",
    "if 'output_dir' not in globals():\n",
    "    output_dir = os.path.join(os.getcwd(), 'pod-processing-outputs')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ensure hathitrust module path is resolvable from repo-relative directory\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "hathitrust_dir = os.path.join(repo_root, 'hathitrust')\n",
    "os.makedirs(hathitrust_dir, exist_ok=True)\n",
    "if hathitrust_dir not in sys.path:\n",
    "    sys.path.insert(0, hathitrust_dir)\n",
    "\n",
    "try:\n",
    "    # Import HathiTrust scanner\n",
    "    from hathitrust_availability_checker_excel import HathiTrustFullScanner\n",
    "\n",
    "    # Load the filtered BorrowDirect results file created in the prior cell\n",
    "    print(\"\\n📂 Loading Penn-unique items for HathiTrust check...\")\n",
    "    excel_file = f\"{output_dir}/physical_books_for_hathitrust_check.xlsx\"\n",
    "    sheet_name = 'Penn Unique'\n",
    "\n",
    "    if os.path.exists(excel_file):\n",
    "        df_penn_unique = pd.read_excel(\n",
    "            excel_file,\n",
    "            sheet_name=sheet_name,\n",
    "            dtype={'OCLC': str, 'F001': str}\n",
    "        )\n",
    "        print(f\"✅ Loaded {len(df_penn_unique):,} Penn-unique items for HathiTrust check\")\n",
    "        print(\"   These records are:\")\n",
    "        print(\"   ✓ Unique to Penn\")\n",
    "        print(\"   ✓ NOT at Harvard\")\n",
    "        print(\"   ✓ Exist in Alma\")\n",
    "        print(\"   ✓ NOT HSP holdings\")\n",
    "        print(\"   ✓ Either NOT in BorrowDirect or Penn-only in BorrowDirect\")\n",
    "\n",
    "        # Clean OCLC numbers\n",
    "        if 'OCLC' in df_penn_unique.columns:\n",
    "            df_penn_unique['OCLC'] = df_penn_unique['OCLC'].fillna('')\n",
    "            df_penn_unique['OCLC'] = df_penn_unique['OCLC'].astype(str).str.replace('.0', '', regex=False)\n",
    "            df_penn_unique['OCLC'] = df_penn_unique['OCLC'].replace('nan', '')\n",
    "            oclc_present = (df_penn_unique['OCLC'] != '').sum()\n",
    "            print(f\"   OCLC numbers present: {oclc_present:,} ({oclc_present/len(df_penn_unique)*100:.1f}%)\")\n",
    "\n",
    "        print(f\"\\n🗂️ Available columns: {list(df_penn_unique.columns)}\")\n",
    "        print(\"\\n🧪 Sample records:\")\n",
    "        sample_cols = ['F001', 'OCLC', 'ISBN', 'Title']\n",
    "        available_cols = [col for col in sample_cols if col in df_penn_unique.columns]\n",
    "        print(df_penn_unique[available_cols].head(3).to_string(index=False))\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Excel file not found: {excel_file}\")\n",
    "        print(\"Please run the BorrowDirect filtering cell to generate this file.\")\n",
    "        raise FileNotFoundError(f\"Missing file: {excel_file}\")\n",
    "\n",
    "    # Check all unique items in HathiTrust\n",
    "    print(f\"\\n✅ Will check all {len(df_penn_unique):,} items in HathiTrust\")\n",
    "    print(\"   These are high priority: Penn-unique, not at Harvard, in Alma, no HSP, and not broadly available via BorrowDirect\")\n",
    "\n",
    "    print(f\"\\n🔄 Preparing records for HathiTrust availability check...\")\n",
    "    dataset_name = f\"penn_unique_not_harvard_alma_{len(df_penn_unique)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    temp_file = f\"{output_dir}/temp_hathitrust_{dataset_name}.xlsx\"\n",
    "\n",
    "    hathi_df = pd.DataFrame({\n",
    "        'MMS_ID': df_penn_unique['F001'].astype(str),\n",
    "        'OCLC': df_penn_unique['OCLC'].astype(str) if 'OCLC' in df_penn_unique.columns else '',\n",
    "        'F245': df_penn_unique['Title'] if 'Title' in df_penn_unique.columns else '',\n",
    "        'F020_str': df_penn_unique['ISBN'].astype(str) if 'ISBN' in df_penn_unique.columns else '',\n",
    "        'F010_str': df_penn_unique['LCCN'].astype(str) if 'LCCN' in df_penn_unique.columns else '',\n",
    "        'F260_str': df_penn_unique['Publication'].astype(str) if 'Publication' in df_penn_unique.columns else '',\n",
    "        'material_category': df_penn_unique['material_category'] if 'material_category' in df_penn_unique.columns else '',\n",
    "        'match_key': df_penn_unique['match_key'] if 'match_key' in df_penn_unique.columns else '',\n",
    "    }).fillna('')\n",
    "\n",
    "    hathi_df['OCLC'] = hathi_df['OCLC'].str.replace('nan', '').str.replace('.0', '')\n",
    "\n",
    "    hathi_df.to_excel(temp_file, index=False)\n",
    "    print(f\"✅ Prepared {len(hathi_df):,} records for HathiTrust check\")\n",
    "    oclc_count = int((hathi_df['OCLC'] != '').sum())\n",
    "    print(f\"   Records with OCLC for HathiTrust: {oclc_count:,} ({oclc_count/len(hathi_df)*100:.1f}%)\")\n",
    "\n",
    "    reports_base = f\"{output_dir}/hathitrust_reports\"\n",
    "    os.makedirs(reports_base, exist_ok=True)\n",
    "    output_report_dir = f\"{reports_base}/{dataset_name}\"\n",
    "    os.makedirs(output_report_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n🔎 Initializing HathiTrust scanner...\")\n",
    "    from hathitrust_availability_checker_excel import HathiTrustFullScanner\n",
    "    scanner = HathiTrustFullScanner(\n",
    "        rate_limit_delay=0.5,\n",
    "        max_workers=2,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🚀 Starting HathiTrust availability check...\")\n",
    "    print(\"   ✓ Penn-unique (conservative)\\n   ✓ NOT at Harvard (title-verified)\\n   ✓ In Alma\\n   ✓ NOT HSP\\n   ✓ Not in BD or Penn-only in BD\")\n",
    "    estimated_time = (len(hathi_df) * 0.5) / 60\n",
    "    print(f\"\\n⏱️ Estimated time: {estimated_time:.1f} minutes ({estimated_time/60:.1f} hours)\")\n",
    "\n",
    "    scanner.scan_full_file(temp_file, start_from=0, batch_size=50)\n",
    "\n",
    "    print(f\"\\n✅ HathiTrust check complete!\")\n",
    "\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "        print(\"   Cleaned up temporary file\")\n",
    "\n",
    "    result_files = [\n",
    "        os.path.join(output_report_dir, f)\n",
    "        for f in os.listdir(output_report_dir)\n",
    "        if f.startswith('hathitrust_scan_results_') and f.endswith('.csv')\n",
    "    ]\n",
    "\n",
    "    if result_files:\n",
    "        results_df = pd.read_csv(result_files[0])\n",
    "        total_checked = len(results_df)\n",
    "        found_in_hathi = int(results_df['found'].sum()) if 'found' in results_df.columns else 0\n",
    "        not_in_hathi = total_checked - found_in_hathi\n",
    "\n",
    "        print(f\"\\n📊 RESULTS SUMMARY:\")\n",
    "        print(f\"   Total checked: {total_checked:,}\")\n",
    "        print(f\"   Found in HathiTrust: {found_in_hathi:,} ({found_in_hathi/total_checked*100:.1f}%)\")\n",
    "        print(f\"   NOT in HathiTrust: {not_in_hathi:,} ({not_in_hathi/total_checked*100:.1f}%)\")\n",
    "\n",
    "        if 'access_type' in results_df.columns:\n",
    "            access_counts = results_df['access_type'].value_counts()\n",
    "            print(\"\\n🗂️ Access levels for items in HathiTrust:\")\n",
    "            for access_type, count in access_counts.items():\n",
    "                print(f\"   - {access_type}: {count:,}\")\n",
    "\n",
    "    summary_info = {\n",
    "        'check_date': datetime.now().isoformat(),\n",
    "        'dataset': 'penn_unique_not_at_harvard_alma_verified_bd_filtered',\n",
    "        'total_items_checked': len(df_penn_unique),\n",
    "        'items_with_oclc': int(oclc_count),\n",
    "        'oclc_coverage_percent': float(oclc_count/len(hathi_df)*100),\n",
    "        'description': 'Penn-unique, not at Harvard, in Alma, no HSP holdings, filtered by BorrowDirect',\n",
    "        'priority': 'HIGHEST',\n",
    "        'source_file': excel_file,\n",
    "        'filtering_applied': [\n",
    "            'Conservative unique filtering (standard identifiers)',\n",
    "            'ISBN deduplication',\n",
    "            'F533 reproductions removed',\n",
    "            'HSP records removed',\n",
    "            'RDA electronic resources removed',\n",
    "            'Harvard API check with title verification',\n",
    "            'Alma existence verification',\n",
    "            'HSP holdings exclusion',\n",
    "            'BorrowDirect availability filtering',\n",
    "        ],\n",
    "        'results_location': output_report_dir,\n",
    "    }\n",
    "\n",
    "    summary_file = f\"{output_dir}/hathitrust_penn_not_harvard_alma_{len(df_penn_unique)}_summary.json\"\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        json.dump(summary_info, f, indent=2)\n",
    "    print(f\"\\n💾 Summary info saved to: {summary_file}\")\n",
    "    print(f\"\\n📁 RESULTS LOCATION:\\n   {output_report_dir}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Could not import HathiTrust scanner: {e}\")\n",
    "    print(\"Please ensure hathitrust_availability_checker_excel.py is in the hathitrust/ directory relative to the repo root.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during HathiTrust check: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    if 'temp_file' in locals() and os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "        print(f\"   Cleaned up temporary file: {temp_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Final Excel Report with HathiTrust Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING FINAL EXCEL REPORT WITH HATHITRUST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "try:\n",
    "    # 1. Load the Penn-unique records (filtered for HathiTrust check)\n",
    "    print(\"\\n📂 Loading Penn-unique records...\")\n",
    "    hathitrust_input = f\"{output_dir}/physical_books_for_hathitrust_check.xlsx\"\n",
    "    \n",
    "    if not os.path.exists(hathitrust_input):\n",
    "        raise FileNotFoundError(f\"Missing file: {hathitrust_input}\")\n",
    "    \n",
    "    df_penn_unique = pd.read_excel(\n",
    "        hathitrust_input,\n",
    "        sheet_name='Penn Unique',\n",
    "        dtype={'OCLC': str, 'F001': str, 'ISBN': str}\n",
    "    )\n",
    "    print(f\"✅ Loaded {len(df_penn_unique):,} Penn-unique records\")\n",
    "    \n",
    "    # 2. Find the latest HathiTrust results\n",
    "    print(\"\\n📂 Looking for HathiTrust scan results...\")\n",
    "    reports_base = f\"{output_dir}/hathitrust_reports\"\n",
    "    \n",
    "    # Find all scan directories (sorted by date, most recent first)\n",
    "    scan_dirs = sorted(\n",
    "        [d for d in glob.glob(f\"{reports_base}/penn_unique_*\") if os.path.isdir(d)],\n",
    "        key=os.path.getmtime,\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    hathi_results_df = None\n",
    "    latest_scan_dir = None\n",
    "    \n",
    "    if scan_dirs:\n",
    "        latest_scan_dir = scan_dirs[0]\n",
    "        print(f\"✅ Found scan directory: {os.path.basename(latest_scan_dir)}\")\n",
    "        \n",
    "        # Look for results CSV in the directory\n",
    "        result_files = glob.glob(f\"{latest_scan_dir}/hathitrust_scan_results_*.csv\")\n",
    "        \n",
    "        if result_files:\n",
    "            latest_results = sorted(result_files, key=os.path.getmtime, reverse=True)[0]\n",
    "            hathi_results_df = pd.read_csv(latest_results, dtype={'MMS_ID': str, 'OCLC': str})\n",
    "            print(f\"✅ Loaded HathiTrust results: {len(hathi_results_df):,} records\")\n",
    "            print(f\"   From: {os.path.basename(latest_results)}\")\n",
    "        else:\n",
    "            print(\"⚠️ No results CSV found in scan directory\")\n",
    "    else:\n",
    "        print(\"⚠️ No HathiTrust scan results found\")\n",
    "        print(\"   Please run the HathiTrust check cell first\")\n",
    "    \n",
    "    # 3. Merge HathiTrust results with Penn-unique data\n",
    "    print(\"\\n🔄 Merging HathiTrust results with Penn-unique data...\")\n",
    "    \n",
    "    if hathi_results_df is not None:\n",
    "        # Clean MMS_ID columns for matching\n",
    "        df_penn_unique['F001_clean'] = df_penn_unique['F001'].astype(str).str.strip()\n",
    "        hathi_results_df['MMS_ID_clean'] = hathi_results_df['MMS_ID'].astype(str).str.strip()\n",
    "        \n",
    "        # Merge on MMS_ID\n",
    "        df_final = df_penn_unique.merge(\n",
    "            hathi_results_df[['MMS_ID_clean', 'found', 'num_items', 'access_type', 'match_type']],\n",
    "            left_on='F001_clean',\n",
    "            right_on='MMS_ID_clean',\n",
    "            how='left',\n",
    "            suffixes=('', '_hathi')\n",
    "        )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        df_final = df_final.drop(columns=['F001_clean', 'MMS_ID_clean'], errors='ignore')\n",
    "        \n",
    "        # Rename HathiTrust columns for clarity\n",
    "        df_final = df_final.rename(columns={\n",
    "            'found': 'in_hathitrust',\n",
    "            'num_items': 'hathi_item_count',\n",
    "            'access_type': 'hathi_access',\n",
    "            'match_type': 'hathi_match_type'\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ Merged data: {len(df_final):,} records\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total = len(df_final)\n",
    "        in_hathi = df_final['in_hathitrust'].sum() if 'in_hathitrust' in df_final.columns else 0\n",
    "        not_in_hathi = total - in_hathi\n",
    "        \n",
    "        print(f\"\\n📊 HathiTrust Coverage:\")\n",
    "        print(f\"   Total Penn-unique items: {total:,}\")\n",
    "        print(f\"   Found in HathiTrust: {in_hathi:,} ({in_hathi/total*100:.1f}%)\")\n",
    "        print(f\"   NOT in HathiTrust: {not_in_hathi:,} ({not_in_hathi/total*100:.1f}%)\")\n",
    "        \n",
    "        if 'hathi_access' in df_final.columns:\n",
    "            print(f\"\\n📋 Access levels for items in HathiTrust:\")\n",
    "            access_counts = df_final['hathi_access'].value_counts()\n",
    "            for access_type, count in access_counts.items():\n",
    "                if pd.notna(access_type):\n",
    "                    print(f\"   - {access_type}: {count:,}\")\n",
    "        \n",
    "    else:\n",
    "        df_final = df_penn_unique.copy()\n",
    "        print(\"⚠️ No HathiTrust results to merge - using Penn-unique data only\")\n",
    "    \n",
    "    # 4. Create priority classification\n",
    "    print(\"\\n🎯 Classifying  priorities...\")\n",
    "    \n",
    "    def classify_priority(row):\n",
    "        \"\"\"Classify priority based on HathiTrust status\"\"\"\n",
    "        if 'in_hathitrust' not in row or pd.isna(row.get('in_hathitrust')):\n",
    "            return 'PRIORITY 1: Unknown (HathiTrust not checked)'\n",
    "        \n",
    "        if not row['in_hathitrust']:\n",
    "            return 'PRIORITY 1: Not in HathiTrust (HIGHEST)'\n",
    "        \n",
    "        access = row.get('hathi_access', '')\n",
    "        if pd.isna(access) or access == '':\n",
    "            return 'PRIORITY 2: In HathiTrust (access unknown)'\n",
    "        \n",
    "        if 'limited' in str(access).lower() or 'restricted' in str(access).lower():\n",
    "            return 'PRIORITY 2: In HathiTrust (restricted access)'\n",
    "        \n",
    "        if 'full' in str(access).lower():\n",
    "            return 'PRIORITY 3: In HathiTrust (full access available)'\n",
    "        \n",
    "        return 'PRIORITY 2: In HathiTrust (access unknown)'\n",
    "    \n",
    "    df_final['preservation_priority'] = df_final.apply(classify_priority, axis=1)\n",
    "    \n",
    "    # Show priority breakdown\n",
    "    print(f\"\\n📋 Priority Breakdown:\")\n",
    "    priority_counts = df_final['preservation_priority'].value_counts().sort_index()\n",
    "    for priority, count in priority_counts.items():\n",
    "        print(f\"   {priority}: {count:,}\")\n",
    "    \n",
    "    # 5. Generate final Excel report\n",
    "    print(\"\\n📄 Generating final Excel report...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_report_path = f\"{output_dir}/FINAL_penn_preservation_priorities_{timestamp}.xlsx\"\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    priority_cols = ['preservation_priority', 'in_hathitrust', 'hathi_access', 'hathi_item_count']\n",
    "    identity_cols = ['F001', 'OCLC', 'ISBN', 'LCCN', 'Title', 'Author', 'Publication']\n",
    "    bd_cols = ['found_in_bd', 'is_penn_only', 'bd_num_holdings', 'bd_holding_institutions']\n",
    "    other_cols = [col for col in df_final.columns if col not in priority_cols + identity_cols + bd_cols]\n",
    "    \n",
    "    # Build ordered column list (only include columns that exist)\n",
    "    ordered_cols = []\n",
    "    for col in priority_cols + identity_cols + bd_cols + other_cols:\n",
    "        if col in df_final.columns:\n",
    "            ordered_cols.append(col)\n",
    "    \n",
    "    df_final = df_final[ordered_cols]\n",
    "    \n",
    "    # Create Excel writer with multiple sheets\n",
    "    with pd.ExcelWriter(final_report_path, engine='openpyxl') as writer:\n",
    "        # Main sheet: All records sorted by priority\n",
    "        df_final.sort_values('preservation_priority').to_excel(\n",
    "            writer, \n",
    "            sheet_name='All Records', \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Priority 1: Items NOT in HathiTrust (highest priority)\n",
    "        priority1 = df_final[df_final['preservation_priority'].str.contains('PRIORITY 1', na=False)]\n",
    "        if len(priority1) > 0:\n",
    "            priority1.to_excel(writer, sheet_name='Priority 1 - Not in HT', index=False)\n",
    "        \n",
    "        # Priority 2: Items in HathiTrust but restricted\n",
    "        priority2 = df_final[df_final['preservation_priority'].str.contains('PRIORITY 2', na=False)]\n",
    "        if len(priority2) > 0:\n",
    "            priority2.to_excel(writer, sheet_name='Priority 2 - Restricted', index=False)\n",
    "        \n",
    "        # Priority 3: Items in HathiTrust with full access\n",
    "        priority3 = df_final[df_final['preservation_priority'].str.contains('PRIORITY 3', na=False)]\n",
    "        if len(priority3) > 0:\n",
    "            priority3.to_excel(writer, sheet_name='Priority 3 - Full Access', index=False)\n",
    "        \n",
    "        # Summary statistics sheet\n",
    "        summary_data = {\n",
    "            'Metric': [\n",
    "                'Report Generated',\n",
    "                'Total Penn-Unique Items',\n",
    "                'Items Checked in HathiTrust',\n",
    "                'Found in HathiTrust',\n",
    "                'NOT in HathiTrust',\n",
    "                'HathiTrust Coverage %',\n",
    "                'Priority 1 (Not in HT)',\n",
    "                'Priority 2 (Restricted)',\n",
    "                'Priority 3 (Full Access)',\n",
    "                '',\n",
    "                'Filtering Applied',\n",
    "                '- Conservative unique filtering',\n",
    "                '- Harvard verification',\n",
    "                '- Alma validation',\n",
    "                '- HSP exclusion',\n",
    "                '- BorrowDirect filtering',\n",
    "                '- HathiTrust check'\n",
    "            ],\n",
    "            'Value': [\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                len(df_final),\n",
    "                len(df_final) if hathi_results_df is not None else 0,\n",
    "                int(in_hathi) if hathi_results_df is not None else 0,\n",
    "                int(not_in_hathi) if hathi_results_df is not None else 0,\n",
    "                f\"{in_hathi/total*100:.1f}%\" if hathi_results_df is not None else 'N/A',\n",
    "                len(priority1),\n",
    "                len(priority2),\n",
    "                len(priority3),\n",
    "                '',\n",
    "                '',\n",
    "                '✓',\n",
    "                '✓',\n",
    "                '✓',\n",
    "                '✓',\n",
    "                '✓',\n",
    "                '✓' if hathi_results_df is not None else 'Pending'\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Final report generated successfully!\")\n",
    "    print(f\"📁 Location: {final_report_path}\")\n",
    "    print(f\"\\n📊 Report contains {len(df_final):,} records across multiple sheets:\")\n",
    "    print(f\"   ✓ 'All Records' - Complete dataset sorted by priority\")\n",
    "    print(f\"   ✓ 'Priority 1 - Not in HT' - {len(priority1):,} records (HIGHEST PRIORITY)\")\n",
    "    print(f\"   ✓ 'Priority 2 - Restricted' - {len(priority2):,} records\")\n",
    "    print(f\"   ✓ 'Priority 3 - Full Access' - {len(priority3):,} records\")\n",
    "    print(f\"   ✓ 'Summary' - Key statistics\")\n",
    "    \n",
    "    print(\"\\n🎯 NEXT STEPS:\")\n",
    "    print(\"   1. Review Priority 1 items - these are NOT in HathiTrust!\")\n",
    "    print(\"   2. Review Priority 2 items - these have restricted access in HathiTrust\")\n",
    "    print(\"   3. Priority 3 items are already fully accessible in HathiTrust\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please ensure all previous cells have been run successfully:\")\n",
    "    print(\"   1. BorrowDirect Phase 1 (full collection)\")\n",
    "    print(\"   2. BorrowDirect Phase 1.5 (enhanced search)\")\n",
    "    print(\"   3. Merge Phase 1 + 1.5\")\n",
    "    print(\"   4. BorrowDirect Phase 2 (Selenium verification)\")\n",
    "    print(\"   5. Filter for HathiTrust check\")\n",
    "    print(\"   6. HathiTrust availability check\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating report: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Cleanup - Run this to free memory after analysis\n",
    "def cleanup_spark_resources():\n",
    "    \"\"\"Clean up all cached DataFrames and temporary views\"\"\"\n",
    "    try:\n",
    "        # Get all cached DataFrames\n",
    "        cached_count = len(spark.sparkContext._jsc.getPersistentRDDs().items())\n",
    "        \n",
    "        for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "            rdd.unpersist()\n",
    "        \n",
    "        # Drop all temporary views\n",
    "        temp_views = [view.name for view in spark.catalog.listTables() if view.isTemporary]\n",
    "        for view_name in temp_views:\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "        \n",
    "        print(f\"✅ Cleaned up {cached_count} cached DataFrames and {len(temp_views)} temporary views\")\n",
    "        print(\"💡 Memory freed. You can safely re-run the notebook or close it.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning: {e}\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_spark_resources()\n",
    "\n",
    "# Optional: Show memory status\n",
    "print(\"\\n📊 Spark UI still available at:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"Check the Storage tab to verify all caches are cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
