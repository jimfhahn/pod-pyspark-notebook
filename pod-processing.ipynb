{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivy Plus MARC Analysis with Enhanced Matching\n",
    "\n",
    "This notebook processes MARC data from Ivy Plus libraries to identify unique records held by Penn that are not held by other institutions in the consortium.\n",
    "\n",
    "## Enhanced Normalization and Matching\n",
    "\n",
    "The matching process has been improved with specialized normalization for different fields:\n",
    "\n",
    "1. **ISBN/LCCN Matching**: When standard identifiers are available, they are normalized and used as primary match keys\n",
    "   - ISBN-10 and ISBN-13 are properly normalized to ensure consistent matching\n",
    "   - LCCNs are standardized to handle different formats and prefixes\n",
    "\n",
    "2. **Match Key Creation**: For records without standard identifiers, a composite key is created from:\n",
    "   - Normalized title (with improved noise word removal)\n",
    "   - Normalized edition statement\n",
    "   - Normalized publication information with year extraction\n",
    "\n",
    "3. **Match Key Validation**: Each match key is validated for quality to detect potential issues\n",
    "   - Short or generic match keys are flagged\n",
    "   - Match key quality metrics are saved for analysis\n",
    "\n",
    "4. **Field Selection**: \n",
    "   - Leader (FLDR) is now included for record type identification\n",
    "   - Core bibliographic fields (F001, F010, F020, F245, F250, F260) are used\n",
    "\n",
    "This improved approach maintains the principle that different editions, printings, and formats are unique bibliographic entities while enhancing the accuracy of matching across cataloging variations.\n",
    "\n",
    "## Initial load only - Institution-specific Processing\n",
    "Converts MARC to Parquet format for faster processing, maintaining institution-specific separation. This step ensures that each institution's MARC files are converted to separate Parquet files for consistent downstream processing.\n",
    "\n",
    "The conversion includes the leader field (FLDR) for each record while excluding the 007 field to optimize the output files. The leader contains important information about the record structure, material type, and bibliographic level.\n",
    "\n",
    "## HIGH MEMORY REQUIREMENT\n",
    "\n",
    "**This notebook is configured for a high-performance server environment with the following specifications:**\n",
    "\n",
    "- **240GB driver memory allocation** (requires ~300GB total system RAM)\n",
    "- **16 cores** for parallel processing\n",
    "- Optimized for a **Linode 300GB server**\n",
    "\n",
    "**Running this notebook with the current configuration on a standard laptop or desktop will likely cause your kernel to crash or your system to become unresponsive.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /home/jovyan/work/July-2025-PODParquet\n",
      "Output directory: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\n"
     ]
    }
   ],
   "source": [
    "# Define paths for your PySpark server\n",
    "# Update these paths to match your server's directory structure\n",
    "input_dir = \"/home/jovyan/work/July-2025-PODParquet\"  # Where your parquet files are located\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"  # Where to save the results\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {input_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session with full configuration...\n",
      "âœ… Spark session initialized with 200GB memory and optimized settings!\n",
      "Spark UI available at: http://fcf8b1bef143:4040\n",
      "\n",
      "Testing Spark with a simple operation...\n",
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "|  0|      0|\n",
      "|  1|      2|\n",
      "|  2|      4|\n",
      "|  3|      6|\n",
      "|  4|      8|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸ“‹ Key configurations:\n",
      "  - Driver memory: 260g\n",
      "  - Max result size: 200g\n",
      "  - Memory fraction: 0.6\n",
      "  - Shuffle partitions: 400\n",
      "\n",
      "âœ… Spark session ready for processing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Clean up any existing Spark sessions\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        time.sleep(2)  # Give it time to clean up\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear environment variables that might conflict\n",
    "for key in list(os.environ.keys()):\n",
    "    if 'SPARK' in key or 'JAVA' in key or 'PYSPARK' in key:\n",
    "        del os.environ[key]\n",
    "\n",
    "# Set JAVA_HOME explicitly\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "# Create temp directory\n",
    "os.makedirs('/tmp/spark-temp', exist_ok=True)\n",
    "\n",
    "# Create Spark session with all configurations at once\n",
    "# Since we know 200GB works from your test, we'll use that\n",
    "print(\"Creating Spark session with full configuration...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PodProcessing-Stable\") \\\n",
    "    .master(\"local[12]\") \\\n",
    "    .config(\"spark.driver.memory\", \"260g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"200g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"30m\") \\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"5min\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"20g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark session initialized with 200GB memory and optimized settings!\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Test it works\n",
    "print(\"\\nTesting Spark with a simple operation...\")\n",
    "test_df = spark.range(100).selectExpr(\"id\", \"id * 2 as doubled\")\n",
    "test_df.show(5)\n",
    "\n",
    "# Verify key configurations\n",
    "print(\"\\nðŸ“‹ Key configurations:\")\n",
    "print(f\"  - Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  - Max result size: {spark.conf.get('spark.driver.maxResultSize')}\")\n",
    "print(f\"  - Memory fraction: {spark.conf.get('spark.memory.fraction')}\")\n",
    "print(f\"  - Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "print(\"\\nâœ… Spark session ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.1.1)\n",
      "Requirement already satisfied: pymarc in /opt/conda/lib/python3.11/site-packages (5.3.1)\n",
      "Requirement already satisfied: poetry in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: marctable in /opt/conda/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: build<2.0.0,>=1.2.1 in /opt/conda/lib/python3.11/site-packages (from poetry) (1.2.2.post1)\n",
      "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (0.14.3)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (2.1.0)\n",
      "Requirement already satisfied: dulwich<0.23.0,>=0.22.6 in /opt/conda/lib/python3.11/site-packages (from poetry) (0.22.8)\n",
      "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (2.18.1)\n",
      "Requirement already satisfied: findpython<0.7.0,>=0.6.2 in /opt/conda/lib/python3.11/site-packages (from poetry) (0.6.3)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: keyring<26.0.0,>=25.1.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (25.6.0)\n",
      "Requirement already satisfied: packaging>=24.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (25.0)\n",
      "Requirement already satisfied: pbs-installer<2026.0.0,>=2025.1.6 in /opt/conda/lib/python3.11/site-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (2025.7.12)\n",
      "Requirement already satisfied: pkginfo<2.0,>=1.12 in /opt/conda/lib/python3.11/site-packages (from poetry) (1.12.1.2)\n",
      "Requirement already satisfied: platformdirs<5,>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (3.11.0)\n",
      "Requirement already satisfied: poetry-core==2.1.3 in /opt/conda/lib/python3.11/site-packages (from poetry) (2.1.3)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (1.2.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in /opt/conda/lib/python3.11/site-packages (from poetry) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /opt/conda/lib/python3.11/site-packages (from poetry) (1.5.4)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /opt/conda/lib/python3.11/site-packages (from poetry) (0.13.3)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /opt/conda/lib/python3.11/site-packages (from poetry) (2025.5.9.12)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.26.6 in /opt/conda/lib/python3.11/site-packages (from poetry) (20.31.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /opt/conda/lib/python3.11/site-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.0.6)\n",
      "Requirement already satisfied: filelock>=3.8.0 in /opt/conda/lib/python3.11/site-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.18.0)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from cleo<3.0.0,>=2.1.0->poetry) (0.4.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from cleo<3.0.0,>=2.1.0->poetry) (3.13.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.11/site-packages (from dulwich<0.23.0,>=0.22.6->poetry) (2.0.7)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (0.9.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.11.4 in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.8.0)\n",
      "Requirement already satisfied: jaraco.classes in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.4.0)\n",
      "Requirement already satisfied: jaraco.functools in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (4.2.1)\n",
      "Requirement already satisfied: jaraco.context in /opt/conda/lib/python3.11/site-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.0.1)\n",
      "Requirement already satisfied: httpx<1,>=0.27.0 in /opt/conda/lib/python3.11/site-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.28.1)\n",
      "Requirement already satisfied: zstandard>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.21.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.26->poetry) (3.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/lib/python3.11/site-packages (from virtualenv<21.0.0,>=20.26.6->poetry) (0.4.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from marctable) (4.12.2)\n",
      "Requirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.11/site-packages (from marctable) (8.1.7)\n",
      "Requirement already satisfied: pandas>=2.1.4 in /opt/conda/lib/python3.11/site-packages (from marctable) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=14.0.2 in /opt/conda/lib/python3.11/site-packages (from marctable) (21.0.0)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /opt/conda/lib/python3.11/site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.12.2->marctable) (2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata>=4.11.4->keyring<26.0.0,>=25.1.0->poetry) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.4->marctable) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.4->marctable) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.4->marctable) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.4->marctable) (2023.3)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.11/site-packages (from SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (41.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (2.21)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.3.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry) (10.7.0)\n",
      "Requirement already satisfied: backports.tarfile in /opt/conda/lib/python3.11/site-packages (from jaraco.context->keyring<26.0.0,>=25.1.0->poetry) (1.2.0)\n",
      "Added /opt/conda/bin to PATH\n",
      "âœ… marctable command found in PATH\n",
      "\n",
      "âœ… All packages installed and environment configured\n",
      "Current PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin:/opt/conda/bin\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pymarc poetry marctable fuzzywuzzy python-Levenshtein langdetect\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the user's local bin directory for macOS\n",
    "user_local_bin = os.path.expanduser('~/.local/bin')\n",
    "\n",
    "# Add the directory to PATH if it exists\n",
    "if os.path.exists(user_local_bin):\n",
    "    os.environ['PATH'] += os.pathsep + user_local_bin\n",
    "    print(f\"Added {user_local_bin} to PATH\")\n",
    "\n",
    "# Also add Python's user site-packages bin directory\n",
    "python_user_bin = os.path.join(sys.prefix, 'bin')\n",
    "if os.path.exists(python_user_bin):\n",
    "    os.environ['PATH'] += os.pathsep + python_user_bin\n",
    "    print(f\"Added {python_user_bin} to PATH\")\n",
    "\n",
    "# For Homebrew Python installations on macOS\n",
    "homebrew_bin = '/usr/local/bin'\n",
    "if os.path.exists(homebrew_bin) and homebrew_bin not in os.environ['PATH']:\n",
    "    os.environ['PATH'] += os.pathsep + homebrew_bin\n",
    "    print(f\"Added {homebrew_bin} to PATH\")\n",
    "\n",
    "# Check if marctable is accessible\n",
    "import shutil\n",
    "if shutil.which('marctable'):\n",
    "    print(\"âœ… marctable command found in PATH\")\n",
    "else:\n",
    "    print(\"âš ï¸  marctable command not found in PATH - checking alternative locations...\")\n",
    "    # Try to find marctable in common locations\n",
    "    possible_locations = [\n",
    "        os.path.expanduser('~/Library/Python/3.11/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.10/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.9/bin'),\n",
    "        '/opt/homebrew/bin',\n",
    "        '/usr/local/bin',\n",
    "    ]\n",
    "    \n",
    "    for loc in possible_locations:\n",
    "        marctable_path = os.path.join(loc, 'marctable')\n",
    "        if os.path.exists(marctable_path):\n",
    "            os.environ['PATH'] += os.pathsep + loc\n",
    "            print(f\"âœ… Found marctable in {loc} and added to PATH\")\n",
    "            break\n",
    "\n",
    "print(\"\\nâœ… All packages installed and environment configured\")\n",
    "print(f\"Current PATH: {os.environ['PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optimized Spark SQL functions loaded - properly handles mixed string/array field types\n",
      "âœ… F010 (LCCN): string, F020 (ISBN): array, F245 (Title): string, F250/F260: arrays\n",
      "âœ… FIXED: add_id_list_spark now properly handles null values with F.lit(None)\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Functions\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Helper function to handle fields that might be strings or arrays\n",
    "def handle_field_as_string(col_name):\n",
    "    \"\"\"\n",
    "    Safely extract string value whether the field is a string or array.\n",
    "    This version handles mixed types properly.\n",
    "    \"\"\"\n",
    "    # Check if it's an array type - if so, get first element; otherwise just cast to string\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        # Try to get the data type - arrays will have size() function\n",
    "        F.when(\n",
    "            F.size(F.col(col_name)) >= 0,  # This will only work for arrays\n",
    "            F.col(col_name).getItem(0)  # Get first element of array\n",
    "        ).otherwise(\n",
    "            F.col(col_name)  # It's already a string, just use it\n",
    "        )\n",
    "    ).cast(\"string\")\n",
    "\n",
    "def create_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Create match keys using pure Spark SQL functions - MUCH faster than UDFs\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"match_key\", \n",
    "        F.concat_ws(\"_\",\n",
    "            # Normalize title (F245 is string)\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(\n",
    "                            F.lower(F.trim(F.col(\"F245\"))),\n",
    "                            \"^(the|a|an)\\\\s+\", \"\"\n",
    "                        ),\n",
    "                        \"[^a-z0-9\\\\s]\", \"\"\n",
    "                    ),\n",
    "                    \"\\\\s+\", \" \"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Normalize edition (F250 is array)\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F250\").getItem(0)), \n",
    "                    \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed|edition)\", \"$1 ed\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Extract year from publication (F260 is array)\n",
    "            F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F260\").getItem(0), \"(1[0-9]{3}|20[0-9]{2})\", 1)\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def normalize_ids_spark(df):\n",
    "    \"\"\"\n",
    "    Normalize ISBN and LCCN using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"normalized_isbn\",\n",
    "        # F020 is array\n",
    "        F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0),\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"F020\").getItem(0), \"([0-9X-]+)\", 1),\n",
    "                \"[^0-9X]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"normalized_lccn\", \n",
    "        # F010 is string\n",
    "        F.when(F.col(\"F010\").isNotNull(),\n",
    "            F.regexp_replace(\n",
    "                F.trim(F.col(\"F010\")),\n",
    "                \"[^a-zA-Z0-9-]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def add_id_list_spark(df):\n",
    "    \"\"\"\n",
    "    Create id_list using Spark SQL array functions - FIXED version\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"id_list\",\n",
    "        F.array_remove(\n",
    "            F.array(\n",
    "                F.when(\n",
    "                    (F.col(\"normalized_isbn\").isNotNull()) & \n",
    "                    (F.col(\"normalized_isbn\") != \"\"), \n",
    "                    F.col(\"normalized_isbn\")\n",
    "                ).otherwise(F.lit(None)),\n",
    "                F.when(\n",
    "                    (F.col(\"normalized_lccn\").isNotNull()) & \n",
    "                    (F.col(\"normalized_lccn\") != \"\"), \n",
    "                    F.col(\"normalized_lccn\")\n",
    "                ).otherwise(F.lit(None))\n",
    "            ),\n",
    "            None\n",
    "        )\n",
    "    )\n",
    "\n",
    "def validate_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Validate match keys using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_valid_match_key\",\n",
    "        (F.length(F.col(\"match_key\")) >= 5) &\n",
    "        (~F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"))\n",
    "    ).withColumn(\"match_key_message\",\n",
    "        F.when(F.length(F.col(\"match_key\")) < 5, \"Match key too short\")\n",
    "         .when(F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"), \"Generic match key\")\n",
    "         .otherwise(\"Valid match key\")\n",
    "    )\n",
    "\n",
    "def process_institution_optimized(df, institution_name):\n",
    "    \"\"\"\n",
    "    Apply all optimizations to an institution's DataFrame\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"source\", F.lit(institution_name))\n",
    "        .transform(normalize_ids_spark)\n",
    "        .transform(create_match_key_spark)\n",
    "        .transform(add_id_list_spark)\n",
    "        .transform(validate_match_key_spark)\n",
    "    )\n",
    "\n",
    "print(\"âœ… Optimized Spark SQL functions loaded - properly handles mixed string/array field types\")\n",
    "print(\"âœ… F010 (LCCN): string, F020 (ISBN): array, F245 (Title): string, F250/F260: arrays\")\n",
    "print(\"âœ… FIXED: add_id_list_spark now properly handles null values with F.lit(None)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import re\n",
    "from pymarc import Record, MARCReader\n",
    "\n",
    "# Setup logging for MARC conversion\n",
    "log_dir = f'{output_dir}/logs'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, 'marc2parquet.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_institution_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extract institution name from filename patterns\"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    \n",
    "    # For files from pod-processing-outputs/final/ like \"harvard_updates-001.mrc\"\n",
    "    if '_' in base:\n",
    "        return base.split('_')[0]\n",
    "    \n",
    "    # Pattern: institution-date-descriptor-format.ext\n",
    "    match = re.match(r'^([a-z]+)-[\\d\\-]+-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Pattern: institution-descriptor.ext\n",
    "    match = re.match(r'^([a-z]+)-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Default: use the first word\n",
    "    return base.split('-')[0].split('.')[0]\n",
    "\n",
    "def safe_read_marc_file_with_recovery(file_path: str, temp_output: str) -> Tuple[int, Dict]:\n",
    "    \"\"\"Read MARC file with maximum error recovery and minimal validation\"\"\"\n",
    "    total_records = 0\n",
    "    valid_records = 0\n",
    "    report = {\"total_attempted\": 0, \"parsed\": 0, \"errors\": 0}\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file, open(temp_output, 'wb') as outfile:\n",
    "            reader = MARCReader(file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            \n",
    "            for record_number, record in enumerate(reader, 1):\n",
    "                total_records += 1\n",
    "                \n",
    "                if record is None:\n",
    "                    report[\"errors\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    outfile.write(record.as_marc())\n",
    "                    valid_records += 1\n",
    "                except Exception as e:\n",
    "                    report[\"errors\"] += 1\n",
    "                    logger.warning(f\"Error writing record {record_number}: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "        \n",
    "    report[\"total_attempted\"] = total_records\n",
    "    report[\"parsed\"] = valid_records\n",
    "    \n",
    "    if total_records > 0:\n",
    "        report[\"success_rate\"] = (valid_records / total_records) * 100\n",
    "    else:\n",
    "        report[\"success_rate\"] = 0\n",
    "        \n",
    "    return valid_records, report\n",
    "\n",
    "def get_institution_specific_marc_files() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get all institution-specific MARC files from processed outputs\"\"\"\n",
    "    institution_file_pairs = []\n",
    "    \n",
    "    # Update base path for PySpark notebook environment\n",
    "    base_path = \"/home/jovyan/work/July-2025-PODParquet\"\n",
    "    \n",
    "    # PRIMARY: Look for processed MARC files in the final output directory\n",
    "    final_dir = os.path.join(base_path, 'pod-processing-outputs/final')\n",
    "    \n",
    "    if os.path.exists(final_dir):\n",
    "        # Get all .mrc files from the final directory\n",
    "        final_marc_files = glob.glob(os.path.join(final_dir, '*.mrc'))\n",
    "        \n",
    "        for file in final_marc_files:\n",
    "            # Extract institution from filename (e.g., \"harvard_updates-001.mrc\" -> \"harvard\")\n",
    "            institution = extract_institution_from_filename(file)\n",
    "            institution_file_pairs.append((institution, file))\n",
    "            \n",
    "        print(f\"Found {len(final_marc_files)} processed MARC files in {final_dir}\")\n",
    "    \n",
    "    # SECONDARY: Check the export directory for the latest export package\n",
    "    export_dir = os.path.join(base_path, 'pod-processing-outputs/export')\n",
    "    if os.path.exists(export_dir) and not institution_file_pairs:\n",
    "        # Find the most recent export package\n",
    "        export_packages = glob.glob(os.path.join(export_dir, 'marc_export_*'))\n",
    "        if export_packages:\n",
    "            latest_export = sorted(export_packages)[-1]  # Get most recent by timestamp\n",
    "            export_marc_files = glob.glob(os.path.join(latest_export, '*.mrc'))\n",
    "            \n",
    "            for file in export_marc_files:\n",
    "                # Skip non-MARC files\n",
    "                if file.endswith('.txt'):\n",
    "                    continue\n",
    "                institution = extract_institution_from_filename(file)\n",
    "                institution_file_pairs.append((institution, file))\n",
    "            \n",
    "            print(f\"Found {len(export_marc_files)} MARC files in latest export: {latest_export}\")\n",
    "    \n",
    "    # FALLBACK: If no processed files found, check for raw files\n",
    "    if not institution_file_pairs:\n",
    "        print(\"No processed files found in pod-processing-outputs/final or export directories\")\n",
    "        print(\"Falling back to raw MARC files in pod_*/file directories\")\n",
    "        \n",
    "        # Look for marc files in institution directories\n",
    "        institution_dirs = glob.glob(os.path.join(base_path, \"pod_*/file\"))\n",
    "        \n",
    "        for institution_dir in institution_dirs:\n",
    "            institution = os.path.basename(os.path.dirname(institution_dir)).replace('pod_', '')\n",
    "            \n",
    "            # Look for .mrc files only (no XML)\n",
    "            mrc_files = glob.glob(f\"{institution_dir}/**/*.mrc\", recursive=True)\n",
    "            for file in mrc_files:\n",
    "                institution_file_pairs.append((institution, file))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_pairs = list(set(institution_file_pairs))\n",
    "    unique_pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    print(f\"\\nTotal institution-specific MARC files to process: {len(unique_pairs)}\")\n",
    "    for institution, file in unique_pairs:\n",
    "        print(f\"  - {institution}: {file}\")\n",
    "    \n",
    "    return unique_pairs\n",
    "\n",
    "def process_file_with_recovery(file: str, institution: str) -> bool:\n",
    "    \"\"\"Process a MARC file with maximum error recovery\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        # Create a temporary file for processing\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                           f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "       \n",
    "        # Process MARC file\n",
    "        written_count, report = safe_read_marc_file_with_recovery(file, temp_file)\n",
    "        \n",
    "        # Proceed if we have at least some records\n",
    "        if written_count == 0:\n",
    "            error_msg = f\"No records could be processed from {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Run marctable command - FLDR is included by default\n",
    "        marctable_cmd = f'marctable parquet {temp_file} {output_file}'\n",
    "        marctable_msg = f\"Running marctable: {marctable_cmd}\"\n",
    "        logger.info(marctable_msg)\n",
    "        print(marctable_msg)\n",
    "        exit_status = os.system(marctable_cmd)\n",
    "        \n",
    "        if exit_status != 0:\n",
    "            error_msg = f\"marctable command failed for {institution} file {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        else:\n",
    "            success_msg = f\"SUCCESS: Created {output_file} with {written_count} {institution} records ({report.get('success_rate', 0):.1f}% success rate)\"\n",
    "            logger.info(success_msg)\n",
    "            print(success_msg)\n",
    "            print(f\"  Note: FLDR (leader) field is included by default in marctable output\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error processing {institution} file {file}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if 'temp_file' in locals() and temp_file and os.path.exists(temp_file):\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error for {temp_file}: {str(e)}\")\n",
    "\n",
    "def marc2parquet_institution_specific(force_reprocess=False):\n",
    "    \"\"\"\n",
    "    Convert institution-specific MARC to Parquet with maximum error recovery\n",
    "    \n",
    "    Args:\n",
    "        force_reprocess: If True, reprocess even if parquet files already exist\n",
    "    \"\"\"\n",
    "    # Check if previous processing has been done\n",
    "    if not os.path.exists(f'{output_dir}/final'):\n",
    "        print(f\"WARNING: No processed files found in {output_dir}/final/\")\n",
    "        print(\"Consider running ivyplus-updated-marc-pyspark.ipynb first for better results\")\n",
    "    \n",
    "    institution_file_pairs = get_institution_specific_marc_files()\n",
    "    \n",
    "    if not institution_file_pairs:\n",
    "        error_msg = \"No institution-specific MARC files found to process\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "    \n",
    "    results = []\n",
    "    institution_summary = {}\n",
    "    \n",
    "    for institution, file in institution_file_pairs:\n",
    "        if institution not in institution_summary:\n",
    "            institution_summary[institution] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "        \n",
    "        institution_summary[institution][\"total\"] += 1\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                   f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "        # Skip if already processed unless force_reprocess is True\n",
    "        if not force_reprocess and os.path.exists(output_file):\n",
    "            skip_msg = f\"Skipping already processed {institution} file {file}\"\n",
    "            logger.info(skip_msg)\n",
    "            print(skip_msg)\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "            results.append(True)\n",
    "            continue\n",
    "            \n",
    "        result = process_file_with_recovery(file, institution)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result:\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "        else:\n",
    "            institution_summary[institution][\"failed\"] += 1\n",
    "    \n",
    "    # Print summary by institution\n",
    "    print(\"\\n=== Institution Processing Summary ===\")\n",
    "    for institution, stats in institution_summary.items():\n",
    "        print(f\"{institution.upper()}: Processed {stats['total']} files - {stats['success']} succeeded, {stats['failed']} failed\")\n",
    "    \n",
    "    # Overall success rate\n",
    "    total_success = sum(results)\n",
    "    total_files = len(results)\n",
    "    if total_files > 0:\n",
    "        print(f\"\\nOverall: Successfully processed {total_success} of {total_files} files ({total_success/total_files*100:.1f}%)\")\n",
    "        return total_success == total_files\n",
    "    else:\n",
    "        print(\"\\nNo files were processed\")\n",
    "        return False\n",
    "\n",
    "# Check if conversion is needed or if we can skip directly to processing\n",
    "print(\"Checking for existing parquet files...\")\n",
    "existing_parquet = glob.glob(f\"{output_dir}/*_marc21.parquet\")\n",
    "if existing_parquet:\n",
    "    print(f\"Found {len(existing_parquet)} existing parquet files\")\n",
    "    print(\"You can skip to the next cell unless you want to reprocess\")\n",
    "else:\n",
    "    print(\"No parquet files found. Running conversion...\")\n",
    "    marc2parquet_institution_specific()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 institution-specific parquet files to process\n",
      "ERROR: No parquet files found!\n",
      "Looking in: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/\n",
      "Found 13 files in /home/jovyan/work/July-2025-PODParquet\n",
      "Files found:\n",
      "  - /home/jovyan/work/July-2025-PODParquet/dartmouth_dartmouth_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/stanford_stanford_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/yale_yale_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/chicago_chicago_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/columbia_columbia_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/princeton_princeton_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/duke_duke_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/mit_mit_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/cornell_cornell_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/harvard_harvard_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/brown_brown_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/penn_penn_filtered-marc21.parquet\n",
      "  - /home/jovyan/work/July-2025-PODParquet/johns_johns_filtered-marc21.parquet\n",
      "\n",
      "=== Processing institutions and saving intermediate results ===\n",
      "\n",
      "Processing dartmouth...\n",
      "  - 3,855,421 records processed\n",
      "  - 3,854,197 (100.0%) with valid match keys\n",
      "\n",
      "Processing stanford...\n",
      "  - 976,982 records processed\n",
      "  - 976,754 (100.0%) with valid match keys\n",
      "\n",
      "Processing yale...\n",
      "  - 7,431,259 records processed\n",
      "  - 7,430,922 (100.0%) with valid match keys\n",
      "\n",
      "Processing chicago...\n",
      "  - 12,294,163 records processed\n",
      "  - 12,291,171 (100.0%) with valid match keys\n",
      "\n",
      "Processing columbia...\n",
      "  - 16,836,893 records processed\n",
      "  - 16,835,374 (100.0%) with valid match keys\n",
      "\n",
      "Processing princeton...\n",
      "  - 30,228,583 records processed\n",
      "  - 30,211,547 (99.9%) with valid match keys\n",
      "\n",
      "Processing duke...\n",
      "  - 10,549,680 records processed\n",
      "  - 10,101,071 (95.7%) with valid match keys\n",
      "\n",
      "Processing mit...\n",
      "  - 5,338,129 records processed\n",
      "  - 5,337,568 (100.0%) with valid match keys\n",
      "\n",
      "Processing cornell...\n",
      "  - 6,944,453 records processed\n",
      "  - 6,937,337 (99.9%) with valid match keys\n",
      "\n",
      "Processing harvard...\n",
      "  - 54,109,207 records processed\n",
      "  - 54,084,570 (100.0%) with valid match keys\n",
      "\n",
      "Processing brown...\n",
      "  - 737,290 records processed\n",
      "  - 736,759 (99.9%) with valid match keys\n",
      "\n",
      "Processing penn...\n",
      "  - 3,663,990 records processed\n",
      "  - 3,658,497 (99.9%) with valid match keys\n",
      "\n",
      "Processing johns...\n",
      "  - 3,416,688 records processed\n",
      "  - 3,416,521 (100.0%) with valid match keys\n",
      "\n",
      "âœ… Processed 13 institutions\n",
      "\n",
      "=== Reading all processed data as unified dataset ===\n",
      "\n",
      "=== Computing statistics ===\n",
      "\n",
      "Match key validation results:\n",
      "  â€¢ Valid match keys: 155,872,288 (99.7%)\n",
      "  â€¢ Invalid match keys: 510,450 (0.3%)\n",
      "\n",
      "Invalid match key reasons:\n",
      "+-------------------+------+\n",
      "|match_key_message  |count |\n",
      "+-------------------+------+\n",
      "|Match key too short|510450|\n",
      "+-------------------+------+\n",
      "\n",
      "\n",
      "=== Checking data quality ===\n",
      "Data quality by institution:\n",
      "Institution  Total      Empty Arrays Null Arrays\n",
      "--------------------------------------------------\n",
      "brown        737,290    0            0         \n",
      "chicago      12,294,163 0            0         \n",
      "columbia     16,836,893 0            0         \n",
      "cornell      6,944,453  0            0         \n",
      "dartmouth    3,855,421  0            0         \n",
      "duke         10,549,680 0            0         \n",
      "harvard      54,109,207 0            0         \n",
      "johns        3,416,688  0            0         \n",
      "mit          5,338,129  0            0         \n",
      "penn         3,663,990  0            0         \n",
      "princeton    30,228,583 0            0         \n",
      "stanford     976,982    0            0         \n",
      "yale         7,431,259  0            0         \n",
      "\n",
      "=== Creating exploded dataset ===\n",
      "\n",
      "Penn records in exploded dataset: 3,663,990\n",
      "\n",
      "ðŸ“Š Key array statistics:\n",
      "Distribution of number of keys per record:\n",
      "  1 keys: 156,382,738 records\n",
      "\n",
      "âœ… Memory-optimized processing complete!\n",
      "Intermediate results saved to: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed\n",
      "Exploded dataset saved to: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/all_records_exploded.parquet\n"
     ]
    }
   ],
   "source": [
    "# Main Processing with Memory-Optimized Approach\n",
    "from pyspark.sql.functions import col, explode, size, array_contains, collect_set\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Find all institution parquet files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "parquet_files = glob.glob(f\"{input_dir}/pod-processing-outputs/*-marc21.parquet\")\n",
    "\n",
    "print(f\"Found {len(parquet_files)} institution-specific parquet files to process\")\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"ERROR: No parquet files found!\")\n",
    "    print(f\"Looking in: {input_dir}/pod-processing-outputs/\")\n",
    "    parquet_files = glob.glob(f\"{input_dir}/*-marc21.parquet\")\n",
    "    if parquet_files:\n",
    "        print(f\"Found {len(parquet_files)} files in {input_dir}\")\n",
    "    else:\n",
    "        raise ValueError(\"No parquet files found in the specified directory\")\n",
    "\n",
    "print(\"Files found:\")\n",
    "for file in parquet_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# MEMORY OPTIMIZATION 1: Process and save each institution separately\n",
    "print(\"\\n=== Processing institutions and saving intermediate results ===\")\n",
    "\n",
    "temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "processed_institutions = []\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    try:\n",
    "        filename = os.path.basename(parquet_file)\n",
    "        institution = filename.split('_')[0]\n",
    "        \n",
    "        print(f\"\\nProcessing {institution}...\")\n",
    "        \n",
    "        # Read and process without caching\n",
    "        df = spark.read.parquet(parquet_file)\n",
    "        processed_df = process_institution_optimized(df, institution)\n",
    "        \n",
    "        # Select only needed columns immediately\n",
    "        slim_df = processed_df.select(\n",
    "            \"F001\", \"source\", \"match_key\", \"id_list\", \n",
    "            \"is_valid_match_key\", \"match_key_message\"\n",
    "        )\n",
    "        \n",
    "        # Write to temp location instead of keeping in memory\n",
    "        temp_path = f\"{temp_output_dir}/{institution}_processed.parquet\"\n",
    "        slim_df.coalesce(20).write.mode(\"overwrite\").parquet(temp_path)\n",
    "        \n",
    "        # Get statistics before releasing memory\n",
    "        total_records = slim_df.count()\n",
    "        if total_records > 0:\n",
    "            valid_keys = slim_df.filter(col(\"is_valid_match_key\") == True).count()\n",
    "            print(f\"  - {total_records:,} records processed\")\n",
    "            print(f\"  - {valid_keys:,} ({valid_keys/total_records*100:.1f}%) with valid match keys\")\n",
    "            processed_institutions.append((institution, temp_path))\n",
    "        else:\n",
    "            print(f\"  - WARNING: No records found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  - ERROR processing {institution}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(processed_institutions)} institutions\")\n",
    "\n",
    "# MEMORY OPTIMIZATION 2: Read all processed files as a single union\n",
    "print(\"\\n=== Reading all processed data as unified dataset ===\")\n",
    "\n",
    "# Build a list of paths to read\n",
    "processed_paths = [path for _, path in processed_institutions]\n",
    "\n",
    "# Read all at once using Spark's optimized union reading\n",
    "all_df = spark.read.parquet(*processed_paths)\n",
    "\n",
    "# FIX: Create key_array handling NULL id_list properly\n",
    "all_df = all_df.withColumn(\"key_array\",\n",
    "    F.array_distinct(\n",
    "        F.concat(\n",
    "            # If id_list is null, use empty array instead\n",
    "            F.coalesce(F.col(\"id_list\"), F.array()),\n",
    "            F.array(F.col(\"match_key\"))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# MEMORY OPTIMIZATION 3: Get statistics using aggregations instead of count()\n",
    "print(\"\\n=== Computing statistics ===\")\n",
    "\n",
    "# Get all statistics in one pass\n",
    "stats_df = all_df.agg(\n",
    "    F.count(\"*\").alias(\"total_keys\"),\n",
    "    F.sum(F.when(col(\"is_valid_match_key\") == True, 1).otherwise(0)).alias(\"valid_keys\"),\n",
    "    F.sum(F.when(col(\"is_valid_match_key\") == False, 1).otherwise(0)).alias(\"invalid_keys\")\n",
    ").collect()[0]\n",
    "\n",
    "total_keys = stats_df[\"total_keys\"]\n",
    "valid_keys = stats_df[\"valid_keys\"]\n",
    "invalid_keys = stats_df[\"invalid_keys\"]\n",
    "\n",
    "if total_keys > 0:\n",
    "    print(f\"\\nMatch key validation results:\")\n",
    "    print(f\"  â€¢ Valid match keys: {valid_keys:,} ({valid_keys/total_keys*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Invalid match keys: {invalid_keys:,} ({invalid_keys/total_keys*100:.1f}%)\")\n",
    "    \n",
    "    if invalid_keys > 0:\n",
    "        print(\"\\nInvalid match key reasons:\")\n",
    "        all_df.filter(col(\"is_valid_match_key\") == False) \\\n",
    "            .groupBy(\"match_key_message\") \\\n",
    "            .count() \\\n",
    "            .orderBy(col(\"count\").desc()) \\\n",
    "            .show(10, False)\n",
    "\n",
    "# Check data quality before exploding\n",
    "print(\"\\n=== Checking data quality ===\")\n",
    "quality_check = all_df.groupBy(\"source\").agg(\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.sum(F.when(F.size(col(\"key_array\")) == 0, 1).otherwise(0)).alias(\"empty_key_arrays\"),\n",
    "    F.sum(F.when(col(\"key_array\").isNull(), 1).otherwise(0)).alias(\"null_key_arrays\")\n",
    ").orderBy(\"source\").collect()\n",
    "\n",
    "print(\"Data quality by institution:\")\n",
    "print(f\"{'Institution':<12} {'Total':<10} {'Empty Arrays':<12} {'Null Arrays':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for row in quality_check:\n",
    "    print(f\"{row['source']:<12} {row['total_records']:<10,} {row['empty_key_arrays']:<12,} {row['null_key_arrays']:<10,}\")\n",
    "\n",
    "# MEMORY OPTIMIZATION 4: Explode and save immediately\n",
    "print(\"\\n=== Creating exploded dataset ===\")\n",
    "\n",
    "# Explode and write to disk instead of keeping in memory\n",
    "all_df_exploded = all_df.withColumn(\"key\", explode(\"key_array\"))\n",
    "\n",
    "# Save the exploded dataset for downstream processing\n",
    "exploded_path = f\"{output_dir}/all_records_exploded.parquet\"\n",
    "all_df_exploded.write.mode(\"overwrite\").parquet(exploded_path)\n",
    "\n",
    "# Verify Penn made it through\n",
    "penn_exploded_count = all_df_exploded.filter(col(\"source\") == \"penn\").count()\n",
    "print(f\"\\nPenn records in exploded dataset: {penn_exploded_count:,}\")\n",
    "\n",
    "# Get key statistics efficiently\n",
    "print(\"\\nðŸ“Š Key array statistics:\")\n",
    "key_stats = all_df.select(F.size(\"key_array\").alias(\"num_keys\")) \\\n",
    "    .groupBy(\"num_keys\").count() \\\n",
    "    .orderBy(\"num_keys\") \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Distribution of number of keys per record:\")\n",
    "for row in key_stats:\n",
    "    print(f\"  {row['num_keys']} keys: {row['count']:,} records\")\n",
    "\n",
    "# For next processing step, read from disk\n",
    "all_df_exploded = spark.read.parquet(exploded_path)\n",
    "\n",
    "print(\"\\nâœ… Memory-optimized processing complete!\")\n",
    "print(f\"Intermediate results saved to: {temp_output_dir}\")\n",
    "print(f\"Exploded dataset saved to: {exploded_path}\")\n",
    "\n",
    "# Optional: Clean up temp files after verification\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis Results ===\n",
      "Total Penn records: 2,443,080\n",
      "Unique Penn records: 1,596,684\n",
      "Uniqueness rate: 65.4%\n",
      "Overlap rate: 34.6%\n",
      "\n",
      "=== Overlap Analysis ===\n",
      "Distribution of Penn records by number of libraries holding them:\n",
      "+-------------+-------+\n",
      "|num_libraries|  count|\n",
      "+-------------+-------+\n",
      "|            1|1508403|\n",
      "|            2| 370409|\n",
      "|            3| 210325|\n",
      "|            4|  85220|\n",
      "|            5|  55123|\n",
      "|            6|  38010|\n",
      "|            7|  27995|\n",
      "|            8|  20915|\n",
      "|            9|  15442|\n",
      "|           10|  10597|\n",
      "|           11|   5921|\n",
      "|           12|   1795|\n",
      "|           13|    236|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[F001: string, source: string, match_key: string, id_list: array<string>, is_valid_match_key: boolean, match_key_message: string, key_array: array<string>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniqueness Analysis and Overlap Detection\n",
    "from pyspark.sql.functions import collect_set, array_contains, size, col\n",
    "import glob  # Add this import for the fallback logic\n",
    "\n",
    "# IMPORTANT: Load the exploded dataset from disk first\n",
    "exploded_path = f\"{output_dir}/all_records_exploded.parquet\"\n",
    "all_df_exploded = spark.read.parquet(exploded_path)\n",
    "\n",
    "# Group by key and collect sources where that key appears\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# Broadcast the grouped DataFrame for more efficient joins\n",
    "grouped_broadcast = F.broadcast(grouped)\n",
    "\n",
    "# Find Penn records that exist in OTHER libraries\n",
    "# A Penn record is NOT unique if it exists in ANY other library\n",
    "penn_keys_in_other_libs = grouped_broadcast.filter(\n",
    "    (array_contains(col(\"sources\"), \"penn\")) & \n",
    "    (F.size(col(\"sources\")) > 1)  # Penn + at least one other library\n",
    ").select(\"key\")\n",
    "\n",
    "# Get Penn records that are truly unique to Penn\n",
    "# Start with all Penn records\n",
    "all_penn_exploded = all_df_exploded.filter(col(\"source\") == \"penn\")\n",
    "\n",
    "# Anti-join to remove Penn records found in other libraries\n",
    "unique_penn_exploded = all_penn_exploded.join(\n",
    "    penn_keys_in_other_libs,  # No need to re-broadcast\n",
    "    on=\"key\", \n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Deduplicate by Penn's F001 (not match_key) to get unique Penn records\n",
    "unique_penn = unique_penn_exploded.drop(\"key\").dropDuplicates([\"F001\"])\n",
    "\n",
    "# Cache the unique Penn records for better performance\n",
    "unique_penn.cache()\n",
    "\n",
    "# Calculate statistics efficiently\n",
    "unique_penn_count = unique_penn.count()  # Force cache materialization\n",
    "\n",
    "# Get total Penn records from the deduplicated exploded DataFrame\n",
    "total_penn = all_penn_exploded.select(\"F001\").distinct().count()\n",
    "\n",
    "print(f\"\\n=== Analysis Results ===\")\n",
    "print(f\"Total Penn records: {total_penn:,}\")\n",
    "print(f\"Unique Penn records: {unique_penn_count:,}\")\n",
    "\n",
    "# Add robust checking for division by zero\n",
    "if total_penn > 0:\n",
    "    print(f\"Uniqueness rate: {unique_penn_count/total_penn*100:.1f}%\")\n",
    "    print(f\"Overlap rate: {(total_penn - unique_penn_count)/total_penn*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Uniqueness rate: N/A (no Penn records found)\")\n",
    "\n",
    "# For analysis, let's also see overlap statistics\n",
    "print(\"\\n=== Overlap Analysis ===\")\n",
    "\n",
    "# More efficient: get Penn overlap stats without re-filtering\n",
    "penn_keys = grouped.filter(array_contains(col(\"sources\"), \"penn\")).cache()\n",
    "\n",
    "penn_overlap_stats = penn_keys \\\n",
    "    .withColumn(\"num_libraries\", F.size(col(\"sources\"))) \\\n",
    "    .groupBy(\"num_libraries\").count() \\\n",
    "    .orderBy(\"num_libraries\")\n",
    "\n",
    "print(\"Distribution of Penn records by number of libraries holding them:\")\n",
    "penn_overlap_stats.show()\n",
    "\n",
    "# Save results with consistent paths using pod-processing-outputs directory\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Save unique Penn records\n",
    "unique_penn.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "\n",
    "# Save detailed overlap information for analysis\n",
    "# Note: Using cached penn_keys for efficiency\n",
    "penn_with_overlap_info = all_penn_exploded.join(\n",
    "    penn_keys.select(\"key\", \"sources\", F.size(\"sources\").alias(\"num_libraries\")),\n",
    "    on=\"key\",\n",
    "    how=\"left\"\n",
    ").drop(\"key\")\n",
    "\n",
    "penn_with_overlap_info.write.mode(\"overwrite\").parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "\n",
    "# Load all_df from intermediate files for validation statistics\n",
    "# Check if processed_institutions variable exists from previous cell\n",
    "if 'processed_institutions' in locals():\n",
    "    # Use the paths from the previous processing\n",
    "    processed_paths = [path for _, path in processed_institutions]\n",
    "    all_df = spark.read.parquet(*processed_paths)\n",
    "else:\n",
    "    # Fallback: read from temp directory\n",
    "    temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "    processed_paths = glob.glob(f\"{temp_output_dir}/*_processed.parquet\")\n",
    "    if processed_paths:\n",
    "        all_df = spark.read.parquet(*processed_paths)\n",
    "    else:\n",
    "        print(\"WARNING: Could not load all_df for validation statistics\")\n",
    "        print(\"Skipping validation statistics save\")\n",
    "        all_df = None\n",
    "\n",
    "# Save validation statistics for analysis if all_df is available\n",
    "if all_df is not None:\n",
    "    validation_stats = all_df.select(\"F001\", \"match_key\", \"is_valid_match_key\", \"match_key_message\", \"id_list\") \\\n",
    "        .filter(col(\"source\") == \"penn\")\n",
    "    \n",
    "    validation_stats.write.mode(\"overwrite\").parquet(f\"{output_dir}/match_key_validation_stats.parquet\")\n",
    "else:\n",
    "    print(\"Validation statistics not saved due to missing all_df\")\n",
    "\n",
    "# Unpersist cached DataFrames to free memory\n",
    "penn_keys.unpersist()\n",
    "unique_penn.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing unique_penn DataFrame\n",
      "\n",
      "=== PENN DATA SOURCE VERIFICATION ===\n",
      "\n",
      "Found Penn records at: /home/jovyan/work/July-2025-PODParquet/penn_penn_filtered-marc21.parquet\n",
      "  - Source type: Raw data\n",
      "  - Data date: unknown\n",
      "  - Total records: 3,663,990\n",
      "  - Recently updated records (2024-2025): 609,205 (16.6%)\n",
      "\n",
      "=== PROCEEDING WITH ANALYSIS ===\n",
      "\n",
      "=== Pre-Join Dataset Verification ===\n",
      "Penn full dataset columns (216 total):\n",
      "Sample columns: FLDR, F001, F003, F005, F006, F007, F008, F010, F013, F015...\n",
      "\n",
      "=== Post-Join Dataset Verification ===\n",
      "Joined dataset columns (216 total):\n",
      "Sample columns: F001, FLDR, F003, F005, F006, F007, F008, F010, F013, F015...\n",
      "\n",
      "=== Checking available columns for filtering ===\n",
      "Looking for F533 column to filter reproduction notes...\n",
      "Filtering out records with F533 (reproduction notes)\n",
      "\n",
      "=== Material Type Distribution ===\n",
      "print_book: 1,713,978\n",
      "other: 121,437\n",
      "visual_material: 99,359\n",
      "print_serial: 51,287\n",
      "print_music: 19,096\n",
      "electronic_resource: 2,627\n",
      "print_maps: 2,109\n",
      "audio_material: 1,667\n",
      "\n",
      "=== Print Material Analysis ===\n",
      "Total unique Penn records: 2,011,560\n",
      "Print materials: 1,786,470 (88.8%)\n",
      "Non-print materials: 225,090 (11.2%)\n",
      "\n",
      "=== Print Material Categories ===\n",
      "print_book: 1,713,978 (95.9% of print materials)\n",
      "print_serial: 51,287 (2.9% of print materials)\n",
      "print_music: 19,096 (1.1% of print materials)\n",
      "print_maps: 2,109 (0.1% of print materials)\n"
     ]
    }
   ],
   "source": [
    "# Data Source Validation (Updated: July 2025)\n",
    "# Validates Penn MARC data sources and ensures current data is used\n",
    "# Requires explicit confirmation for legacy data usage\n",
    "\n",
    "# Use Leader field FLDR to make a print set from unique penn and non-print\n",
    "from pyspark.sql.functions import col, substring, when, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load the unique Penn dataset if not already loaded\n",
    "if 'unique_penn' not in locals() or unique_penn is None:\n",
    "    print(\"Loading unique Penn dataset...\")\n",
    "    unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "else:\n",
    "    print(\"Using existing unique_penn DataFrame\")\n",
    "\n",
    "# CRITICAL: Verify Penn data currency before processing\n",
    "def verify_penn_data_source(matching_files):\n",
    "    \"\"\"\n",
    "    Verify the Penn data source and warn if using outdated data\n",
    "    \"\"\"\n",
    "    if not matching_files:\n",
    "        return None\n",
    "        \n",
    "    selected_file = matching_files[0]\n",
    "    file_info = {\n",
    "        'path': selected_file,\n",
    "        'filename': os.path.basename(selected_file),\n",
    "        'is_legacy': 'penn-2022-07-20' in selected_file,\n",
    "        'is_processed': 'pod-processing-outputs' in selected_file\n",
    "    }\n",
    "    \n",
    "    # Extract date from filename if possible\n",
    "    date_pattern = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "    date_match = re.search(date_pattern, file_info['filename'])\n",
    "    if date_match:\n",
    "        file_info['data_date'] = date_match.group(1)\n",
    "    else:\n",
    "        file_info['data_date'] = 'unknown'\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Load full Penn records - prioritize most recent processed data\n",
    "penn_full_paths = [\n",
    "    # PRIMARY: Direct path to known Penn data\n",
    "    f\"{input_dir}/penn_penn_filtered-marc21.parquet\",\n",
    "    \n",
    "    # SECONDARY: Penn parquet files from current processing pipeline\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*updates*marc21.parquet\",\n",
    "    \n",
    "    # TERTIARY: Any Penn marc21 parquet files in processing outputs\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*marc21.parquet\",\n",
    "    \n",
    "    # QUATERNARY: Check for raw Penn parquet files (less preferred)\n",
    "    f\"{input_dir}/pod_penn/file/**/*.parquet\"\n",
    "]\n",
    "\n",
    "# Add data source verification\n",
    "penn_full = None\n",
    "selected_source = None\n",
    "\n",
    "print(\"\\n=== PENN DATA SOURCE VERIFICATION ===\")\n",
    "for path_pattern in penn_full_paths:\n",
    "    try:\n",
    "        matching_files = glob.glob(path_pattern, recursive=True)\n",
    "        if matching_files:\n",
    "            # Sort files by modification time to get most recent\n",
    "            matching_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "            \n",
    "            source_info = verify_penn_data_source(matching_files)\n",
    "            if source_info:\n",
    "                print(f\"\\nFound Penn records at: {source_info['path']}\")\n",
    "                print(f\"  - Source type: {'Processed updates' if source_info['is_processed'] else 'Raw data'}\")\n",
    "                print(f\"  - Data date: {source_info['data_date']}\")\n",
    "                \n",
    "                # Warn if data appears old\n",
    "                if source_info['data_date'] != 'unknown':\n",
    "                    try:\n",
    "                        data_date = datetime.strptime(source_info['data_date'], '%Y-%m-%d')\n",
    "                        days_old = (datetime.now() - data_date).days\n",
    "                        if days_old > 365:\n",
    "                            print(f\"  âš ï¸  WARNING: Data is {days_old} days old!\")\n",
    "                            print(f\"  âš ï¸  Results may not reflect current Penn holdings\")\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Load the data\n",
    "                penn_full = spark.read.parquet(source_info['path'])\n",
    "                \n",
    "                # CRITICAL: Verify this is a MARC dataset with FLDR field\n",
    "                if \"FLDR\" not in penn_full.columns:\n",
    "                    print(f\"  âš ï¸  WARNING: File does not contain FLDR field - not a valid MARC dataset\")\n",
    "                    penn_full = None\n",
    "                    continue\n",
    "                \n",
    "                selected_source = source_info\n",
    "                \n",
    "                # Verify record count and sample for currency check\n",
    "                record_count = penn_full.count()\n",
    "                print(f\"  - Total records: {record_count:,}\")\n",
    "                \n",
    "                # Sample check for recent cataloging activity\n",
    "                if 'F005' in penn_full.columns:\n",
    "                    recent_updates = penn_full.filter(\n",
    "                        col(\"F005\").rlike(\"202[4-5]\")\n",
    "                    ).count()\n",
    "                    recent_percentage = (recent_updates / record_count * 100) if record_count > 0 else 0\n",
    "                    print(f\"  - Recently updated records (2024-2025): {recent_updates:,} ({recent_percentage:.1f}%)\")\n",
    "                    \n",
    "                    if recent_percentage < 5:\n",
    "                        print(f\"  âš ï¸  WARNING: Only {recent_percentage:.1f}% of records updated recently\")\n",
    "                        print(f\"  âš ï¸  Data may be significantly outdated\")\n",
    "                \n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {path_pattern}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# If no MARC file with FLDR found, search broader for MARC datasets\n",
    "if penn_full is None or \"FLDR\" not in penn_full.columns:\n",
    "    print(\"\\nâš ï¸  MARC files without FLDR field detected! Searching for proper MARC datasets...\")\n",
    "    \n",
    "    # Search for any MARC21 parquet files\n",
    "    marc_paths = glob.glob(f\"{input_dir}/**/*marc21*.parquet\", recursive=True)\n",
    "    \n",
    "    if marc_paths:\n",
    "        print(f\"Found {len(marc_paths)} potential MARC datasets\")\n",
    "        for path in marc_paths:\n",
    "            try:\n",
    "                test_df = spark.read.parquet(path)\n",
    "                if \"FLDR\" in test_df.columns:\n",
    "                    # Verify this is a Penn dataset\n",
    "                    filename = os.path.basename(path)\n",
    "                    if \"penn\" in filename.lower():\n",
    "                        print(f\"âœ… Found valid Penn MARC dataset with FLDR field: {path}\")\n",
    "                        penn_full = test_df\n",
    "                        selected_source = {\n",
    "                            'path': path,\n",
    "                            'filename': filename,\n",
    "                            'is_legacy': 'penn-2022-07-20' in path,\n",
    "                            'is_processed': 'pod-processing-outputs' in path\n",
    "                        }\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {path}: {str(e)}\")\n",
    "\n",
    "# Final fallback with strong warning\n",
    "if penn_full is None:\n",
    "    print(\"\\nâš ï¸  CRITICAL WARNING: No current Penn data found!\")\n",
    "    print(\"As a last resort, checking for legacy data...\")\n",
    "    \n",
    "    legacy_path = \"/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet\"\n",
    "    if os.path.exists(legacy_path):\n",
    "        response = input(\"\\nðŸš¨ Found 2022 Penn data. This is SEVERELY OUTDATED. Use anyway? (yes/no): \")\n",
    "        if response.lower() == 'yes':\n",
    "            penn_full = spark.read.parquet(legacy_path)\n",
    "            selected_source = {'is_legacy': True, 'filename': 'penn-2022-07-20-full-marc21.parquet'}\n",
    "            print(\"âš ï¸  Using 2022 data - results will NOT reflect current Penn holdings!\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No Penn full MARC records found and user declined legacy data\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not find Penn full MARC records!\")\n",
    "        print(\"Please ensure Penn MARC data has been converted to Parquet format.\")\n",
    "        print(\"Run the previous cells to process MARC files first.\")\n",
    "        raise FileNotFoundError(\"Penn full MARC records not found\")\n",
    "\n",
    "print(\"\\n=== PROCEEDING WITH ANALYSIS ===\")\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"âš ï¸  USING OUTDATED DATA - RESULTS MAY BE INACCURATE\")\n",
    "\n",
    "# CRITICAL: Verify penn_full dataset has the required MARC fields\n",
    "print(\"\\n=== Pre-Join Dataset Verification ===\")\n",
    "print(f\"Penn full dataset columns ({len(penn_full.columns)} total):\")\n",
    "# Print first 10 columns as a sample\n",
    "print(f\"Sample columns: {', '.join(penn_full.columns[:10])}...\")\n",
    "\n",
    "if \"FLDR\" not in penn_full.columns:\n",
    "    raise ValueError(\"ERROR: Penn dataset is missing the FLDR field required for analysis!\")\n",
    "\n",
    "# OPTIMIZATION: Use broadcast join for better performance with small unique_penn_ids DataFrame\n",
    "unique_penn_ids = unique_penn.select(\"F001\").distinct()\n",
    "unique_penn_full = penn_full.join(F.broadcast(unique_penn_ids), on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Verify join kept FLDR column\n",
    "print(\"\\n=== Post-Join Dataset Verification ===\")\n",
    "print(f\"Joined dataset columns ({len(unique_penn_full.columns)} total):\")\n",
    "# Print first 10 columns as a sample\n",
    "print(f\"Sample columns: {', '.join(unique_penn_full.columns[:10])}...\")\n",
    "\n",
    "if \"FLDR\" not in unique_penn_full.columns:\n",
    "    raise ValueError(\"ERROR: FLDR column was lost during join operation!\")\n",
    "\n",
    "# Check available columns before filtering\n",
    "print(\"\\n=== Checking available columns for filtering ===\")\n",
    "available_columns = unique_penn_full.columns\n",
    "print(f\"Looking for F533 column to filter reproduction notes...\")\n",
    "\n",
    "# Start with the base dataset\n",
    "df_with_material_type = unique_penn_full\n",
    "\n",
    "# Only apply F533 filter if the column exists\n",
    "if \"F533\" in available_columns:\n",
    "    print(\"Filtering out records with F533 (reproduction notes)\")\n",
    "    df_with_material_type = df_with_material_type.filter(col(\"F533\").isNull())\n",
    "else:\n",
    "    print(\"Note: F533 column not found in dataset, skipping reproduction filter\")\n",
    "\n",
    "# Continue with the rest of the transformations\n",
    "unique_penn_with_material_type = (df_with_material_type\n",
    "    # Add material type columns\n",
    "    .withColumn(\"record_type\", substring(col(\"FLDR\"), 7, 1))\n",
    "    .withColumn(\"bib_level\", substring(col(\"FLDR\"), 8, 1))\n",
    "    .withColumn(\"combined_type\", concat(col(\"record_type\"), col(\"bib_level\")))\n",
    "    .withColumn(\"material_category\", \n",
    "        when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"m\")), \"print_book\")\n",
    "        .when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"s\")), \"print_serial\")\n",
    "        .when((col(\"record_type\") == \"c\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_music\")\n",
    "        .when((col(\"record_type\") == \"e\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_maps\")\n",
    "        .when(col(\"record_type\") == \"m\", \"electronic_resource\")\n",
    "        .when(col(\"record_type\").isin(\"g\", \"k\"), \"visual_material\")\n",
    "        .when(col(\"record_type\") == \"i\", \"audio_material\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "    .withColumn(\"is_print\", \n",
    "        col(\"material_category\").isin(\"print_book\", \"print_serial\", \"print_music\", \"print_maps\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache before multiple operations\n",
    "unique_penn_with_material_type.cache()\n",
    "\n",
    "# OPTIMIZATION: Get all statistics in one pass\n",
    "print(\"\\n=== Material Type Distribution ===\")\n",
    "material_stats = unique_penn_with_material_type.groupBy(\"material_category\", \"is_print\").count().collect()\n",
    "\n",
    "# Process statistics\n",
    "material_counts_dict = {}\n",
    "print_count = 0\n",
    "non_print_count = 0\n",
    "\n",
    "for row in material_stats:\n",
    "    material_counts_dict[row[\"material_category\"]] = row[\"count\"]\n",
    "    if row[\"is_print\"]:\n",
    "        print_count += row[\"count\"]\n",
    "    else:\n",
    "        non_print_count += row[\"count\"]\n",
    "\n",
    "total_unique = print_count + non_print_count\n",
    "\n",
    "# Display material distribution\n",
    "for category, count in sorted(material_counts_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{category}: {count:,}\")\n",
    "\n",
    "# Filter for print materials only\n",
    "print_only_df = unique_penn_with_material_type.filter(col(\"is_print\") == True)\n",
    "\n",
    "# Add metadata if we have source information\n",
    "if selected_source:\n",
    "    print_only_df_with_metadata = print_only_df.withColumn(\n",
    "        \"processing_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    ).withColumn(\n",
    "        \"source_file\", lit(selected_source.get('filename', 'unknown'))\n",
    "    ).withColumn(\n",
    "        \"data_currency_warning\", \n",
    "        lit(\"OUTDATED - 2022 data\" if selected_source.get('is_legacy') else \"Current\")\n",
    "    )\n",
    "else:\n",
    "    print_only_df_with_metadata = print_only_df\n",
    "\n",
    "# Save datasets\n",
    "unique_penn_with_material_type.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn_full_no_533.parquet\")\n",
    "print_only_df_with_metadata.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\n=== Print Material Analysis ===\")\n",
    "print(f\"Total unique Penn records: {total_unique:,}\")\n",
    "\n",
    "if total_unique > 0:\n",
    "    print(f\"Print materials: {print_count:,} ({print_count/total_unique*100:.1f}%)\")\n",
    "    print(f\"Non-print materials: {non_print_count:,} ({non_print_count/total_unique*100:.1f}%)\")\n",
    "    \n",
    "    # Show print categories breakdown\n",
    "    print(\"\\n=== Print Material Categories ===\")\n",
    "    print_categories = [\"print_book\", \"print_serial\", \"print_music\", \"print_maps\"]\n",
    "    for category in print_categories:\n",
    "        if category in material_counts_dict:\n",
    "            count = material_counts_dict[category]\n",
    "            print(f\"{category}: {count:,} ({count/print_count*100:.1f}% of print materials)\")\n",
    "else:\n",
    "    print(\"No unique Penn records found to analyze\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "unique_penn_with_material_type.unpersist()\n",
    "\n",
    "# Final warning if using outdated data\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš¨ CRITICAL WARNING: Analysis completed using 2022 Penn data\")\n",
    "    print(\"ðŸš¨ Results do NOT reflect current Penn holdings\")\n",
    "    print(\"ðŸš¨ Recommended: Re-run with current Penn MARC export\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing print_only_df DataFrame\n",
      "Creating stratified sample based on material_category...\n",
      "Strata distribution:\n",
      "  - print_book: 1,713,978 records (95.94%)\n",
      "  - print_maps: 2,109 records (0.12%)\n",
      "  - print_music: 19,096 records (1.07%)\n",
      "  - print_serial: 51,287 records (2.87%)\n",
      "First pass sample size: 1035\n",
      "Final sample size: 1035\n",
      "\n",
      "Sample distribution by material_category:\n",
      "  - print_book: 973 (94.01% of sample vs 95.94% of population)\n",
      "  - print_maps: 5 (0.48% of sample vs 0.12% of population)\n",
      "  - print_music: 17 (1.64% of sample vs 1.07% of population)\n",
      "  - print_serial: 40 (3.86% of sample vs 2.87% of population)\n",
      "\n",
      "âœ… Processing complete!\n",
      "Results saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/\n",
      "\n",
      "Final outputs:\n",
      "  - unique_penn.parquet: All unique Penn records\n",
      "  - physical_books_no_533.parquet: Unique Penn physical books\n",
      "  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\n",
      "  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\n",
      "  - sample_summary_no_hsp.json: Summary statistics\n",
      "\n",
      "ðŸ“Š Summary Statistics:\n",
      "  - Total Penn records: 2,443,080\n",
      "  - Unique Penn records: 1,596,684\n",
      "  - Uniqueness rate: 65.4%\n",
      "  - Print materials: 1,786,470\n",
      "  - Print materials percentage: 1.1%\n",
      "  - Sample size: 1,035\n",
      "\n",
      "ðŸ“š Material Category Breakdown:\n",
      "  - audio_material: 1,667 (0.1%)\n",
      "  - electronic_resource: 2,627 (0.1%)\n",
      "  - other: 121,437 (6.8%)\n",
      "  - print_book: 1,713,978 (95.9%)\n",
      "  - print_maps: 2,109 (0.1%)\n",
      "  - print_music: 19,096 (1.1%)\n",
      "  - print_serial: 51,287 (2.9%)\n",
      "  - visual_material: 99,359 (5.6%)\n"
     ]
    }
   ],
   "source": [
    "# Stratified Sampling and Final Analysis\n",
    "from pyspark.sql.functions import rand, col\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "# Define output directory if not already defined\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load print materials dataset if not already loaded\n",
    "if 'print_only_df' not in locals() or print_only_df is None:\n",
    "    print(\"Loading print materials dataset...\")\n",
    "    print_only_df_raw = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "    \n",
    "    # Check if metadata columns exist and drop them for sampling\n",
    "    metadata_cols = [\"processing_date\", \"source_file\", \"data_currency_warning\"]\n",
    "    existing_metadata_cols = [col for col in metadata_cols if col in print_only_df_raw.columns]\n",
    "    \n",
    "    if existing_metadata_cols:\n",
    "        print(f\"Dropping metadata columns: {existing_metadata_cols}\")\n",
    "        print_only_df = print_only_df_raw.drop(*existing_metadata_cols)\n",
    "    else:\n",
    "        print_only_df = print_only_df_raw\n",
    "else:\n",
    "    print(\"Using existing print_only_df DataFrame\")\n",
    "\n",
    "# Load or compute necessary statistics if not available\n",
    "if 'total_penn' not in locals() or 'unique_penn_count' not in locals():\n",
    "    print(\"Loading required statistics...\")\n",
    "    # Load from saved parquet files\n",
    "    if 'unique_penn' not in locals():\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "    unique_penn_count = unique_penn.count()\n",
    "    \n",
    "    # Load Penn overlap analysis to get total Penn records\n",
    "    penn_overlap = spark.read.parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "    total_penn = penn_overlap.select(\"F001\").distinct().count()\n",
    "\n",
    "# Compute print statistics if not available\n",
    "if 'print_count' not in locals() or 'material_counts_dict' not in locals():\n",
    "    print(\"Computing material type statistics...\")\n",
    "    # Check for material_category column\n",
    "    if 'material_category' not in print_only_df.columns:\n",
    "        print(\"ERROR: material_category column not found in print_only_df\")\n",
    "        raise ValueError(\"Missing required column: material_category\")\n",
    "    \n",
    "    material_stats = print_only_df.groupBy(\"material_category\").count().collect()\n",
    "    material_counts_dict = {row[\"material_category\"]: row[\"count\"] for row in material_stats}\n",
    "    print_count = sum(material_counts_dict.values())\n",
    "\n",
    "# Define sampling function with improved stratification\n",
    "def create_stratified_sample(df, strata_column, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with improved randomization.\n",
    "    Uses multiple passes to ensure representation of all strata.\n",
    "    \"\"\"\n",
    "    print(f\"Creating stratified sample based on {strata_column}...\")\n",
    "    \n",
    "    # Verify the strata column exists\n",
    "    if strata_column not in df.columns:\n",
    "        print(f\"ERROR: Column '{strata_column}' not found in DataFrame\")\n",
    "        print(f\"Available columns: {df.columns}\")\n",
    "        raise ValueError(f\"Missing required column: {strata_column}\")\n",
    "    \n",
    "    # Get counts by strata for weighting\n",
    "    strata_counts = df.groupBy(strata_column).count().collect()\n",
    "    total_records = df.count()\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"WARNING: No records to sample from!\")\n",
    "        return df\n",
    "    \n",
    "    strata_map = {row[strata_column]: row[\"count\"] for row in strata_counts}\n",
    "    print(f\"Strata distribution:\")\n",
    "    for strata, count in sorted(strata_map.items()):\n",
    "        print(f\"  - {strata}: {count:,} records ({count/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate proportional sample sizes with minimum threshold\n",
    "    min_per_strata = 5  # Ensure at least a few records from each stratum\n",
    "    sample_fractions = {}\n",
    "    \n",
    "    for strata, count in strata_map.items():\n",
    "        # Proportional sampling with minimum threshold\n",
    "        if count > 0:\n",
    "            # Calculate proportional share but ensure at least min_per_strata\n",
    "            prop_size = max(\n",
    "                min_per_strata,\n",
    "                int((count / total_records) * sample_size)\n",
    "            )\n",
    "            \n",
    "            # Don't sample more than we have\n",
    "            prop_size = min(prop_size, count)\n",
    "            \n",
    "            # Calculate fraction\n",
    "            sample_fractions[strata] = prop_size / count\n",
    "    \n",
    "    # First pass: Stratified sampling\n",
    "    sampled_df = df.sampleBy(strata_column, fractions=sample_fractions, seed=42)\n",
    "    \n",
    "    # Check if we need a second pass to reach target size\n",
    "    current_size = sampled_df.count()\n",
    "    print(f\"First pass sample size: {current_size}\")\n",
    "    \n",
    "    if current_size < sample_size and current_size < total_records:\n",
    "        # Second pass: Sample from under-represented strata\n",
    "        remaining = min(sample_size - current_size, total_records - current_size)\n",
    "        print(f\"Need {remaining} more records to reach target sample size\")\n",
    "        \n",
    "        # Get records not in first sample\n",
    "        sampled_ids = sampled_df.select(\"F001\").distinct()\n",
    "        remaining_df = df.join(sampled_ids, on=\"F001\", how=\"left_anti\")\n",
    "        \n",
    "        remaining_count = remaining_df.count()\n",
    "        if remaining_count > 0:\n",
    "            # Simple random sample from remaining records\n",
    "            additional_sample = remaining_df.orderBy(rand(seed=43)).limit(remaining)\n",
    "            \n",
    "            # Union the samples\n",
    "            sampled_df = sampled_df.union(additional_sample)\n",
    "            print(f\"Added {min(remaining, remaining_count)} additional records\")\n",
    "    \n",
    "    final_size = sampled_df.count()\n",
    "    print(f\"Final sample size: {final_size}\")\n",
    "    \n",
    "    # Check distribution in final sample\n",
    "    sample_distribution = sampled_df.groupBy(strata_column).count().collect()\n",
    "    print(f\"\\nSample distribution by {strata_column}:\")\n",
    "    sample_dict = {row[strata_column]: row[\"count\"] for row in sample_distribution}\n",
    "    \n",
    "    for strata_val in sorted(strata_map.keys()):\n",
    "        original_count = strata_map.get(strata_val, 0)\n",
    "        sample_count = sample_dict.get(strata_val, 0)\n",
    "        if original_count > 0 and final_size > 0:\n",
    "            print(f\"  - {strata_val}: {sample_count} ({sample_count/final_size*100:.2f}% of sample vs {original_count/total_records*100:.2f}% of population)\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Create a stratified sample by material category\n",
    "sample_df = create_stratified_sample(print_only_df, \"material_category\", sample_size=1000)\n",
    "\n",
    "# Cache the sample for better performance\n",
    "sample_df.cache()\n",
    "\n",
    "# Save the sample for API validation\n",
    "sample_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/statistical_sample_for_api_no_hsp.parquet\")\n",
    "\n",
    "# Select key fields for the CSV, handling array fields\n",
    "sample_for_csv = sample_df.select(\n",
    "    \"F001\", \n",
    "    # F020 is an array - get first ISBN if available\n",
    "    F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0), \n",
    "           F.col(\"F020\").getItem(0)).otherwise(\"\").alias(\"F020\"),\n",
    "    \"F010\",  # This is already a string\n",
    "    \"F245\",  # This is already a string\n",
    "    # F250 is an array - get first edition statement if available\n",
    "    F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0), \n",
    "           F.col(\"F250\").getItem(0)).otherwise(\"\").alias(\"F250\"),\n",
    "    # F260 is an array - get first publication info if available\n",
    "    F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0), \n",
    "           F.col(\"F260\").getItem(0)).otherwise(\"\").alias(\"F260\"),\n",
    "    \"material_category\"\n",
    ")\n",
    "\n",
    "# Save as CSV (single file for easier review)\n",
    "sample_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/statistical_sample_for_api_no_hsp.csv\")\n",
    "# Save as CSV (single file for easier review)\n",
    "sample_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/statistical_sample_for_api_no_hsp.csv\")\n",
    "\n",
    "# Generate final summary statistics in JSON format\n",
    "summary_stats = {\n",
    "    \"processing_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_penn_records\": int(total_penn),\n",
    "    \"unique_penn_records\": int(unique_penn_count),\n",
    "    \"uniqueness_rate\": float(unique_penn_count/total_penn) if total_penn > 0 else 0.0,\n",
    "    \"print_materials\": int(print_count),\n",
    "    \"print_materials_percentage\": float(print_count/unique_penn_count) if unique_penn_count > 0 else 0.0,\n",
    "    \"sample_size\": int(sample_df.count()),\n",
    "    \"material_categories\": {}\n",
    "}\n",
    "\n",
    "# Add material categories to summary\n",
    "for category, count in sorted(material_counts_dict.items()):\n",
    "    summary_stats[\"material_categories\"][category] = {\n",
    "        \"count\": int(count),\n",
    "        \"percentage\": float(count/print_count*100) if print_count > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# Write summary to JSON file\n",
    "with open(f\"{output_dir}/sample_summary_no_hsp.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "# Unpersist the cached sample\n",
    "sample_df.unpersist()\n",
    "\n",
    "print(\"\\nâœ… Processing complete!\")\n",
    "print(f\"Results saved to {output_dir}/\")\n",
    "print(\"\\nFinal outputs:\")\n",
    "print(f\"  - unique_penn.parquet: All unique Penn records\")\n",
    "print(f\"  - physical_books_no_533.parquet: Unique Penn physical books\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\")\n",
    "print(f\"  - sample_summary_no_hsp.json: Summary statistics\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(f\"  - Total Penn records: {summary_stats['total_penn_records']:,}\")\n",
    "print(f\"  - Unique Penn records: {summary_stats['unique_penn_records']:,}\")\n",
    "print(f\"  - Uniqueness rate: {summary_stats['uniqueness_rate']*100:.1f}%\")\n",
    "print(f\"  - Print materials: {summary_stats['print_materials']:,}\")\n",
    "print(f\"  - Print materials percentage: {summary_stats['print_materials_percentage']:.1f}%\")\n",
    "print(f\"  - Sample size: {summary_stats['sample_size']:,}\")\n",
    "\n",
    "# Display material category breakdown\n",
    "if material_counts_dict:\n",
    "    print(\"\\nðŸ“š Material Category Breakdown:\")\n",
    "    for category, info in sorted(summary_stats[\"material_categories\"].items()):\n",
    "        print(f\"  - {category}: {info['count']:,} ({info['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All Spark resources cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Cleanup Cell - Run this to free all resources\n",
    "def cleanup_spark_resources():\n",
    "    \"\"\"Clean up all cached DataFrames and temporary views\"\"\"\n",
    "    try:\n",
    "        # Get all cached DataFrames\n",
    "        for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "            rdd.unpersist()\n",
    "        \n",
    "        # Drop all temporary views\n",
    "        for view in spark.catalog.listTables():\n",
    "            if view.isTemporary:\n",
    "                spark.catalog.dropTempView(view.name)\n",
    "        \n",
    "        print(\"âœ… All Spark resources cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Cleanup warning: {e}\")\n",
    "\n",
    "cleanup_spark_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
