{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Institution MARC Data Collection Setup\n",
    "\n",
    "## Overview\n",
    "This notebook collects MARC data from all Ivy Plus libraries. The Ivy Plus consortium includes:\n",
    "\n",
    "- Brown University\n",
    "- University of Chicago  \n",
    "- Columbia University\n",
    "- Cornell University\n",
    "- Dartmouth College\n",
    "- Duke University\n",
    "- Harvard University\n",
    "- Johns Hopkins University\n",
    "- Massachusetts Institute of Technology\n",
    "- University of Pennsylvania\n",
    "- Princeton University\n",
    "- Stanford University\n",
    "- Yale University\n",
    "\n",
    "## Data Source\n",
    "POD\n",
    "```\n",
    "https://pod.stanford.edu/organizations/{org_code}/streams/{stream_date}/normalized_resourcelist/marc21\n",
    "```\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "```python\n",
    "# Sync data for a single institution\n",
    "sync_institution_data('harvard')  # Uses stream date from config\n",
    "\n",
    "# Sync data for all active institutions (incremental sync)\n",
    "sync_all_active_institutions()\n",
    "\n",
    "# Force re-download all files for an institution\n",
    "sync_institution_data('yale', force_refresh=True)\n",
    "\n",
    "# Use a different stream date\n",
    "sync_institution_data('yale', stream_date='2023-01-01')\n",
    "\n",
    "# Check current data status\n",
    "for institution_key in ivy_plus_config:\n",
    "    if not ivy_plus_config[institution_key]['active']:\n",
    "        continue\n",
    "    pod_dir = os.path.join(VOLUME_BASE, f\"pod_{institution_key}\")\n",
    "    if os.path.exists(pod_dir):\n",
    "        marc_files = glob.glob(os.path.join(pod_dir, \"**/*.mrc.gz\"), recursive=True)\n",
    "        print(f\"{institution_key}: {len(marc_files)} MARC files\")\n",
    "```\n",
    "\n",
    "## Data Organization\n",
    "- **Raw Downloads**: Each institution's data is stored in `pod_{institution}` directories\n",
    "- **Work Directory**: Files are copied to `pod-processing-outputs/work/marc/` with institution prefix\n",
    "- **Processed Files**: Institution-specific MARC files in `pod-processing-outputs/processed/`\n",
    "- **Export Directory**: Final filtered files in timestamped subdirectories\n",
    "\n",
    "## Processing Workflow\n",
    "1. **Sync Data**: Download/update MARC and delete files from POD\n",
    "2. **Organize Files**: Copy MARC files to work directory with institution prefixes\n",
    "3. **Process Records**: Extract and consolidate MARC records by institution\n",
    "4. **Export with Filtering**: Apply delete filtering and create final export package\n",
    "\n",
    "## Key Features\n",
    "- **Incremental Sync**: Only downloads new/updated files (unless `force_refresh=True`)\n",
    "- **Delete Filtering**: Automatically processes `.del.txt` files to filter out deleted records\n",
    "- **Progress Tracking**: Saves progress to allow resuming interrupted processing\n",
    "- **Institution Prefixing**: Prevents filename conflicts between institutions\n",
    "\n",
    "## Notes\n",
    "- Harvard's collection is particularly large (~43GB, 962 files)\n",
    "- Processing uses PySpark for efficient handling of large datasets\n",
    "- A single POD access token provides authentication for all institutions\n",
    "- Stream dates vary by institution (configured in `ivy_plus_config`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install resync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Configuration for paths - adjust this when moving between systems\n",
    "VOLUME_BASE = '/Volumes/Samsung_T5/pod-pyspark-notbook'\n",
    "os.chdir(VOLUME_BASE)  # Set working directory to the new volume\n",
    "\n",
    "# Define all paths relative to the new volume\n",
    "base_dir = os.path.join(VOLUME_BASE, 'pod-processing-outputs')\n",
    "work_dir = os.path.join(base_dir, 'work', 'marc')\n",
    "processed_dir = os.path.join(base_dir, 'processed')\n",
    "final_dir = os.path.join(base_dir, 'final')\n",
    "export_dir = os.path.join(base_dir, 'export')\n",
    "archive_dir = os.path.join(base_dir, 'archive')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [base_dir, work_dir, processed_dir, final_dir, export_dir, archive_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Volume base: {VOLUME_BASE}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(\"All directories created/verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this verification cell after your path setup cell (after cell id=\"d2a40ed6\")\n",
    "# This ensures we're starting with a clean slate\n",
    "\n",
    "print(\"=== VERIFYING CLEAN STATE ===\")\n",
    "\n",
    "# Check processed directory is empty\n",
    "processed_files = os.listdir(processed_dir) if os.path.exists(processed_dir) else []\n",
    "if processed_files:\n",
    "    print(f\"⚠️ WARNING: Processed directory is not empty! Contains {len(processed_files)} files\")\n",
    "    print(\"Files found:\", processed_files[:5], \"...\" if len(processed_files) > 5 else \"\")\n",
    "    print(\"\\nTo ensure clean processing, run:\")\n",
    "    print(f\"  shutil.rmtree('{processed_dir}')\")\n",
    "    print(f\"  os.makedirs('{processed_dir}')\")\n",
    "else:\n",
    "    print(\"✓ Processed directory is clean\")\n",
    "\n",
    "# Check for any old export directories\n",
    "if os.path.exists(export_dir):\n",
    "    export_subdirs = [d for d in os.listdir(export_dir) if os.path.isdir(os.path.join(export_dir, d))]\n",
    "    if export_subdirs:\n",
    "        print(f\"\\n✓ Found {len(export_subdirs)} previous export(s)\")\n",
    "        print(\"  These won't interfere with new processing\")\n",
    "else:\n",
    "    print(\"\\n✓ No previous exports found\")\n",
    "\n",
    "print(\"\\n✓ Ready for fresh processing run!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "print(\"=== VERIFYING SETUP ===\")\n",
    "print(f\"Volume base: {VOLUME_BASE}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check directories\n",
    "for name, path in [\n",
    "    ('Work dir', work_dir),\n",
    "    ('Processed dir', processed_dir),\n",
    "    ('Final dir', final_dir)\n",
    "]:\n",
    "    exists = \"✓\" if os.path.exists(path) else \"✗\"\n",
    "    print(f\"{exists} {name}: {path}\")\n",
    "\n",
    "# Check for progress file\n",
    "progress_file = os.path.join(processed_dir, 'processing_progress.json')\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress = json.load(f)\n",
    "    print(f\"\\n✓ Progress file found:\")\n",
    "    print(f\"  - Files processed: {len(progress.get('processed_files', []))}\")\n",
    "    print(f\"  - Total records: {progress.get('total_records', 0):,}\")\n",
    "else:\n",
    "    print(f\"\\n✗ No progress file - will start fresh\")\n",
    "\n",
    "# Check source files\n",
    "marc_files = glob.glob(os.path.join(work_dir, '*.mrc.gz'))\n",
    "print(f\"\\nSource files: {len(marc_files)} .mrc.gz files in work directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Ivy Plus Libraries\n",
    "# This dictionary contains the organization codes and basic info for each institution\n",
    "# The access token appears to be system-wide for POD, not institution-specific\n",
    "\n",
    "# Common POD access token (appears to work for all institutions)\n",
    "POD_ACCESS_TOKEN = 'YOUR_POD_ACCESS_TOKEN_HERE'  # Replace with your actual token\n",
    "\n",
    "ivy_plus_config = {\n",
    "    'brown': {\n",
    "        'name': 'Brown University',\n",
    "        'org_code': 'brown',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': 'brown_2022-05-05',\n",
    "    },\n",
    "    'chicago': {\n",
    "        'name': 'University of Chicago',\n",
    "        'org_code': 'chicago',\n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '2022-06-18',\n",
    "    },\n",
    "    'columbia': {\n",
    "        'name': 'Columbia University',\n",
    "        'org_code': 'columbia',\n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': 'prod',\n",
    "    },\n",
    "    'cornell': {\n",
    "        'name': 'Cornell University',\n",
    "        'org_code': 'cornell',\n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '2023-04',\n",
    "    },\n",
    "    'dartmouth': {\n",
    "        'name': 'Dartmouth College',\n",
    "        'org_code': 'dartmouth',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '202208',\n",
    "    },\n",
    "    'duke': {\n",
    "        'name': 'Duke University',\n",
    "        'org_code': 'duke',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '2025-01-15',\n",
    "    },\n",
    "    'harvard': {\n",
    "        'name': 'Harvard University',\n",
    "        'org_code': 'harvard',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '20220204-with-items',\n",
    "    },\n",
    "    'johns_hopkins': {\n",
    "        'name': 'Johns Hopkins University',\n",
    "        'org_code': 'jhu',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': 'jhu',\n",
    "    },\n",
    "    'mit': {\n",
    "        'name': 'Massachusetts Institute of Technology',\n",
    "        'org_code': 'mit',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': 'prod-2022-06-08',\n",
    "    },\n",
    "    'penn': {\n",
    "        'name': 'University of Pennsylvania',\n",
    "        'org_code': 'penn',\n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,  # Currently working and verified\n",
    "        'stream_date': '2022-07-18',\n",
    "    },\n",
    "    'princeton': {\n",
    "        'name': 'Princeton University',\n",
    "        'org_code': 'princeton',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': 'princeton-prod-0223',\n",
    "    },\n",
    "    'stanford': {\n",
    "        'name': 'Stanford University',\n",
    "        'org_code': 'stanford',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '2024-08-27',\n",
    "    },\n",
    "    'yale': {\n",
    "        'name': 'Yale University',\n",
    "        'org_code': 'yale',  \n",
    "        'access_token': POD_ACCESS_TOKEN,\n",
    "        'active': True,\n",
    "        'stream_date': '2025-07-alma',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Base URLs for POD system\n",
    "POD_BASE_URL = 'https://pod.stanford.edu/'\n",
    "SITEMAP_BASE_URL = 'https://pod.stanford.edu/organizations/{org}/streams/{stream}/normalized_resourcelist/marc21'\n",
    "\n",
    "# Default stream (you may need to adjust this for each institution)\n",
    "DEFAULT_STREAM = '2022-07-18'\n",
    "\n",
    "print(\"Ivy Plus Libraries Configuration (with shared POD token):\")\n",
    "for key, config in ivy_plus_config.items():\n",
    "    status = \"✓ Active\" if config['active'] else \"✗ Inactive (need to verify org_code)\"\n",
    "    print(f\"  {config['name']}: {status}\")\n",
    "\n",
    "print(f\"\\nNOTE: All institutions now use the same POD access token.\")\n",
    "print(f\"To activate an institution, verify its org_code and set active=True\")\n",
    "print(f\"Use test_all_organization_codes() to verify org_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell after configuration (id=\"eca2352d\") and before organize files (id=\"05b4fe0f\")\n",
    "\n",
    "# Import shutil for the organize function later\n",
    "import shutil\n",
    "\n",
    "def sync_institution_data(institution_key, stream_date=None, output_base_dir=None, force_refresh=False):\n",
    "    \"\"\"\n",
    "    Sync MARC data for a specific institution using resync-sync.\n",
    "    By default, only downloads missing or updated files.\n",
    "    \n",
    "    Args:\n",
    "        institution_key: Institution identifier from ivy_plus_config\n",
    "        stream_date: Override stream date (optional)\n",
    "        output_base_dir: Base directory for output (optional)\n",
    "        force_refresh: If True, forces re-download of all files\n",
    "    \"\"\"\n",
    "    if institution_key not in ivy_plus_config:\n",
    "        print(f\"Error: Institution '{institution_key}' not found in configuration\")\n",
    "        return False\n",
    "\n",
    "    config = ivy_plus_config[institution_key]\n",
    "\n",
    "    if not config['active']:\n",
    "        print(f\"Warning: Institution '{config['name']}' is not active\")\n",
    "        return False\n",
    "\n",
    "    # Use institution-specific stream_date if present, else argument, else default\n",
    "    effective_stream_date = stream_date or config.get('stream_date') or DEFAULT_STREAM\n",
    "\n",
    "    # Set up output directory\n",
    "    if output_base_dir is None:\n",
    "        output_base_dir = VOLUME_BASE\n",
    "\n",
    "    output_dir = os.path.join(output_base_dir, f\"pod_{institution_key}\")\n",
    "\n",
    "    # Build sitemap URL\n",
    "    sitemap_url = SITEMAP_BASE_URL.format(\n",
    "        org=config['org_code'],\n",
    "        stream=effective_stream_date\n",
    "    )\n",
    "\n",
    "    # Check existing data before sync\n",
    "    existing_marc = 0\n",
    "    existing_del = 0\n",
    "    if os.path.exists(output_dir):\n",
    "        existing_marc = len(glob.glob(os.path.join(output_dir, \"**/*.mrc.gz\"), recursive=True))\n",
    "        existing_del = len(glob.glob(os.path.join(output_dir, \"**/*.del.txt\"), recursive=True))\n",
    "        if existing_marc > 0 or existing_del > 0:\n",
    "            print(f\"Found existing data: {existing_marc} MARC files, {existing_del} delete files\")\n",
    "\n",
    "    # Build command - resync-sync will handle incremental updates\n",
    "    command = [\n",
    "        'resync-sync',\n",
    "        '-v',\n",
    "        '--sitemap', sitemap_url,\n",
    "        '--access-token', config['access_token'],\n",
    "        '-b', POD_BASE_URL,\n",
    "    ]\n",
    "    \n",
    "    # Add force refresh flag if requested\n",
    "    if force_refresh:\n",
    "        command.append('--force')\n",
    "        print(\"Force refresh enabled - will re-download all files\")\n",
    "    \n",
    "    command.append(output_dir)\n",
    "\n",
    "    print(f\"Syncing data for {config['name']}...\")\n",
    "    print(f\"Stream: {effective_stream_date}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    if not force_refresh:\n",
    "        print(\"Mode: Incremental (only missing/updated files)\")\n",
    "\n",
    "    try:\n",
    "        # Run resync-sync\n",
    "        result = subprocess.run(command)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Check what was downloaded\n",
    "            new_marc = len(glob.glob(os.path.join(output_dir, \"**/*.mrc.gz\"), recursive=True))\n",
    "            new_del = len(glob.glob(os.path.join(output_dir, \"**/*.del.txt\"), recursive=True))\n",
    "            \n",
    "            print(f\"✓ Successfully synced {config['name']}\")\n",
    "            print(f\"  Total files: {new_marc} MARC, {new_del} delete files\")\n",
    "            \n",
    "            if existing_marc > 0 or existing_del > 0:\n",
    "                downloaded_marc = new_marc - existing_marc\n",
    "                downloaded_del = new_del - existing_del\n",
    "                if downloaded_marc > 0 or downloaded_del > 0:\n",
    "                    print(f\"  New files: {downloaded_marc} MARC, {downloaded_del} delete files\")\n",
    "                else:\n",
    "                    print(f\"  No new files needed - already up to date\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Error syncing data for {config['name']}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Exception occurred while syncing {config['name']}: {e}\")\n",
    "        return False\n",
    "\n",
    "def sync_all_active_institutions(force_refresh=False):\n",
    "    \"\"\"\n",
    "    Sync data for all active institutions, leveraging rsync's incremental capabilities\n",
    "    \"\"\"\n",
    "    print(\"=== SYNCING ALL ACTIVE INSTITUTIONS ===\")\n",
    "    print(\"Using rsync incremental mode - only missing/updated files will be downloaded\\n\")\n",
    "    \n",
    "    successful = []\n",
    "    failed = []\n",
    "    already_complete = []\n",
    "    \n",
    "    for institution_key, config in ivy_plus_config.items():\n",
    "        if config['active']:\n",
    "            print(f\"\\n--- {config['name']} ---\")\n",
    "            \n",
    "            # Quick check if data already exists\n",
    "            output_dir = os.path.join(VOLUME_BASE, f\"pod_{institution_key}\")\n",
    "            if os.path.exists(output_dir) and not force_refresh:\n",
    "                marc_count = len(glob.glob(os.path.join(output_dir, \"**/*.mrc.gz\"), recursive=True))\n",
    "                if marc_count > 0:\n",
    "                    print(f\"Checking for updates ({marc_count} existing files)...\")\n",
    "            \n",
    "            if sync_institution_data(institution_key, force_refresh=force_refresh):\n",
    "                successful.append(institution_key)\n",
    "            else:\n",
    "                failed.append(institution_key)\n",
    "        else:\n",
    "            print(f\"\\nSkipping {config['name']} (inactive)\")\n",
    "    \n",
    "    print(\"\\n=== SYNC SUMMARY ===\")\n",
    "    print(f\"Successfully synced: {len(successful)} institutions\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nFailed: {len(failed)} institutions\")\n",
    "        for inst in failed:\n",
    "            print(f\"  ✗ {ivy_plus_config[inst]['name']}\")\n",
    "    \n",
    "    return successful\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "\n",
    "# First, show current data status\n",
    "print(\"=== CURRENT DATA STATUS ===\")\n",
    "total_marc_files = 0\n",
    "total_del_files = 0\n",
    "institutions_with_data = []\n",
    "\n",
    "for institution_key in ivy_plus_config:\n",
    "    if not ivy_plus_config[institution_key]['active']:\n",
    "        continue\n",
    "        \n",
    "    pod_dir = os.path.join(VOLUME_BASE, f\"pod_{institution_key}\")\n",
    "    if os.path.exists(pod_dir):\n",
    "        marc_files = glob.glob(os.path.join(pod_dir, \"**/*.mrc.gz\"), recursive=True)\n",
    "        del_files = glob.glob(os.path.join(pod_dir, \"**/*.del.txt\"), recursive=True)\n",
    "        if marc_files or del_files:\n",
    "            institutions_with_data.append(institution_key)\n",
    "            total_marc_files += len(marc_files)\n",
    "            total_del_files += len(del_files)\n",
    "            print(f\"✓ {ivy_plus_config[institution_key]['name']}: {len(marc_files)} MARC, {len(del_files)} delete files\")\n",
    "\n",
    "if institutions_with_data:\n",
    "    print(f\"\\nTotal: {len(institutions_with_data)} institutions with data\")\n",
    "    print(f\"Files: {total_marc_files} MARC, {total_del_files} delete files\")\n",
    "    print(\"\\n✓ Ready to run incremental sync to check for updates\")\n",
    "else:\n",
    "    print(\"\\nNo existing data found - will download all files on first sync\")\n",
    "\n",
    "\n",
    "sync_all_active_institutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_institution_files():\n",
    "    \"\"\"\n",
    "    Move/copy relevant files from institution directories to work directory\n",
    "    \"\"\"\n",
    "    print(\"=== ORGANIZING INSTITUTION FILES ===\")\n",
    "    total_copied = 0\n",
    "    \n",
    "    for institution_key, config in ivy_plus_config.items():\n",
    "        if not config['active']:\n",
    "            continue\n",
    "            \n",
    "        # Use absolute path with new volume\n",
    "        source_dir = os.path.join(VOLUME_BASE, f\"pod_{institution_key}\")\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"Directory not found: {source_dir}\")\n",
    "            continue\n",
    "            \n",
    "        # Find all MARC files (adjust pattern as needed)\n",
    "        marc_files = glob.glob(f\"{source_dir}/**/*.mrc.gz\", recursive=True)\n",
    "        \n",
    "        # Copy to work directory with institution prefix\n",
    "        copied_count = 0\n",
    "        for file in marc_files:\n",
    "            filename = os.path.basename(file)\n",
    "            dest_file = os.path.join(work_dir, f\"{institution_key}_{filename}\")\n",
    "            shutil.copy2(file, dest_file)\n",
    "            copied_count += 1\n",
    "            \n",
    "        print(f\"  {institution_key}: Copied {copied_count} files\")\n",
    "        total_copied += copied_count\n",
    "    \n",
    "    print(f\"\\nTotal: Copied {total_copied} files to {work_dir}\")\n",
    "    return total_copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyspark to import files in multiple folders\n",
    "# and save them in a single folder\n",
    "# usage: spark-submit --master local[*] import_files.py\n",
    "\n",
    "# Complete Spark setup for MARC processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Initialize Spark if not already done\n",
    "try:\n",
    "    sc\n",
    "    print(\"Using existing SparkContext\")\n",
    "    # If SparkContext exists, get SparkSession from it\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "except NameError:\n",
    "    # Create SparkSession with configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"marcUpdateDeleteApp\") \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.7\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .config(\"spark.default.parallelism\", \"2\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Get SparkContext from SparkSession\n",
    "    sc = spark.sparkContext\n",
    "    print(\"Created new SparkSession and SparkContext with memory-optimized settings\")\n",
    "\n",
    "# Now these will work\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark context web UI: {sc.uiWebUrl}\")\n",
    "print(\"Spark setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARC record consolidation and organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "from pymarc import MARCReader, Record, MARCWriter\n",
    "import json\n",
    "\n",
    "# Check if Spark is already initialized\n",
    "try:\n",
    "    sc\n",
    "    print(\"Using existing SparkContext\")\n",
    "    print(f\"SparkContext ID: {sc.applicationId}\")\n",
    "    print(f\"Spark UI: {sc.uiWebUrl}\")\n",
    "except NameError:\n",
    "    print(\"ERROR: SparkContext not found!\")\n",
    "    print(\"Please run the Spark initialization cell first\")\n",
    "    raise\n",
    "\n",
    "# Define directories using the volume base\n",
    "source_dir = os.path.join(base_dir, 'work', 'marc')\n",
    "output_dir = os.path.join(base_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Progress tracking file\n",
    "progress_file = os.path.join(output_dir, 'processing_progress.json')\n",
    "\n",
    "# Load previous progress if exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress = json.load(f)\n",
    "    print(f\"Resuming from previous run. Already processed {len(progress['processed_files'])} files.\")\n",
    "    processed_files = set(progress['processed_files'])\n",
    "    total_records = progress['total_records']\n",
    "    total_updates = progress['total_updates']\n",
    "    errors = progress['errors']\n",
    "else:\n",
    "    print(\"Starting fresh processing run.\")\n",
    "    processed_files = set()\n",
    "    total_records = 0\n",
    "    total_updates = 0\n",
    "    errors = []\n",
    "\n",
    "def save_progress():\n",
    "    \"\"\"Save current progress to file\"\"\"\n",
    "    progress = {\n",
    "        'processed_files': list(processed_files),\n",
    "        'total_records': total_records,\n",
    "        'total_updates': total_updates,\n",
    "        'errors': errors\n",
    "    }\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "def get_all_marc_gz_files():\n",
    "    \"\"\"Get all .mrc.gz files from the source directory\"\"\"\n",
    "    pattern = os.path.join(source_dir, '*.mrc.gz')\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"Found {len(files)} .mrc.gz files in {source_dir}\")\n",
    "    return files\n",
    "\n",
    "def extract_institution_from_filename(filename):\n",
    "    \"\"\"Extract institution code from filename\"\"\"\n",
    "    if '_' in filename:\n",
    "        return filename.split('_')[0]\n",
    "    elif '-' in filename:\n",
    "        return filename.split('-')[0]\n",
    "    return 'unknown'\n",
    "\n",
    "def process_pod_delete_files():\n",
    "    \"\"\"\n",
    "    Process all POD delete files (.del.txt) separately\n",
    "    \"\"\"\n",
    "    print(\"\\n=== PROCESSING POD DELETE FILES ===\")\n",
    "    \n",
    "    delete_patterns = [\n",
    "        \"pod_*/**/*.del.txt\",\n",
    "        os.path.join(work_dir, \"*.del.txt\")\n",
    "    ]\n",
    "    \n",
    "    all_delete_ids = set()\n",
    "    delete_file_count = 0\n",
    "    \n",
    "    for pattern in delete_patterns:\n",
    "        delete_files = glob.glob(pattern, recursive=True)\n",
    "        for del_file in delete_files:\n",
    "            delete_file_count += 1\n",
    "            try:\n",
    "                with open(del_file, 'r') as f:\n",
    "                    file_deletes = [line.strip() for line in f if line.strip()]\n",
    "                    all_delete_ids.update(file_deletes)\n",
    "                    print(f\"  {os.path.basename(del_file)}: {len(file_deletes)} IDs\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {del_file}: {e}\")\n",
    "    \n",
    "    # Save delete IDs to file\n",
    "    if all_delete_ids:\n",
    "        delete_ids_file = os.path.join(output_dir, 'delete_ids.txt')\n",
    "        with open(delete_ids_file, 'w') as f:\n",
    "            for delete_id in sorted(all_delete_ids):\n",
    "                f.write(f\"{delete_id}\\n\")\n",
    "        print(f\"\\nProcessed {delete_file_count} delete files\")\n",
    "        print(f\"Total unique delete IDs: {len(all_delete_ids)}\")\n",
    "        print(f\"Delete IDs saved to: {delete_ids_file}\")\n",
    "    else:\n",
    "        print(\"No POD delete files found\")\n",
    "    \n",
    "    return all_delete_ids\n",
    "\n",
    "# First, process delete files separately\n",
    "delete_ids = process_pod_delete_files()\n",
    "\n",
    "# Get all .mrc.gz files\n",
    "marc_gz_files = get_all_marc_gz_files()\n",
    "\n",
    "# Filter out already processed files\n",
    "files_to_process = [f for f in marc_gz_files if os.path.basename(f) not in processed_files]\n",
    "print(f\"\\nFiles to process: {len(files_to_process)} (skipping {len(processed_files)} already processed)\")\n",
    "\n",
    "# Check if processing is complete\n",
    "if len(files_to_process) == 0 and len(processed_files) > 0:\n",
    "    print(\"\\n✓ All MARC files have been processed!\")\n",
    "    print(f\"Total processed: {len(processed_files)} files\")\n",
    "    print(\"Use the next cells to split and export the data.\")\n",
    "else:\n",
    "    # Open existing institution writers in append mode\n",
    "    institution_writers = {}\n",
    "    \n",
    "    # Process files\n",
    "    print(\"\\n=== PROCESSING MARC UPDATE FILES ===\")\n",
    "    print(\"Note: POD provides all records in MARC files as updates\")\n",
    "    print(\"Delete records are provided separately in .del.txt files\\n\")\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing {i+1}/{len(files_to_process)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the gzipped MARC file\n",
    "            with gzip.open(file_path, 'rb') as gz_file:\n",
    "                marc_data = gz_file.read()\n",
    "            \n",
    "            # Parse MARC records\n",
    "            reader = MARCReader(marc_data, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            \n",
    "            # Extract institution from filename\n",
    "            institution = extract_institution_from_filename(filename)\n",
    "            \n",
    "            # Create/open writer for this institution\n",
    "            if institution not in institution_writers:\n",
    "                output_file = os.path.join(output_dir, f\"{institution}_updates.mrc\")\n",
    "                # Open in append mode if file exists\n",
    "                mode = 'ab' if os.path.exists(output_file) else 'wb'\n",
    "                output_file_handle = open(output_file, mode)\n",
    "                institution_writers[institution] = {\n",
    "                    'file': output_file_handle,\n",
    "                    'writer': MARCWriter(output_file_handle),\n",
    "                    'count': 0,\n",
    "                    'filename': output_file\n",
    "                }\n",
    "            \n",
    "            # Process records - ALL records in POD MARC files are updates\n",
    "            file_records = 0\n",
    "            file_updates = 0\n",
    "            \n",
    "            for record in reader:\n",
    "                if record is None:\n",
    "                    continue\n",
    "                \n",
    "                file_records += 1\n",
    "                file_updates += 1\n",
    "                \n",
    "                # Extract record ID from 001 field for logging\n",
    "                record_id = record['001'].data if '001' in record else None\n",
    "                \n",
    "                try:\n",
    "                    institution_writers[institution]['writer'].write(record)\n",
    "                    institution_writers[institution]['count'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Could not write record {record_id}: {e}\")\n",
    "            \n",
    "            # Update totals\n",
    "            total_records += file_records\n",
    "            total_updates += file_updates\n",
    "            \n",
    "            print(f\"  Processed: {file_records} update records\")\n",
    "            \n",
    "            # Mark file as processed and save progress\n",
    "            processed_files.add(filename)\n",
    "            save_progress()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {filename}: {e}\")\n",
    "            errors.append({'file': filename, 'error': str(e)})\n",
    "            save_progress()\n",
    "    \n",
    "    # Close all writers\n",
    "    print(\"\\n=== CLOSING FILES ===\")\n",
    "    for institution, writer_info in institution_writers.items():\n",
    "        writer_info['writer'].close()\n",
    "        writer_info['file'].close()\n",
    "        print(f\"  {institution}: {writer_info['count']} records written in this session\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total MARC files processed: {len(processed_files)}\")\n",
    "    print(f\"Total update records found: {total_records:,}\")\n",
    "    print(f\"Files with errors: {len(errors)}\")\n",
    "    if delete_ids:\n",
    "        print(f\"Delete IDs from .del.txt files: {len(delete_ids):,}\")\n",
    "    \n",
    "    if len(files_to_process) == 0:\n",
    "        print(\"\\nNo more files to process in this run.\")\n",
    "        print(f\"Total processed so far: {len(processed_files)}/{len(marc_gz_files)}\")\n",
    "        \n",
    "        if len(processed_files) == len(marc_gz_files):\n",
    "            print(\"\\n=== ALL MARC FILES COMPLETELY PROCESSED! ===\")\n",
    "            print(\"Progress file has been kept for safety.\")\n",
    "            print(\"To manually remove progress file and allow fresh processing, run:\")\n",
    "            print(f\"  os.remove('{progress_file}')\")\n",
    "        else:\n",
    "            print(\"\\nProcessing appears incomplete. Check for errors above.\")\n",
    "    else:\n",
    "        print(f\"\\nProcessed {len(processed_files)} files so far.\")\n",
    "        print(f\"Progress saved. You can safely restart if needed.\")\n",
    "    \n",
    "    print(f\"\\nInstitution-specific update records saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARC Processing Validation\n",
    "\n",
    "## Purpose\n",
    "This cell performs a comprehensive validation check of the MARC processing pipeline output. It ensures that the institution consolidation step completed successfully and provides detailed statistics about the processed data.\n",
    "\n",
    "## What It Does\n",
    "\n",
    "### 1. **Directory Verification**\n",
    "- Checks if the processed directory (`pod-processing-outputs/processed`) exists\n",
    "- Verifies that MARC consolidation has been run\n",
    "\n",
    "### 2. **Institution File Analysis**\n",
    "- Locates all `*_updates.mrc` files (one per institution)\n",
    "- Counts MARC records in each file using pymarc\n",
    "- Calculates file sizes\n",
    "- Aggregates statistics by institution\n",
    "\n",
    "### 3. **Auxiliary File Inspection**\n",
    "- Identifies non-MARC files in the processed directory:\n",
    "  - `delete_ids.txt` - Lists record IDs to be deleted\n",
    "  - `processing_progress.json` - Tracks processing state\n",
    "  - Any other generated files\n",
    "\n",
    "### 4. **Source Directory Check**\n",
    "- Verifies the work directory still contains source `.mrc.gz` files\n",
    "- Confirms data integrity throughout the pipeline\n",
    "\n",
    "### 5. **Spark Status Verification**\n",
    "- Checks if SparkContext is still active\n",
    "- Reports Spark application details\n",
    "- Ensures resources are available for next steps\n",
    "\n",
    "## Output Information\n",
    "\n",
    "### Success Indicators (✓)\n",
    "- Processed directory exists with institution files\n",
    "- Source files are still available\n",
    "- Spark is active and ready\n",
    "\n",
    "### Warning Indicators (✗)\n",
    "- Missing directories or files\n",
    "- Processing errors\n",
    "- Spark unavailable\n",
    "\n",
    "## Key Metrics Reported\n",
    "- **Per Institution**: Record count and file size\n",
    "- **Total**: Aggregate records across all institutions\n",
    "- **Delete IDs**: Number of records marked for deletion\n",
    "- **Processing State**: Progress tracking information\n",
    "\n",
    "## Next Steps\n",
    "Based on validation results:\n",
    "- ✅ If successful → Proceed to export with delete filtering\n",
    "- ❌ If failed → Return to processing cell and investigate errors\n",
    "\n",
    "## Example Output\n",
    "✓ Processed directory contains 13 institution update files harvard_updates.mrc: 1,234,567 records (543.21 MB) yale_updates.mrc: 987,654 records (432.10 MB) ... Total records across all files: 15,678,901\n",
    "\n",
    "This validation ensures data integrity before the final export step where delete filtering is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate processed output files \n",
    "import os\n",
    "import glob\n",
    "from pymarc import MARCReader\n",
    "\n",
    "# Check processed directory\n",
    "processed_dir = 'pod-processing-outputs/processed'\n",
    "\n",
    "print(\"=== VALIDATION SUMMARY ===\")\n",
    "print(\"This validation checks the processed institution files\")\n",
    "print(\"(No splitting step - working with full institution files)\\n\")\n",
    "\n",
    "# Check if directories exist\n",
    "if not os.path.exists(processed_dir):\n",
    "    print(f\"✗ Processed directory does not exist: {processed_dir}\")\n",
    "    print(\"  Run the processing cell first!\")\n",
    "else:\n",
    "    # Look for institution update files\n",
    "    update_files = glob.glob(os.path.join(processed_dir, '*_updates.mrc'))\n",
    "    print(f\"✓ Processed directory contains {len(update_files)} institution update files\")\n",
    "    \n",
    "    # Group by institution for better overview\n",
    "    institution_counts = {}\n",
    "    total_records = 0\n",
    "    \n",
    "    for file_path in update_files:\n",
    "        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "        institution = os.path.basename(file_path).replace('_updates.mrc', '')\n",
    "        \n",
    "        # Count records in file\n",
    "        try:\n",
    "            with open(file_path, 'rb') as marc_file:\n",
    "                reader = MARCReader(marc_file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "                record_count = sum(1 for record in reader if record is not None)\n",
    "                total_records += record_count\n",
    "                \n",
    "                institution_counts[institution] = {\n",
    "                    'files': 1, \n",
    "                    'records': record_count,\n",
    "                    'size_mb': file_size\n",
    "                }\n",
    "                \n",
    "                print(f\"  {os.path.basename(file_path)}: {record_count:,} records ({file_size:.2f} MB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {os.path.basename(file_path)}: Error reading file - {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal records across all files: {total_records:,}\")\n",
    "    print(\"\\nBy institution:\")\n",
    "    for inst, counts in sorted(institution_counts.items()):\n",
    "        print(f\"  {inst}: {counts['records']:,} records ({counts['size_mb']:.2f} MB)\")\n",
    "\n",
    "# Check for other important files\n",
    "print(f\"\\n=== OTHER FILES IN PROCESSED DIRECTORY ===\")\n",
    "if os.path.exists(processed_dir):\n",
    "    all_files = os.listdir(processed_dir)\n",
    "    other_files = [f for f in all_files if not f.endswith('_updates.mrc')]\n",
    "    \n",
    "    for file in sorted(other_files):\n",
    "        file_path = os.path.join(processed_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            if file.endswith('.txt'):\n",
    "                # Count lines in text files (like delete_ids.txt)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    line_count = sum(1 for line in f if line.strip())\n",
    "                print(f\"  {file} ({file_size:.2f} MB, {line_count:,} lines)\")\n",
    "            elif file.endswith('.json'):\n",
    "                print(f\"  {file} ({file_size:.2f} MB) - Progress tracking\")\n",
    "            else:\n",
    "                print(f\"  {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "# Check source directory\n",
    "print(f\"\\n=== SOURCE DIRECTORY ===\")\n",
    "source_dir = 'pod-processing-outputs/work/marc'\n",
    "if os.path.exists(source_dir):\n",
    "    source_files = glob.glob(os.path.join(source_dir, '*.mrc.gz'))\n",
    "    print(f\"✓ Source directory contains {len(source_files)} .mrc.gz files\")\n",
    "else:\n",
    "    print(f\"✗ Source directory does not exist: {source_dir}\")\n",
    "\n",
    "print(f\"\\n=== SPARK STATUS ===\")\n",
    "try:\n",
    "    executor_info = sc.statusTracker().getExecutorInfos()\n",
    "    print(f\"✓ SparkContext is active\")\n",
    "    print(f\"  Application ID: {sc.applicationId}\")\n",
    "    print(f\"  Executors: {len(executor_info)}\")\n",
    "except:\n",
    "    print(\"✗ SparkContext not available\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "if update_files:\n",
    "    print(\"✓ Data is ready for export with delete filtering\")\n",
    "    print(\"  Run the export cell to create filtered output\")\n",
    "    print(\"  Delete filtering will be applied during export\")\n",
    "else:\n",
    "    print(\"✗ No processed data ready for export\")\n",
    "    print(\"  Make sure to run the processing cell first\")\n",
    "\n",
    "print(f\"\\nValidation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export with delete filtering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export with delete filtering \n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pymarc import MARCReader, MARCWriter\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Define base directory and subdirectories\n",
    "base_dir = 'pod-processing-outputs'\n",
    "processed_dir = os.path.join(base_dir, 'processed')\n",
    "export_dir = os.path.join(base_dir, 'export')\n",
    "archive_dir = os.path.join(base_dir, 'archive')\n",
    "work_dir = os.path.join(base_dir, 'work', 'marc')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [export_dir, processed_dir, archive_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def find_and_process_pod_delete_files():\n",
    "    \"\"\"\n",
    "    Find and process all POD delete files (.del.txt) to ensure we have all deletes\n",
    "    \"\"\"\n",
    "    print(\"=== SEARCHING FOR POD DELETE FILES (.del.txt) ===\")\n",
    "    \n",
    "    delete_patterns = [\n",
    "        \"pod_*/**/*.del.txt\",\n",
    "        \"pod-processing-outputs/work/**/*.del.txt\"\n",
    "    ]\n",
    "    \n",
    "    all_pod_delete_ids = set()\n",
    "    delete_file_count = 0\n",
    "    \n",
    "    for pattern in delete_patterns:\n",
    "        delete_files = glob.glob(pattern, recursive=True)\n",
    "        for del_file in delete_files:\n",
    "            delete_file_count += 1\n",
    "            try:\n",
    "                with open(del_file, 'r') as f:\n",
    "                    file_deletes = [line.strip() for line in f if line.strip()]\n",
    "                    all_pod_delete_ids.update(file_deletes)\n",
    "                    print(f\"  {os.path.basename(del_file)}: {len(file_deletes)} IDs\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {del_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nFound {delete_file_count} POD delete files\")\n",
    "    print(f\"Total unique delete IDs from POD files: {len(all_pod_delete_ids)}\")\n",
    "    \n",
    "    return all_pod_delete_ids\n",
    "\n",
    "def reconcile_delete_ids():\n",
    "    \"\"\"\n",
    "    Reconcile delete IDs from different sources\n",
    "    \"\"\"\n",
    "    # Get POD delete IDs\n",
    "    pod_delete_ids = find_and_process_pod_delete_files()\n",
    "    \n",
    "    # Check existing delete_ids.txt\n",
    "    existing_delete_ids = set()\n",
    "    delete_ids_file = os.path.join(processed_dir, 'delete_ids.txt')\n",
    "    \n",
    "    if os.path.exists(delete_ids_file):\n",
    "        with open(delete_ids_file, 'r') as f:\n",
    "            existing_delete_ids = {line.strip() for line in f if line.strip()}\n",
    "        print(f\"\\nExisting delete_ids.txt contains: {len(existing_delete_ids)} IDs\")\n",
    "    \n",
    "    # Combine all delete IDs\n",
    "    all_delete_ids = pod_delete_ids.union(existing_delete_ids)\n",
    "    \n",
    "    print(f\"\\n=== DELETE ID RECONCILIATION ===\")\n",
    "    print(f\"From POD .del.txt files: {len(pod_delete_ids)}\")\n",
    "    print(f\"From existing delete_ids.txt: {len(existing_delete_ids)}\")\n",
    "    print(f\"Total unique delete IDs: {len(all_delete_ids)}\")\n",
    "    \n",
    "    # Update delete_ids.txt with all IDs\n",
    "    if len(all_delete_ids) > len(existing_delete_ids):\n",
    "        print(f\"\\nUpdating delete_ids.txt with {len(all_delete_ids) - len(existing_delete_ids)} additional IDs\")\n",
    "        with open(delete_ids_file, 'w') as f:\n",
    "            for delete_id in sorted(all_delete_ids):\n",
    "                f.write(f\"{delete_id}\\n\")\n",
    "    \n",
    "    return all_delete_ids\n",
    "\n",
    "def create_export_package(filter_deletes=True):\n",
    "    \"\"\"\n",
    "    Create a final export package with all processed files (NO SPLITTING VERSION)\n",
    "    \n",
    "    Args:\n",
    "        filter_deletes: If True, filter out records matching delete IDs\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    export_subdir = os.path.join(export_dir, f\"marc_export_{timestamp}\")\n",
    "    os.makedirs(export_subdir, exist_ok=True)\n",
    "    \n",
    "    # Reconcile and load all delete IDs\n",
    "    delete_ids = set()\n",
    "    if filter_deletes:\n",
    "        delete_ids = reconcile_delete_ids()\n",
    "        if delete_ids:\n",
    "            print(f\"\\nUsing {len(delete_ids)} delete IDs for filtering\")\n",
    "        else:\n",
    "            print(\"No delete IDs found - proceeding without filtering\")\n",
    "            filter_deletes = False\n",
    "    \n",
    "    # Process institution update files from processed directory\n",
    "    total_filtered = 0\n",
    "    update_files = glob.glob(os.path.join(processed_dir, '*_updates.mrc'))\n",
    "    \n",
    "    if update_files:\n",
    "        print(f\"\\n=== PROCESSING {len(update_files)} INSTITUTION FILES ===\")\n",
    "        \n",
    "        for input_file in update_files:\n",
    "            institution = os.path.basename(input_file).replace('_updates.mrc', '')\n",
    "            output_file = os.path.join(export_subdir, f\"{institution}_filtered.mrc\")\n",
    "            \n",
    "            if filter_deletes and delete_ids:\n",
    "                # Filter while copying\n",
    "                print(f\"Processing {institution}...\")\n",
    "                with open(input_file, 'rb') as marc_file:\n",
    "                    reader = MARCReader(marc_file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "                    writer = MARCWriter(open(output_file, 'wb'))\n",
    "                    \n",
    "                    file_records = 0\n",
    "                    file_filtered = 0\n",
    "                    \n",
    "                    for record in reader:\n",
    "                        if record is None:\n",
    "                            continue\n",
    "                        \n",
    "                        file_records += 1\n",
    "                        \n",
    "                        # Check if record should be filtered\n",
    "                        if '001' in record and record['001'].data in delete_ids:\n",
    "                            file_filtered += 1\n",
    "                            total_filtered += 1\n",
    "                        else:\n",
    "                            writer.write(record)\n",
    "                    \n",
    "                    writer.close()\n",
    "                    \n",
    "                    print(f\"  {institution}: {file_records} records, filtered {file_filtered}\")\n",
    "            else:\n",
    "                # Just copy without filtering\n",
    "                shutil.copy2(input_file, export_subdir)\n",
    "                # Rename to indicate it's unfiltered\n",
    "                os.rename(\n",
    "                    os.path.join(export_subdir, os.path.basename(input_file)),\n",
    "                    os.path.join(export_subdir, f\"{institution}_unfiltered.mrc\")\n",
    "                )\n",
    "        \n",
    "        if filter_deletes and total_filtered > 0:\n",
    "            print(f\"\\nTotal records filtered across all files: {total_filtered}\")\n",
    "        print(f\"Processed {len(update_files)} institution files to export package\")\n",
    "    else:\n",
    "        print(\"\\n✗ No institution update files found in processed directory!\")\n",
    "        return None\n",
    "    \n",
    "    # Copy delete IDs file for reference\n",
    "    delete_ids_file = os.path.join(processed_dir, 'delete_ids.txt')\n",
    "    if os.path.exists(delete_ids_file):\n",
    "        shutil.copy2(delete_ids_file, export_subdir)\n",
    "        print(\"Copied delete IDs file to export package for reference\")\n",
    "    \n",
    "    # Create detailed summary file\n",
    "    summary_file = os.path.join(export_subdir, 'processing_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"MARC Processing Summary (No Splitting Version)\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Delete ID Sources:\\n\")\n",
    "        f.write(f\"  - POD .del.txt files processed\\n\")\n",
    "        f.write(f\"  - Total delete IDs: {len(delete_ids)}\\n\\n\")\n",
    "        \n",
    "        if filter_deletes and delete_ids:\n",
    "            f.write(f\"Delete filtering: ENABLED\\n\")\n",
    "            f.write(f\"Delete IDs used: {len(delete_ids)}\\n\")\n",
    "            f.write(f\"Records filtered: {total_filtered}\\n\\n\")\n",
    "        else:\n",
    "            f.write(f\"Delete filtering: DISABLED\\n\\n\")\n",
    "        \n",
    "        # Count records and files by institution\n",
    "        f.write(f\"Institution Breakdown:\\n\")\n",
    "        f.write(f\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        institution_stats = {}\n",
    "        total_records = 0\n",
    "        total_files = 0\n",
    "        \n",
    "        for file in glob.glob(os.path.join(export_subdir, '*.mrc')):\n",
    "            total_files += 1\n",
    "            institution = os.path.basename(file).split('_')[0]\n",
    "            \n",
    "            if institution not in institution_stats:\n",
    "                institution_stats[institution] = {'files': 0, 'records': 0}\n",
    "            \n",
    "            try:\n",
    "                with open(file, 'rb') as marc_file:\n",
    "                    reader = MARCReader(marc_file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "                    record_count = sum(1 for record in reader if record is not None)\n",
    "                    total_records += record_count\n",
    "                    institution_stats[institution]['files'] += 1\n",
    "                    institution_stats[institution]['records'] += record_count\n",
    "                    f.write(f\"{os.path.basename(file)}: {record_count:,} records\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"{os.path.basename(file)}: Error reading file - {e}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n{'-' * 30}\\n\")\n",
    "        f.write(f\"Total: {total_records:,} records in {total_files} files\\n\\n\")\n",
    "        \n",
    "        # Institution summary\n",
    "        f.write(f\"By Institution:\\n\")\n",
    "        for inst, stats in sorted(institution_stats.items()):\n",
    "            f.write(f\"  {inst.upper()}: {stats['records']:,} records in {stats['files']} file(s)\\n\")\n",
    "        \n",
    "        # Delete IDs info\n",
    "        if os.path.exists(os.path.join(export_subdir, 'delete_ids.txt')):\n",
    "            with open(os.path.join(export_subdir, 'delete_ids.txt'), 'r') as del_file:\n",
    "                delete_count = sum(1 for line in del_file if line.strip())\n",
    "            f.write(f\"\\nDelete IDs in reference file: {delete_count:,}\\n\")\n",
    "    \n",
    "    print(f\"\\nExport package created: {export_subdir}\")\n",
    "    return export_subdir\n",
    "\n",
    "# Main execution\n",
    "print(\"=== EXPORT WITH DELETE FILTERING (NO SPLITTING) ===\")\n",
    "\n",
    "# Create export package with delete filtering enabled\n",
    "export_path = create_export_package(filter_deletes=True)\n",
    "\n",
    "if export_path:\n",
    "    # List export contents\n",
    "    export_files = os.listdir(export_path)\n",
    "    print(f\"\\n=== EXPORT PACKAGE CONTENTS ===\")\n",
    "    for file in sorted(export_files):\n",
    "        file_path = os.path.join(export_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            print(f\"  {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Final export package: {export_path}\")\n",
    "    print(f\"Ready for conversion to Parquet format\")\n",
    "\n",
    "    # Display final statistics\n",
    "    summary_file = os.path.join(export_path, 'processing_summary.txt')\n",
    "    if os.path.exists(summary_file):\n",
    "        print(f\"\\nKey Statistics from Processing Summary:\")\n",
    "        with open(summary_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and any(x in line for x in [\"Total:\", \"Delete IDs used:\", \"Records filtered:\", \"By Institution:\"]):\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                    if \"By Institution:\" in line:\n",
    "                        # Print the next few institution lines\n",
    "                        for _ in range(13):  # For all Ivy Plus institutions\n",
    "                            inst_line = next(f, None)\n",
    "                            if inst_line and inst_line.strip():\n",
    "                                print(f\"  {inst_line.strip()}\")\n",
    "\n",
    "# Keep SparkContext active for any additional processing\n",
    "print(f\"\\nSparkContext remains active for additional processing if needed\")\n",
    "print(f\"To stop Spark when completely done, run: sc.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Export Step: Copy Filtered Files to Final Directory\n",
    "\n",
    "## Purpose\n",
    "This cell copies all filtered MARC files from the timestamped export directory to a standardized `final` directory for streamlined processing in the next pipeline stage.\n",
    "\n",
    "## What It Does\n",
    "- Copies all `.mrc` files (filtered by institution) from the export package\n",
    "- Places them in `pod-processing-outputs/final/` directory\n",
    "- Skips non-MARC files (like `.txt` summaries)\n",
    "- Provides a clean, consistent location for `pod-processing.ipynb` to read from\n",
    "\n",
    "## Why This Step\n",
    "- **Consistency**: The `final` directory always contains the latest processed files\n",
    "- **Simplicity**: Next notebook doesn't need to navigate timestamped directories\n",
    "- **Clean Interface**: Only MARC files are copied, excluding metadata files\n",
    "\n",
    "## Output\n",
    "Files are copied as:\n",
    "- `harvard_filtered.mrc`\n",
    "- `yale_filtered.mrc`\n",
    "- `princeton_filtered.mrc`\n",
    "- etc.\n",
    "\n",
    "Ready for conversion to Parquet format in `pod-processing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy filtered files to final directory for streamlined processing\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "if export_path:\n",
    "    print(\"\\n=== COPYING TO FINAL DIRECTORY ===\")\n",
    "    \n",
    "    # Ensure final directory exists\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy all filtered MARC files to final directory\n",
    "    marc_files = glob.glob(os.path.join(export_path, '*.mrc'))\n",
    "    \n",
    "    for marc_file in marc_files:\n",
    "        if not marc_file.endswith('.txt'):  # Skip text files\n",
    "            dest = os.path.join(final_dir, os.path.basename(marc_file))\n",
    "            shutil.copy2(marc_file, dest)\n",
    "            print(f\"  Copied: {os.path.basename(marc_file)}\")\n",
    "    \n",
    "    print(f\"\\n✓ {len(marc_files)} files copied to {final_dir}\")\n",
    "    print(\"Files are now ready for pod-processing.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
