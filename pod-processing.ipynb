{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivy Plus MARC Analysis with Enhanced Matching\n",
    "\n",
    "This notebook processes MARC data from Ivy Plus libraries to identify unique records held by Penn that are not held by other institutions in the consortium.\n",
    "\n",
    "## Enhanced Normalization and Matching\n",
    "\n",
    "The matching process has been improved with specialized normalization for different fields:\n",
    "\n",
    "1. **ISBN/LCCN Matching**: When standard identifiers are available, they are normalized and used as primary match keys\n",
    "   - ISBN-10 and ISBN-13 are properly normalized to ensure consistent matching\n",
    "   - LCCNs are standardized to handle different formats and prefixes\n",
    "\n",
    "2. **Match Key Creation**: For records without standard identifiers, a composite key is created from:\n",
    "   - Normalized title (with improved noise word removal)\n",
    "   - Normalized edition statement\n",
    "   - Normalized publication information with year extraction\n",
    "\n",
    "3. **Match Key Validation**: Each match key is validated for quality to detect potential issues\n",
    "   - Short or generic match keys are flagged\n",
    "   - Match key quality metrics are saved for analysis\n",
    "\n",
    "4. **Field Selection**: \n",
    "   - Leader (FLDR) is now included for record type identification\n",
    "   - Core bibliographic fields (F001, F010, F020, F245, F250, F260) are used\n",
    "\n",
    "This improved approach maintains the principle that different editions, printings, and formats are unique bibliographic entities while enhancing the accuracy of matching across cataloging variations.\n",
    "\n",
    "## Initial load only - Institution-specific Processing\n",
    "Converts MARC to Parquet format for faster processing, maintaining institution-specific separation. This step ensures that each institution's MARC files are converted to separate Parquet files for consistent downstream processing.\n",
    "\n",
    "The conversion includes the leader field (FLDR) for each record while excluding the 007 field to optimize the output files. The leader contains important information about the record structure, material type, and bibliographic level.\n",
    "\n",
    "## HIGH MEMORY REQUIREMENT\n",
    "\n",
    "**This notebook is configured for a high-performance server environment with the following specifications:**\n",
    "\n",
    "- **240GB driver memory allocation** (requires ~300GB total system RAM)\n",
    "- **16 cores** for parallel processing\n",
    "- Optimized for a **Linode 300GB server**\n",
    "\n",
    "**Running this notebook with the current configuration on a standard laptop or desktop will likely cause your kernel to crash or your system to become unresponsive.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for your PySpark server\n",
    "# Update these paths to match your server's directory structure\n",
    "input_dir = \"/home/jovyan/work/July-2025-PODParquet\"  # Where your parquet files are located\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"  # Where to save the results\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {input_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Clean up any existing Spark sessions\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear environment variables that might conflict\n",
    "for key in list(os.environ.keys()):\n",
    "    if 'SPARK' in key or 'JAVA' in key:\n",
    "        del os.environ[key]\n",
    "\n",
    "# Set JAVA_HOME explicitly\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "# Create temp directory\n",
    "os.makedirs('/tmp/spark-temp', exist_ok=True)\n",
    "\n",
    "# Create Spark session with more conservative memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PodProcessing-Stable\") \\\n",
    "    .master(\"local[12]\") \\\n",
    "    .config(\"spark.driver.memory\", \"200g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"20g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xmx210g -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:+UseCompressedOops\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"30m\") \\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"5min\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"20g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session initialized with 200GB memory and optimized settings!\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Test it works\n",
    "print(\"\\nTesting Spark with a simple operation...\")\n",
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pymarc poetry marctable fuzzywuzzy python-Levenshtein langdetect\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the user's local bin directory for macOS\n",
    "user_local_bin = os.path.expanduser('~/.local/bin')\n",
    "\n",
    "# Add the directory to PATH if it exists\n",
    "if os.path.exists(user_local_bin):\n",
    "    os.environ['PATH'] += os.pathsep + user_local_bin\n",
    "    print(f\"Added {user_local_bin} to PATH\")\n",
    "\n",
    "# Also add Python's user site-packages bin directory\n",
    "python_user_bin = os.path.join(sys.prefix, 'bin')\n",
    "if os.path.exists(python_user_bin):\n",
    "    os.environ['PATH'] += os.pathsep + python_user_bin\n",
    "    print(f\"Added {python_user_bin} to PATH\")\n",
    "\n",
    "# For Homebrew Python installations on macOS\n",
    "homebrew_bin = '/usr/local/bin'\n",
    "if os.path.exists(homebrew_bin) and homebrew_bin not in os.environ['PATH']:\n",
    "    os.environ['PATH'] += os.pathsep + homebrew_bin\n",
    "    print(f\"Added {homebrew_bin} to PATH\")\n",
    "\n",
    "# Check if marctable is accessible\n",
    "import shutil\n",
    "if shutil.which('marctable'):\n",
    "    print(\"✅ marctable command found in PATH\")\n",
    "else:\n",
    "    print(\"⚠️  marctable command not found in PATH - checking alternative locations...\")\n",
    "    # Try to find marctable in common locations\n",
    "    possible_locations = [\n",
    "        os.path.expanduser('~/Library/Python/3.11/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.10/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.9/bin'),\n",
    "        '/opt/homebrew/bin',\n",
    "        '/usr/local/bin',\n",
    "    ]\n",
    "    \n",
    "    for loc in possible_locations:\n",
    "        marctable_path = os.path.join(loc, 'marctable')\n",
    "        if os.path.exists(marctable_path):\n",
    "            os.environ['PATH'] += os.pathsep + loc\n",
    "            print(f\"✅ Found marctable in {loc} and added to PATH\")\n",
    "            break\n",
    "\n",
    "print(\"\\n✅ All packages installed and environment configured\")\n",
    "print(f\"Current PATH: {os.environ['PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION: Replace Python UDFs with Spark SQL Functions\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Helper function to handle fields that might be strings or arrays\n",
    "def handle_field_as_string(col_name):\n",
    "    \"\"\"\n",
    "    Safely extract string value whether the field is a string or array.\n",
    "    This version handles mixed types properly.\n",
    "    \"\"\"\n",
    "    # Check if it's an array type - if so, get first element; otherwise just cast to string\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        # Try to get the data type - arrays will have size() function\n",
    "        F.when(\n",
    "            F.size(F.col(col_name)) >= 0,  # This will only work for arrays\n",
    "            F.col(col_name).getItem(0)  # Get first element of array\n",
    "        ).otherwise(\n",
    "            F.col(col_name)  # It's already a string, just use it\n",
    "        )\n",
    "    ).cast(\"string\")\n",
    "\n",
    "def create_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Create match keys using pure Spark SQL functions - MUCH faster than UDFs\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"match_key\", \n",
    "        F.concat_ws(\"_\",\n",
    "            # Normalize title (F245 is string)\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.regexp_replace(\n",
    "                            F.lower(F.trim(F.col(\"F245\"))),\n",
    "                            \"^(the|a|an)\\\\s+\", \"\"\n",
    "                        ),\n",
    "                        \"[^a-z0-9\\\\s]\", \"\"\n",
    "                    ),\n",
    "                    \"\\\\s+\", \" \"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Normalize edition (F250 is array)\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F250\").getItem(0)), \n",
    "                    \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed|edition)\", \"$1 ed\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Extract year from publication (F260 is array)\n",
    "            F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F260\").getItem(0), \"(1[0-9]{3}|20[0-9]{2})\", 1)\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def normalize_ids_spark(df):\n",
    "    \"\"\"\n",
    "    Normalize ISBN and LCCN using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"normalized_isbn\",\n",
    "        # F020 is array\n",
    "        F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0),\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"F020\").getItem(0), \"([0-9X-]+)\", 1),\n",
    "                \"[^0-9X]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"normalized_lccn\", \n",
    "        # F010 is string\n",
    "        F.when(F.col(\"F010\").isNotNull(),\n",
    "            F.regexp_replace(\n",
    "                F.trim(F.col(\"F010\")),\n",
    "                \"[^a-zA-Z0-9-]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def add_id_list_spark(df):\n",
    "    \"\"\"\n",
    "    Create id_list using Spark SQL array functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"id_list\",\n",
    "        F.array_remove(\n",
    "            F.array(\n",
    "                F.when(F.col(\"normalized_isbn\").isNotNull() & (F.col(\"normalized_isbn\") != \"\"), \n",
    "                       F.col(\"normalized_isbn\")),\n",
    "                F.when(F.col(\"normalized_lccn\").isNotNull() & (F.col(\"normalized_lccn\") != \"\"), \n",
    "                       F.col(\"normalized_lccn\"))\n",
    "            ),\n",
    "            None\n",
    "        )\n",
    "    )\n",
    "\n",
    "def validate_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Validate match keys using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_valid_match_key\",\n",
    "        (F.length(F.col(\"match_key\")) >= 5) &\n",
    "        (~F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"))\n",
    "    ).withColumn(\"match_key_message\",\n",
    "        F.when(F.length(F.col(\"match_key\")) < 5, \"Match key too short\")\n",
    "         .when(F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"), \"Generic match key\")\n",
    "         .otherwise(\"Valid match key\")\n",
    "    )\n",
    "\n",
    "def process_institution_optimized(df, institution_name):\n",
    "    \"\"\"\n",
    "    Apply all optimizations to an institution's DataFrame\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"source\", F.lit(institution_name))\n",
    "        .transform(normalize_ids_spark)\n",
    "        .transform(create_match_key_spark)\n",
    "        .transform(add_id_list_spark)\n",
    "        .transform(validate_match_key_spark)\n",
    "    )\n",
    "\n",
    "print(\"✅ Optimized Spark SQL functions loaded - properly handles mixed string/array field types\")\n",
    "print(\"✅ F010 (LCCN): string, F020 (ISBN): array, F245 (Title): string, F250/F260: arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import re\n",
    "from pymarc import Record, MARCReader\n",
    "\n",
    "# Setup logging for MARC conversion\n",
    "log_dir = f'{output_dir}/logs'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, 'marc2parquet.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_institution_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extract institution name from filename patterns\"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    \n",
    "    # For files from pod-processing-outputs/final/ like \"harvard_updates-001.mrc\"\n",
    "    if '_' in base:\n",
    "        return base.split('_')[0]\n",
    "    \n",
    "    # Pattern: institution-date-descriptor-format.ext\n",
    "    match = re.match(r'^([a-z]+)-[\\d\\-]+-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Pattern: institution-descriptor.ext\n",
    "    match = re.match(r'^([a-z]+)-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Default: use the first word\n",
    "    return base.split('-')[0].split('.')[0]\n",
    "\n",
    "def safe_read_marc_file_with_recovery(file_path: str, temp_output: str) -> Tuple[int, Dict]:\n",
    "    \"\"\"Read MARC file with maximum error recovery and minimal validation\"\"\"\n",
    "    total_records = 0\n",
    "    valid_records = 0\n",
    "    report = {\"total_attempted\": 0, \"parsed\": 0, \"errors\": 0}\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file, open(temp_output, 'wb') as outfile:\n",
    "            reader = MARCReader(file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            \n",
    "            for record_number, record in enumerate(reader, 1):\n",
    "                total_records += 1\n",
    "                \n",
    "                if record is None:\n",
    "                    report[\"errors\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    outfile.write(record.as_marc())\n",
    "                    valid_records += 1\n",
    "                except Exception as e:\n",
    "                    report[\"errors\"] += 1\n",
    "                    logger.warning(f\"Error writing record {record_number}: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "        \n",
    "    report[\"total_attempted\"] = total_records\n",
    "    report[\"parsed\"] = valid_records\n",
    "    \n",
    "    if total_records > 0:\n",
    "        report[\"success_rate\"] = (valid_records / total_records) * 100\n",
    "    else:\n",
    "        report[\"success_rate\"] = 0\n",
    "        \n",
    "    return valid_records, report\n",
    "\n",
    "def get_institution_specific_marc_files() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get all institution-specific MARC files from processed outputs\"\"\"\n",
    "    institution_file_pairs = []\n",
    "    \n",
    "    # Update base path for PySpark notebook environment\n",
    "    base_path = \"/home/jovyan/work/July-2025-PODParquet\"\n",
    "    \n",
    "    # PRIMARY: Look for processed MARC files in the final output directory\n",
    "    final_dir = os.path.join(base_path, 'pod-processing-outputs/final')\n",
    "    \n",
    "    if os.path.exists(final_dir):\n",
    "        # Get all .mrc files from the final directory\n",
    "        final_marc_files = glob.glob(os.path.join(final_dir, '*.mrc'))\n",
    "        \n",
    "        for file in final_marc_files:\n",
    "            # Extract institution from filename (e.g., \"harvard_updates-001.mrc\" -> \"harvard\")\n",
    "            institution = extract_institution_from_filename(file)\n",
    "            institution_file_pairs.append((institution, file))\n",
    "            \n",
    "        print(f\"Found {len(final_marc_files)} processed MARC files in {final_dir}\")\n",
    "    \n",
    "    # SECONDARY: Check the export directory for the latest export package\n",
    "    export_dir = os.path.join(base_path, 'pod-processing-outputs/export')\n",
    "    if os.path.exists(export_dir) and not institution_file_pairs:\n",
    "        # Find the most recent export package\n",
    "        export_packages = glob.glob(os.path.join(export_dir, 'marc_export_*'))\n",
    "        if export_packages:\n",
    "            latest_export = sorted(export_packages)[-1]  # Get most recent by timestamp\n",
    "            export_marc_files = glob.glob(os.path.join(latest_export, '*.mrc'))\n",
    "            \n",
    "            for file in export_marc_files:\n",
    "                # Skip non-MARC files\n",
    "                if file.endswith('.txt'):\n",
    "                    continue\n",
    "                institution = extract_institution_from_filename(file)\n",
    "                institution_file_pairs.append((institution, file))\n",
    "            \n",
    "            print(f\"Found {len(export_marc_files)} MARC files in latest export: {latest_export}\")\n",
    "    \n",
    "    # FALLBACK: If no processed files found, check for raw files\n",
    "    if not institution_file_pairs:\n",
    "        print(\"No processed files found in pod-processing-outputs/final or export directories\")\n",
    "        print(\"Falling back to raw MARC files in pod_*/file directories\")\n",
    "        \n",
    "        # Look for marc files in institution directories\n",
    "        institution_dirs = glob.glob(os.path.join(base_path, \"pod_*/file\"))\n",
    "        \n",
    "        for institution_dir in institution_dirs:\n",
    "            institution = os.path.basename(os.path.dirname(institution_dir)).replace('pod_', '')\n",
    "            \n",
    "            # Look for .mrc files only (no XML)\n",
    "            mrc_files = glob.glob(f\"{institution_dir}/**/*.mrc\", recursive=True)\n",
    "            for file in mrc_files:\n",
    "                institution_file_pairs.append((institution, file))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_pairs = list(set(institution_file_pairs))\n",
    "    unique_pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    print(f\"\\nTotal institution-specific MARC files to process: {len(unique_pairs)}\")\n",
    "    for institution, file in unique_pairs:\n",
    "        print(f\"  - {institution}: {file}\")\n",
    "    \n",
    "    return unique_pairs\n",
    "\n",
    "def process_file_with_recovery(file: str, institution: str) -> bool:\n",
    "    \"\"\"Process a MARC file with maximum error recovery\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        # Create a temporary file for processing\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                           f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "       \n",
    "        # Process MARC file\n",
    "        written_count, report = safe_read_marc_file_with_recovery(file, temp_file)\n",
    "        \n",
    "        # Proceed if we have at least some records\n",
    "        if written_count == 0:\n",
    "            error_msg = f\"No records could be processed from {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Run marctable command - FLDR is included by default\n",
    "        marctable_cmd = f'marctable parquet {temp_file} {output_file}'\n",
    "        marctable_msg = f\"Running marctable: {marctable_cmd}\"\n",
    "        logger.info(marctable_msg)\n",
    "        print(marctable_msg)\n",
    "        exit_status = os.system(marctable_cmd)\n",
    "        \n",
    "        if exit_status != 0:\n",
    "            error_msg = f\"marctable command failed for {institution} file {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        else:\n",
    "            success_msg = f\"SUCCESS: Created {output_file} with {written_count} {institution} records ({report.get('success_rate', 0):.1f}% success rate)\"\n",
    "            logger.info(success_msg)\n",
    "            print(success_msg)\n",
    "            print(f\"  Note: FLDR (leader) field is included by default in marctable output\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error processing {institution} file {file}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if 'temp_file' in locals() and temp_file and os.path.exists(temp_file):\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error for {temp_file}: {str(e)}\")\n",
    "\n",
    "def marc2parquet_institution_specific(force_reprocess=False):\n",
    "    \"\"\"\n",
    "    Convert institution-specific MARC to Parquet with maximum error recovery\n",
    "    \n",
    "    Args:\n",
    "        force_reprocess: If True, reprocess even if parquet files already exist\n",
    "    \"\"\"\n",
    "    # Check if previous processing has been done\n",
    "    if not os.path.exists(f'{output_dir}/final'):\n",
    "        print(f\"WARNING: No processed files found in {output_dir}/final/\")\n",
    "        print(\"Consider running ivyplus-updated-marc-pyspark.ipynb first for better results\")\n",
    "    \n",
    "    institution_file_pairs = get_institution_specific_marc_files()\n",
    "    \n",
    "    if not institution_file_pairs:\n",
    "        error_msg = \"No institution-specific MARC files found to process\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "    \n",
    "    results = []\n",
    "    institution_summary = {}\n",
    "    \n",
    "    for institution, file in institution_file_pairs:\n",
    "        if institution not in institution_summary:\n",
    "            institution_summary[institution] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "        \n",
    "        institution_summary[institution][\"total\"] += 1\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                   f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "        # Skip if already processed unless force_reprocess is True\n",
    "        if not force_reprocess and os.path.exists(output_file):\n",
    "            skip_msg = f\"Skipping already processed {institution} file {file}\"\n",
    "            logger.info(skip_msg)\n",
    "            print(skip_msg)\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "            results.append(True)\n",
    "            continue\n",
    "            \n",
    "        result = process_file_with_recovery(file, institution)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result:\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "        else:\n",
    "            institution_summary[institution][\"failed\"] += 1\n",
    "    \n",
    "    # Print summary by institution\n",
    "    print(\"\\n=== Institution Processing Summary ===\")\n",
    "    for institution, stats in institution_summary.items():\n",
    "        print(f\"{institution.upper()}: Processed {stats['total']} files - {stats['success']} succeeded, {stats['failed']} failed\")\n",
    "    \n",
    "    # Overall success rate\n",
    "    total_success = sum(results)\n",
    "    total_files = len(results)\n",
    "    if total_files > 0:\n",
    "        print(f\"\\nOverall: Successfully processed {total_success} of {total_files} files ({total_success/total_files*100:.1f}%)\")\n",
    "        return total_success == total_files\n",
    "    else:\n",
    "        print(\"\\nNo files were processed\")\n",
    "        return False\n",
    "\n",
    "# Check if conversion is needed or if we can skip directly to processing\n",
    "print(\"Checking for existing parquet files...\")\n",
    "existing_parquet = glob.glob(f\"{output_dir}/*_marc21.parquet\")\n",
    "if existing_parquet:\n",
    "    print(f\"Found {len(existing_parquet)} existing parquet files\")\n",
    "    print(\"You can skip to the next cell unless you want to reprocess\")\n",
    "else:\n",
    "    print(\"No parquet files found. Running conversion...\")\n",
    "    marc2parquet_institution_specific()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing with Memory-Optimized Approach\n",
    "from pyspark.sql.functions import col, explode, size, array_contains, collect_set\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Find all institution parquet files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "parquet_files = glob.glob(f\"{input_dir}/pod-processing-outputs/*-marc21.parquet\")\n",
    "\n",
    "print(f\"Found {len(parquet_files)} institution-specific parquet files to process\")\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"ERROR: No parquet files found!\")\n",
    "    print(f\"Looking in: {input_dir}/pod-processing-outputs/\")\n",
    "    parquet_files = glob.glob(f\"{input_dir}/*-marc21.parquet\")\n",
    "    if parquet_files:\n",
    "        print(f\"Found {len(parquet_files)} files in {input_dir}\")\n",
    "    else:\n",
    "        raise ValueError(\"No parquet files found in the specified directory\")\n",
    "\n",
    "print(\"Files found:\")\n",
    "for file in parquet_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# MEMORY OPTIMIZATION 1: Process and save each institution separately\n",
    "print(\"\\n=== Processing institutions and saving intermediate results ===\")\n",
    "\n",
    "temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "processed_institutions = []\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    try:\n",
    "        filename = os.path.basename(parquet_file)\n",
    "        institution = filename.split('_')[0]\n",
    "        \n",
    "        print(f\"\\nProcessing {institution}...\")\n",
    "        \n",
    "        # Read and process without caching\n",
    "        df = spark.read.parquet(parquet_file)\n",
    "        processed_df = process_institution_optimized(df, institution)\n",
    "        \n",
    "        # Select only needed columns immediately\n",
    "        slim_df = processed_df.select(\n",
    "            \"F001\", \"source\", \"match_key\", \"id_list\", \n",
    "            \"is_valid_match_key\", \"match_key_message\"\n",
    "        )\n",
    "        \n",
    "        # Write to temp location instead of keeping in memory\n",
    "        temp_path = f\"{temp_output_dir}/{institution}_processed.parquet\"\n",
    "        slim_df.coalesce(20).write.mode(\"overwrite\").parquet(temp_path)\n",
    "        \n",
    "        # Get statistics before releasing memory\n",
    "        total_records = slim_df.count()\n",
    "        if total_records > 0:\n",
    "            valid_keys = slim_df.filter(col(\"is_valid_match_key\") == True).count()\n",
    "            print(f\"  - {total_records:,} records processed\")\n",
    "            print(f\"  - {valid_keys:,} ({valid_keys/total_records*100:.1f}%) with valid match keys\")\n",
    "            processed_institutions.append((institution, temp_path))\n",
    "        else:\n",
    "            print(f\"  - WARNING: No records found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  - ERROR processing {institution}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Processed {len(processed_institutions)} institutions\")\n",
    "\n",
    "# MEMORY OPTIMIZATION 2: Read all processed files as a single union\n",
    "print(\"\\n=== Reading all processed data as unified dataset ===\")\n",
    "\n",
    "# Build a list of paths to read\n",
    "processed_paths = [path for _, path in processed_institutions]\n",
    "\n",
    "# Read all at once using Spark's optimized union reading\n",
    "all_df = spark.read.parquet(*processed_paths)\n",
    "\n",
    "# Add key array without caching the full dataset\n",
    "all_df = all_df.withColumn(\"key_array\",\n",
    "    F.array_distinct(\n",
    "        F.concat(\n",
    "            F.col(\"id_list\"),\n",
    "            F.array(F.col(\"match_key\"))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# MEMORY OPTIMIZATION 3: Get statistics using aggregations instead of count()\n",
    "print(\"\\n=== Computing statistics ===\")\n",
    "\n",
    "# Get all statistics in one pass\n",
    "stats_df = all_df.agg(\n",
    "    F.count(\"*\").alias(\"total_keys\"),\n",
    "    F.sum(F.when(col(\"is_valid_match_key\") == True, 1).otherwise(0)).alias(\"valid_keys\"),\n",
    "    F.sum(F.when(col(\"is_valid_match_key\") == False, 1).otherwise(0)).alias(\"invalid_keys\")\n",
    ").collect()[0]\n",
    "\n",
    "total_keys = stats_df[\"total_keys\"]\n",
    "valid_keys = stats_df[\"valid_keys\"]\n",
    "invalid_keys = stats_df[\"invalid_keys\"]\n",
    "\n",
    "if total_keys > 0:\n",
    "    print(f\"\\nMatch key validation results:\")\n",
    "    print(f\"  • Valid match keys: {valid_keys:,} ({valid_keys/total_keys*100:.1f}%)\")\n",
    "    print(f\"  • Invalid match keys: {invalid_keys:,} ({invalid_keys/total_keys*100:.1f}%)\")\n",
    "    \n",
    "    if invalid_keys > 0:\n",
    "        print(\"\\nInvalid match key reasons:\")\n",
    "        all_df.filter(col(\"is_valid_match_key\") == False) \\\n",
    "            .groupBy(\"match_key_message\") \\\n",
    "            .count() \\\n",
    "            .orderBy(col(\"count\").desc()) \\\n",
    "            .show(10, False)\n",
    "\n",
    "# MEMORY OPTIMIZATION 4: Explode and save immediately\n",
    "print(\"\\n=== Creating exploded dataset ===\")\n",
    "\n",
    "# Explode and write to disk instead of keeping in memory\n",
    "all_df_exploded = all_df.withColumn(\"key\", explode(\"key_array\"))\n",
    "\n",
    "# Save the exploded dataset for downstream processing\n",
    "exploded_path = f\"{output_dir}/all_records_exploded.parquet\"\n",
    "all_df_exploded.write.mode(\"overwrite\").parquet(exploded_path)\n",
    "\n",
    "# Get key statistics efficiently\n",
    "print(\"\\n📊 Key array statistics:\")\n",
    "key_stats = all_df.select(F.size(\"key_array\").alias(\"num_keys\")) \\\n",
    "    .groupBy(\"num_keys\").count() \\\n",
    "    .orderBy(\"num_keys\") \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Distribution of number of keys per record:\")\n",
    "for row in key_stats:\n",
    "    print(f\"  {row['num_keys']} keys: {row['count']:,} records\")\n",
    "\n",
    "# For next processing step, read from disk\n",
    "all_df_exploded = spark.read.parquet(exploded_path)\n",
    "\n",
    "print(\"\\n✅ Memory-optimized processing complete!\")\n",
    "print(f\"Intermediate results saved to: {temp_output_dir}\")\n",
    "print(f\"Exploded dataset saved to: {exploded_path}\")\n",
    "\n",
    "# Optional: Clean up temp files after verification\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness Analysis and Overlap Detection\n",
    "from pyspark.sql.functions import collect_set, array_contains, size, col\n",
    "\n",
    "# Group by key and collect sources where that key appears\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# Broadcast the grouped DataFrame for more efficient joins\n",
    "grouped_broadcast = F.broadcast(grouped)\n",
    "\n",
    "# Find Penn records that exist in OTHER libraries\n",
    "# A Penn record is NOT unique if it exists in ANY other library\n",
    "penn_keys_in_other_libs = grouped_broadcast.filter(\n",
    "    (array_contains(col(\"sources\"), \"penn\")) & \n",
    "    (F.size(col(\"sources\")) > 1)  # Penn + at least one other library\n",
    ").select(\"key\")\n",
    "\n",
    "# Get Penn records that are truly unique to Penn\n",
    "# Start with all Penn records\n",
    "all_penn_exploded = all_df_exploded.filter(col(\"source\") == \"penn\")\n",
    "\n",
    "# Anti-join to remove Penn records found in other libraries\n",
    "unique_penn_exploded = all_penn_exploded.join(\n",
    "    penn_keys_in_other_libs,  # No need to re-broadcast\n",
    "    on=\"key\", \n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Deduplicate by Penn's F001 (not match_key) to get unique Penn records\n",
    "unique_penn = unique_penn_exploded.drop(\"key\").dropDuplicates([\"F001\"])\n",
    "\n",
    "# Cache the unique Penn records for better performance\n",
    "unique_penn.cache()\n",
    "\n",
    "# Calculate statistics efficiently\n",
    "unique_penn_count = unique_penn.count()  # Force cache materialization\n",
    "\n",
    "# Get total Penn records from the deduplicated exploded DataFrame\n",
    "total_penn = all_penn_exploded.select(\"F001\").distinct().count()\n",
    "\n",
    "print(f\"\\n=== Analysis Results ===\")\n",
    "print(f\"Total Penn records: {total_penn:,}\")\n",
    "print(f\"Unique Penn records: {unique_penn_count:,}\")\n",
    "\n",
    "# Add robust checking for division by zero\n",
    "if total_penn > 0:\n",
    "    print(f\"Uniqueness rate: {unique_penn_count/total_penn*100:.1f}%\")\n",
    "    print(f\"Overlap rate: {(total_penn - unique_penn_count)/total_penn*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Uniqueness rate: N/A (no Penn records found)\")\n",
    "\n",
    "# For analysis, let's also see overlap statistics\n",
    "print(\"\\n=== Overlap Analysis ===\")\n",
    "\n",
    "# More efficient: get Penn overlap stats without re-filtering\n",
    "penn_keys = grouped.filter(array_contains(col(\"sources\"), \"penn\")).cache()\n",
    "\n",
    "penn_overlap_stats = penn_keys \\\n",
    "    .withColumn(\"num_libraries\", F.size(col(\"sources\"))) \\\n",
    "    .groupBy(\"num_libraries\").count() \\\n",
    "    .orderBy(\"num_libraries\")\n",
    "\n",
    "print(\"Distribution of Penn records by number of libraries holding them:\")\n",
    "penn_overlap_stats.show()\n",
    "\n",
    "# Save results with consistent paths using pod-processing-outputs directory\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Save unique Penn records\n",
    "unique_penn.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "\n",
    "# Save detailed overlap information for analysis\n",
    "# Note: Using cached penn_keys for efficiency\n",
    "penn_with_overlap_info = all_penn_exploded.join(\n",
    "    penn_keys.select(\"key\", \"sources\", F.size(\"sources\").alias(\"num_libraries\")),\n",
    "    on=\"key\",\n",
    "    how=\"left\"\n",
    ").drop(\"key\")\n",
    "\n",
    "penn_with_overlap_info.write.mode(\"overwrite\").parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "\n",
    "# Save validation statistics for analysis\n",
    "validation_stats = all_df.select(\"F001\", \"match_key\", \"is_valid_match_key\", \"match_key_message\", \"id_list\") \\\n",
    "    .filter(col(\"source\") == \"penn\")\n",
    "\n",
    "validation_stats.write.mode(\"overwrite\").parquet(f\"{output_dir}/match_key_validation_stats.parquet\")\n",
    "\n",
    "# Unpersist cached DataFrames to free memory\n",
    "penn_keys.unpersist()\n",
    "unique_penn.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source Validation (Updated: July 2025)\n",
    "# Validates Penn MARC data sources and ensures current data is used\n",
    "# Requires explicit confirmation for legacy data usage\n",
    "\n",
    "# Use Leader field FLDR to make a print set from unique penn and non-print\n",
    "from pyspark.sql.functions import col, substring, when, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load the unique Penn dataset if not already loaded\n",
    "if 'unique_penn' not in locals() or unique_penn is None:\n",
    "    print(\"Loading unique Penn dataset...\")\n",
    "    unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "else:\n",
    "    print(\"Using existing unique_penn DataFrame\")\n",
    "\n",
    "# CRITICAL: Verify Penn data currency before processing\n",
    "def verify_penn_data_source(matching_files):\n",
    "    \"\"\"\n",
    "    Verify the Penn data source and warn if using outdated data\n",
    "    \"\"\"\n",
    "    if not matching_files:\n",
    "        return None\n",
    "        \n",
    "    selected_file = matching_files[0]\n",
    "    file_info = {\n",
    "        'path': selected_file,\n",
    "        'filename': os.path.basename(selected_file),\n",
    "        'is_legacy': 'penn-2022-07-20' in selected_file,\n",
    "        'is_processed': 'pod-processing-outputs' in selected_file\n",
    "    }\n",
    "    \n",
    "    # Extract date from filename if possible\n",
    "    date_pattern = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "    date_match = re.search(date_pattern, file_info['filename'])\n",
    "    if date_match:\n",
    "        file_info['data_date'] = date_match.group(1)\n",
    "    else:\n",
    "        file_info['data_date'] = 'unknown'\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Load full Penn records - prioritize most recent processed data\n",
    "penn_full_paths = [\n",
    "    # PRIMARY: Penn parquet files from current processing pipeline\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*updates*marc21.parquet\",\n",
    "    \n",
    "    # SECONDARY: Any Penn parquet files in processing outputs\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*.parquet\",\n",
    "    \n",
    "    # TERTIARY: Check for raw Penn parquet files (less preferred)\n",
    "    f\"{input_dir}/pod_penn/file/**/*.parquet\"\n",
    "]\n",
    "\n",
    "# Add data source verification\n",
    "penn_full = None\n",
    "selected_source = None\n",
    "\n",
    "print(\"\\n=== PENN DATA SOURCE VERIFICATION ===\")\n",
    "for path_pattern in penn_full_paths:\n",
    "    try:\n",
    "        matching_files = glob.glob(path_pattern, recursive=True)\n",
    "        if matching_files:\n",
    "            # Sort files by modification time to get most recent\n",
    "            matching_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "            \n",
    "            source_info = verify_penn_data_source(matching_files)\n",
    "            if source_info:\n",
    "                print(f\"\\nFound Penn records at: {source_info['path']}\")\n",
    "                print(f\"  - Source type: {'Processed updates' if source_info['is_processed'] else 'Raw data'}\")\n",
    "                print(f\"  - Data date: {source_info['data_date']}\")\n",
    "                \n",
    "                # Warn if data appears old\n",
    "                if source_info['data_date'] != 'unknown':\n",
    "                    try:\n",
    "                        data_date = datetime.strptime(source_info['data_date'], '%Y-%m-%d')\n",
    "                        days_old = (datetime.now() - data_date).days\n",
    "                        if days_old > 365:\n",
    "                            print(f\"  ⚠️  WARNING: Data is {days_old} days old!\")\n",
    "                            print(f\"  ⚠️  Results may not reflect current Penn holdings\")\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Load the data\n",
    "                penn_full = spark.read.parquet(source_info['path'])\n",
    "                selected_source = source_info\n",
    "                \n",
    "                # Verify record count and sample for currency check\n",
    "                record_count = penn_full.count()\n",
    "                print(f\"  - Total records: {record_count:,}\")\n",
    "                \n",
    "                # Sample check for recent cataloging activity\n",
    "                if 'F005' in penn_full.columns:\n",
    "                    recent_updates = penn_full.filter(\n",
    "                        col(\"F005\").rlike(\"202[4-5]\")\n",
    "                    ).count()\n",
    "                    recent_percentage = (recent_updates / record_count * 100) if record_count > 0 else 0\n",
    "                    print(f\"  - Recently updated records (2024-2025): {recent_updates:,} ({recent_percentage:.1f}%)\")\n",
    "                    \n",
    "                    if recent_percentage < 5:\n",
    "                        print(f\"  ⚠️  WARNING: Only {recent_percentage:.1f}% of records updated recently\")\n",
    "                        print(f\"  ⚠️  Data may be significantly outdated\")\n",
    "                \n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {path_pattern}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Final fallback with strong warning\n",
    "if penn_full is None:\n",
    "    print(\"\\n⚠️  CRITICAL WARNING: No current Penn data found!\")\n",
    "    print(\"As a last resort, checking for legacy data...\")\n",
    "    \n",
    "    legacy_path = \"/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet\"\n",
    "    if os.path.exists(legacy_path):\n",
    "        response = input(\"\\n🚨 Found 2022 Penn data. This is SEVERELY OUTDATED. Use anyway? (yes/no): \")\n",
    "        if response.lower() == 'yes':\n",
    "            penn_full = spark.read.parquet(legacy_path)\n",
    "            selected_source = {'is_legacy': True, 'filename': 'penn-2022-07-20-full-marc21.parquet'}\n",
    "            print(\"⚠️  Using 2022 data - results will NOT reflect current Penn holdings!\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No Penn full MARC records found and user declined legacy data\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not find Penn full MARC records!\")\n",
    "        print(\"Please ensure Penn MARC data has been converted to Parquet format.\")\n",
    "        print(\"Run the previous cells to process MARC files first.\")\n",
    "        raise FileNotFoundError(\"Penn full MARC records not found\")\n",
    "\n",
    "print(\"\\n=== PROCEEDING WITH ANALYSIS ===\")\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"⚠️  USING OUTDATED DATA - RESULTS MAY BE INACCURATE\")\n",
    "\n",
    "# OPTIMIZATION: Use join instead of SQL IN clause for better performance\n",
    "unique_penn_ids = unique_penn.select(\"F001\").distinct()\n",
    "unique_penn_full = penn_full.join(unique_penn_ids, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# OPTIMIZATION: Apply all transformations in a single chain\n",
    "unique_penn_with_material_type = (unique_penn_full\n",
    "    # Filter out 533 fields first\n",
    "    .filter(col(\"F533\").isNull())\n",
    "    # Add material type columns\n",
    "    .withColumn(\"record_type\", substring(col(\"FLDR\"), 7, 1))\n",
    "    .withColumn(\"bib_level\", substring(col(\"FLDR\"), 8, 1))\n",
    "    .withColumn(\"combined_type\", concat(col(\"record_type\"), col(\"bib_level\")))\n",
    "    .withColumn(\"material_category\", \n",
    "        when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"m\")), \"print_book\")\n",
    "        .when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"s\")), \"print_serial\")\n",
    "        .when((col(\"record_type\") == \"c\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_music\")\n",
    "        .when((col(\"record_type\") == \"e\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_maps\")\n",
    "        .when(col(\"record_type\") == \"m\", \"electronic_resource\")\n",
    "        .when(col(\"record_type\").isin(\"g\", \"k\"), \"visual_material\")\n",
    "        .when(col(\"record_type\") == \"i\", \"audio_material\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "    .withColumn(\"is_print\", \n",
    "        col(\"material_category\").isin(\"print_book\", \"print_serial\", \"print_music\", \"print_maps\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache before multiple operations\n",
    "unique_penn_with_material_type.cache()\n",
    "\n",
    "# OPTIMIZATION: Get all statistics in one pass\n",
    "print(\"\\n=== Material Type Distribution ===\")\n",
    "material_stats = unique_penn_with_material_type.groupBy(\"material_category\", \"is_print\").count().collect()\n",
    "\n",
    "# Process statistics\n",
    "material_counts_dict = {}\n",
    "print_count = 0\n",
    "non_print_count = 0\n",
    "\n",
    "for row in material_stats:\n",
    "    material_counts_dict[row[\"material_category\"]] = row[\"count\"]\n",
    "    if row[\"is_print\"]:\n",
    "        print_count += row[\"count\"]\n",
    "    else:\n",
    "        non_print_count += row[\"count\"]\n",
    "\n",
    "total_unique = print_count + non_print_count\n",
    "\n",
    "# Display material distribution\n",
    "for category, count in sorted(material_counts_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{category}: {count:,}\")\n",
    "\n",
    "# Filter for print materials only\n",
    "print_only_df = unique_penn_with_material_type.filter(col(\"is_print\") == True)\n",
    "\n",
    "# Add metadata if we have source information\n",
    "if selected_source:\n",
    "    print_only_df_with_metadata = print_only_df.withColumn(\n",
    "        \"processing_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    ).withColumn(\n",
    "        \"source_file\", lit(selected_source.get('filename', 'unknown'))\n",
    "    ).withColumn(\n",
    "        \"data_currency_warning\", \n",
    "        lit(\"OUTDATED - 2022 data\" if selected_source.get('is_legacy') else \"Current\")\n",
    "    )\n",
    "else:\n",
    "    print_only_df_with_metadata = print_only_df\n",
    "\n",
    "# Save datasets\n",
    "unique_penn_with_material_type.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn_full_no_533.parquet\")\n",
    "print_only_df_with_metadata.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\n=== Print Material Analysis ===\")\n",
    "print(f\"Total unique Penn records: {total_unique:,}\")\n",
    "\n",
    "if total_unique > 0:\n",
    "    print(f\"Print materials: {print_count:,} ({print_count/total_unique*100:.1f}%)\")\n",
    "    print(f\"Non-print materials: {non_print_count:,} ({non_print_count/total_unique*100:.1f}%)\")\n",
    "    \n",
    "    # Show print categories breakdown\n",
    "    print(\"\\n=== Print Material Categories ===\")\n",
    "    print_categories = [\"print_book\", \"print_serial\", \"print_music\", \"print_maps\"]\n",
    "    for category in print_categories:\n",
    "        if category in material_counts_dict:\n",
    "            count = material_counts_dict[category]\n",
    "            print(f\"{category}: {count:,} ({count/print_count*100:.1f}% of print materials)\")\n",
    "else:\n",
    "    print(\"No unique Penn records found to analyze\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "unique_penn_with_material_type.unpersist()\n",
    "\n",
    "# Final warning if using outdated data\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚨 CRITICAL WARNING: Analysis completed using 2022 Penn data\")\n",
    "    print(\"🚨 Results do NOT reflect current Penn holdings\")\n",
    "    print(\"🚨 Recommended: Re-run with current Penn MARC export\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling and Final Analysis\n",
    "from pyspark.sql.functions import rand, col\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "# Define output directory if not already defined\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load print materials dataset if not already loaded\n",
    "if 'print_only_df' not in locals() or print_only_df is None:\n",
    "    print(\"Loading print materials dataset...\")\n",
    "    print_only_df_raw = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "    \n",
    "    # Check if metadata columns exist and drop them for sampling\n",
    "    metadata_cols = [\"processing_date\", \"source_file\", \"data_currency_warning\"]\n",
    "    existing_metadata_cols = [col for col in metadata_cols if col in print_only_df_raw.columns]\n",
    "    \n",
    "    if existing_metadata_cols:\n",
    "        print(f\"Dropping metadata columns: {existing_metadata_cols}\")\n",
    "        print_only_df = print_only_df_raw.drop(*existing_metadata_cols)\n",
    "    else:\n",
    "        print_only_df = print_only_df_raw\n",
    "else:\n",
    "    print(\"Using existing print_only_df DataFrame\")\n",
    "\n",
    "# Load or compute necessary statistics if not available\n",
    "if 'total_penn' not in locals() or 'unique_penn_count' not in locals():\n",
    "    print(\"Loading required statistics...\")\n",
    "    # Load from saved parquet files\n",
    "    if 'unique_penn' not in locals():\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "    unique_penn_count = unique_penn.count()\n",
    "    \n",
    "    # Load Penn overlap analysis to get total Penn records\n",
    "    penn_overlap = spark.read.parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "    total_penn = penn_overlap.select(\"F001\").distinct().count()\n",
    "\n",
    "# Compute print statistics if not available\n",
    "if 'print_count' not in locals() or 'material_counts_dict' not in locals():\n",
    "    print(\"Computing material type statistics...\")\n",
    "    # Check for material_category column\n",
    "    if 'material_category' not in print_only_df.columns:\n",
    "        print(\"ERROR: material_category column not found in print_only_df\")\n",
    "        raise ValueError(\"Missing required column: material_category\")\n",
    "    \n",
    "    material_stats = print_only_df.groupBy(\"material_category\").count().collect()\n",
    "    material_counts_dict = {row[\"material_category\"]: row[\"count\"] for row in material_stats}\n",
    "    print_count = sum(material_counts_dict.values())\n",
    "\n",
    "# Define sampling function with improved stratification\n",
    "def create_stratified_sample(df, strata_column, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with improved randomization.\n",
    "    Uses multiple passes to ensure representation of all strata.\n",
    "    \"\"\"\n",
    "    print(f\"Creating stratified sample based on {strata_column}...\")\n",
    "    \n",
    "    # Verify the strata column exists\n",
    "    if strata_column not in df.columns:\n",
    "        print(f\"ERROR: Column '{strata_column}' not found in DataFrame\")\n",
    "        print(f\"Available columns: {df.columns}\")\n",
    "        raise ValueError(f\"Missing required column: {strata_column}\")\n",
    "    \n",
    "    # Get counts by strata for weighting\n",
    "    strata_counts = df.groupBy(strata_column).count().collect()\n",
    "    total_records = df.count()\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"WARNING: No records to sample from!\")\n",
    "        return df\n",
    "    \n",
    "    strata_map = {row[strata_column]: row[\"count\"] for row in strata_counts}\n",
    "    print(f\"Strata distribution:\")\n",
    "    for strata, count in sorted(strata_map.items()):\n",
    "        print(f\"  - {strata}: {count:,} records ({count/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate proportional sample sizes with minimum threshold\n",
    "    min_per_strata = 5  # Ensure at least a few records from each stratum\n",
    "    sample_fractions = {}\n",
    "    \n",
    "    for strata, count in strata_map.items():\n",
    "        # Proportional sampling with minimum threshold\n",
    "        if count > 0:\n",
    "            # Calculate proportional share but ensure at least min_per_strata\n",
    "            prop_size = max(\n",
    "                min_per_strata,\n",
    "                int((count / total_records) * sample_size)\n",
    "            )\n",
    "            \n",
    "            # Don't sample more than we have\n",
    "            prop_size = min(prop_size, count)\n",
    "            \n",
    "            # Calculate fraction\n",
    "            sample_fractions[strata] = prop_size / count\n",
    "    \n",
    "    # First pass: Stratified sampling\n",
    "    sampled_df = df.sampleBy(strata_column, fractions=sample_fractions, seed=42)\n",
    "    \n",
    "    # Check if we need a second pass to reach target size\n",
    "    current_size = sampled_df.count()\n",
    "    print(f\"First pass sample size: {current_size}\")\n",
    "    \n",
    "    if current_size < sample_size and current_size < total_records:\n",
    "        # Second pass: Sample from under-represented strata\n",
    "        remaining = min(sample_size - current_size, total_records - current_size)\n",
    "        print(f\"Need {remaining} more records to reach target sample size\")\n",
    "        \n",
    "        # Get records not in first sample\n",
    "        sampled_ids = sampled_df.select(\"F001\").distinct()\n",
    "        remaining_df = df.join(sampled_ids, on=\"F001\", how=\"left_anti\")\n",
    "        \n",
    "        remaining_count = remaining_df.count()\n",
    "        if remaining_count > 0:\n",
    "            # Simple random sample from remaining records\n",
    "            additional_sample = remaining_df.orderBy(rand(seed=43)).limit(remaining)\n",
    "            \n",
    "            # Union the samples\n",
    "            sampled_df = sampled_df.union(additional_sample)\n",
    "            print(f\"Added {min(remaining, remaining_count)} additional records\")\n",
    "    \n",
    "    final_size = sampled_df.count()\n",
    "    print(f\"Final sample size: {final_size}\")\n",
    "    \n",
    "    # Check distribution in final sample\n",
    "    sample_distribution = sampled_df.groupBy(strata_column).count().collect()\n",
    "    print(f\"\\nSample distribution by {strata_column}:\")\n",
    "    sample_dict = {row[strata_column]: row[\"count\"] for row in sample_distribution}\n",
    "    \n",
    "    for strata_val in sorted(strata_map.keys()):\n",
    "        original_count = strata_map.get(strata_val, 0)\n",
    "        sample_count = sample_dict.get(strata_val, 0)\n",
    "        if original_count > 0 and final_size > 0:\n",
    "            print(f\"  - {strata_val}: {sample_count} ({sample_count/final_size*100:.2f}% of sample vs {original_count/total_records*100:.2f}% of population)\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Create a stratified sample by material category\n",
    "sample_df = create_stratified_sample(print_only_df, \"material_category\", sample_size=1000)\n",
    "\n",
    "# Cache the sample for better performance\n",
    "sample_df.cache()\n",
    "\n",
    "# Save the sample for API validation\n",
    "sample_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/statistical_sample_for_api_no_hsp.parquet\")\n",
    "\n",
    "# Convert to CSV for easier human review\n",
    "# Select key fields for the CSV\n",
    "sample_for_csv = sample_df.select(\n",
    "    \"F001\", \"F020\", \"F010\", \"F245\", \"F250\", \"F260\", \"material_category\"\n",
    ")\n",
    "\n",
    "# Save as CSV (single file for easier review)\n",
    "sample_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/statistical_sample_for_api_no_hsp.csv\")\n",
    "\n",
    "# Generate final summary statistics in JSON format\n",
    "summary_stats = {\n",
    "    \"processing_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_penn_records\": int(total_penn),\n",
    "    \"unique_penn_records\": int(unique_penn_count),\n",
    "    \"uniqueness_rate\": float(unique_penn_count/total_penn) if total_penn > 0 else 0.0,\n",
    "    \"print_materials\": int(print_count),\n",
    "    \"print_materials_percentage\": float(print_count/unique_penn_count) if unique_penn_count > 0 else 0.0,\n",
    "    \"sample_size\": int(sample_df.count()),\n",
    "    \"material_categories\": {}\n",
    "}\n",
    "\n",
    "# Add material categories to summary\n",
    "for category, count in sorted(material_counts_dict.items()):\n",
    "    summary_stats[\"material_categories\"][category] = {\n",
    "        \"count\": int(count),\n",
    "        \"percentage\": float(count/print_count*100) if print_count > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# Write summary to JSON file\n",
    "with open(f\"{output_dir}/sample_summary_no_hsp.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "# Unpersist the cached sample\n",
    "sample_df.unpersist()\n",
    "\n",
    "print(\"\\n✅ Processing complete!\")\n",
    "print(f\"Results saved to {output_dir}/\")\n",
    "print(\"\\nFinal outputs:\")\n",
    "print(f\"  - unique_penn.parquet: All unique Penn records\")\n",
    "print(f\"  - physical_books_no_533.parquet: Unique Penn physical books\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\")\n",
    "print(f\"  - sample_summary_no_hsp.json: Summary statistics\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n📊 Summary Statistics:\")\n",
    "print(f\"  - Total Penn records: {summary_stats['total_penn_records']:,}\")\n",
    "print(f\"  - Unique Penn records: {summary_stats['unique_penn_records']:,}\")\n",
    "print(f\"  - Uniqueness rate: {summary_stats['uniqueness_rate']*100:.1f}%\")\n",
    "print(f\"  - Print materials: {summary_stats['print_materials']:,}\")\n",
    "print(f\"  - Print materials percentage: {summary_stats['print_materials_percentage']:.1f}%\")\n",
    "print(f\"  - Sample size: {summary_stats['sample_size']:,}\")\n",
    "\n",
    "# Display material category breakdown\n",
    "if material_counts_dict:\n",
    "    print(\"\\n📚 Material Category Breakdown:\")\n",
    "    for category, info in sorted(summary_stats[\"material_categories\"].items()):\n",
    "        print(f\"  - {category}: {info['count']:,} ({info['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Cell - Run this to free all resources\n",
    "def cleanup_spark_resources():\n",
    "    \"\"\"Clean up all cached DataFrames and temporary views\"\"\"\n",
    "    try:\n",
    "        # Get all cached DataFrames\n",
    "        for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "            rdd.unpersist()\n",
    "        \n",
    "        # Drop all temporary views\n",
    "        for view in spark.catalog.listTables():\n",
    "            if view.isTemporary:\n",
    "                spark.catalog.dropTempView(view.name)\n",
    "        \n",
    "        print(\"✅ All Spark resources cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning: {e}\")\n",
    "\n",
    "cleanup_spark_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
