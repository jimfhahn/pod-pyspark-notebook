{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POST Processing POD Reports\n",
    "\n",
    "This notebook verifies the uniqueness of Penn holdings identified by the main processing pipeline. It reads from the standardized outputs in `pod-processing-outputs/` and performs final validation using the BorrowDirect API and Selenium-based verification.\n",
    "\n",
    "## Key Integration Points:\n",
    "1. **Input**: Reads from pipeline outputs in order of preference:\n",
    "   - `pod-processing-outputs/statistical_sample_for_api_no_hsp.parquet` (statistical sample)\n",
    "   - `pod-processing-outputs/physical_books_no_533.parquet` (filtered dataset)\n",
    "   - `pod-processing-outputs/unique_penn.parquet` (unique Penn records)\n",
    "   - Legacy Excel fallback support\n",
    "2. **HSP Filtering**: Already applied in main pipeline (conditionally applied here if needed)\n",
    "3. **ML Filtering**: Applies machine learning to identify ~1M BorrowDirect-unique records from the 1.6M dataset\n",
    "4. **BorrowDirect Results**: Leverages existing results or performs fresh API calls with recovery support\n",
    "5. **HathiTrust Integration**: Checks digital availability for identified holdings\n",
    "6. **Output**: Saves multiple datasets with confidence intervals:\n",
    "   - Confirmed Penn-only holdings: `penn_unique_confirmed.xlsx/parquet`\n",
    "   - Indeterminate holdings: `penn_indeterminate_holdings.xlsx/parquet`\n",
    "   - ML-filtered BD-unique dataset: `penn_bd_unique_1m_filtered.parquet`\n",
    "   - Complete verification results and summary statistics\n",
    "\n",
    "## Enhanced Workflow:\n",
    "1. **Data Loading & Validation**: Load data from main pipeline outputs with robust column handling and data lineage tracking\n",
    "2. **Data Processing Overview**: Visual pipeline flow showing all processing stages\n",
    "3. **HSP Filtering**: Apply only if not already done in main pipeline\n",
    "4. **API Processing**: Use existing BorrowDirect results or fetch fresh data via API (with sample-based optimization for large datasets)\n",
    "5. **ML Filtering**: Random Forest model identifies ~1M likely BD-unique records from 1.6M dataset\n",
    "6. **Selenium Verification**: Sample-based holdings verification (1,000 records) with statistical context\n",
    "7. **Statistical Extrapolation**: Results extrapolated to full ~1M dataset with 95% confidence intervals\n",
    "8. **HathiTrust Check**: Digital availability check on representative sample (5,000 records)\n",
    "9. **Final Export**: Categorized holdings with comprehensive documentation and confidence intervals\n",
    "\n",
    "## Key Features:\n",
    "- **Smart Recovery**: Automatically uses existing API results if available\n",
    "- **Large Dataset Handling**: Uses statistical sampling for datasets >10,000 records\n",
    "- **Machine Learning**: Random Forest model reduces 1.6M records to ~1M BD-unique holdings\n",
    "- **Statistical Rigor**: 95% confidence intervals for all extrapolated estimates\n",
    "- **Memory Management**: Automatic Spark cleanup after ML processing\n",
    "- **Coverage Monitoring**: Alerts when API coverage is below 50% with actionable suggestions\n",
    "- **Status Tracking**: Distinguishes between determined, indeterminate, and error states\n",
    "- **Dual Export**: Tracks both confirmed unique holdings and potentially unique indeterminate records\n",
    "- **HathiTrust Integration**: Identifies digitization opportunities\n",
    "\n",
    "## Statistical Methodology:\n",
    "- **Sampling**: Uses 1,000 record sample for Selenium verification (95% confidence ¬±3.1%)\n",
    "- **ML Training**: Trains on sample to identify borrow-direct-unique characteristics\n",
    "- **Extrapolation**: Projects results to full ~1M ML-filtered dataset with confidence intervals\n",
    "- **Transparency**: Clear documentation of sample sizes, confidence levels, and margins of error\n",
    "\n",
    "## Output Interpretation:\n",
    "The pipeline produces estimates with confidence intervals rather than exact counts:\n",
    "- **Example**: \"~300,000 (287,000-313,000) Penn-unique holdings\" instead of just \"300,000\"\n",
    "- **Context**: Results show both minimum confirmed and maximum potential unique holdings\n",
    "- **Coverage**: Automatically monitors and reports API coverage percentage\n",
    "\n",
    "The pipeline now provides statistically sound estimates with proper confidence intervals, enhanced memory management, and comprehensive quality monitoring throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/23 09:44:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/23 09:44:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session ready\n",
      "üìÇ Attempting to load: pod-processing-outputs/unique_penn.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1,596,684 records from pod-processing-outputs/unique_penn.parquet\n",
      "\n",
      "üéØ Dataset loaded from: pod-processing-outputs/unique_penn.parquet\n",
      "üìä Shape: (1596684, 7)\n",
      "üìã Columns (7): ['F001', 'source', 'match_key', 'id_list', 'is_valid_match_key', 'match_key_message', 'key_array']\n",
      "\n",
      "üìà Quick Statistics:\n",
      "  Total records: 1,596,684\n",
      "  Memory usage: 726.2 MB\n",
      "  Memory usage: 726.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Load data from main pipeline outputs - Updated and Robust\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "# Initialize Spark if needed\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"PostProcessing-Aligned\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "    print(\"‚úÖ Spark session ready\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Spark not available, using pandas for file reading\")\n",
    "    spark = None\n",
    "\n",
    "# Replace the input_files list in post-processing.ipynb with:\n",
    "input_files = [\n",
    "    \"pod-processing-outputs/statistical_sample_for_api_no_hsp.parquet\",  # Sample from pod-processing\n",
    "    \"pod-processing-outputs/physical_books_no_533.parquet\",  # Final filtered dataset (533 removed)\n",
    "    \"pod-processing-outputs/unique_penn.parquet\",  # Basic unique Penn records\n",
    "    \"pod-processing-outputs/penn_overlap_analysis.parquet\",  # Alternative analysis file\n",
    "    \"unique_penn_text.xlsx\"  # Legacy Excel fallback\n",
    "]\n",
    "\n",
    "# Try to load from pipeline outputs\n",
    "df = None\n",
    "loaded_from = None\n",
    "\n",
    "for input_file in input_files:\n",
    "    if os.path.exists(input_file):\n",
    "        try:\n",
    "            print(f\"üìÇ Attempting to load: {input_file}\")\n",
    "            if input_file.endswith('.parquet'):\n",
    "                if spark:\n",
    "                    df_spark = spark.read.parquet(input_file)\n",
    "                    df = df_spark.toPandas()\n",
    "                else:\n",
    "                    df = pd.read_parquet(input_file)\n",
    "                loaded_from = input_file\n",
    "                print(f\"‚úÖ Loaded {len(df):,} records from {input_file}\")\n",
    "                break\n",
    "            elif input_file.endswith('.xlsx'):\n",
    "                df = pd.read_excel(input_file)\n",
    "                loaded_from = input_file\n",
    "                print(f\"‚úÖ Loaded {len(df):,} records from {input_file}\")\n",
    "                break\n",
    "            elif input_file.endswith('.csv'):\n",
    "                df = pd.read_csv(input_file)\n",
    "                loaded_from = input_file\n",
    "                print(f\"‚úÖ Loaded {len(df):,} records from {input_file}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {input_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"‚ùå No valid input files found. Please run the main pipeline first.\")\n",
    "\n",
    "print(f\"\\nüéØ Dataset loaded from: {loaded_from}\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"üìã Columns ({len(df.columns)}): {list(df.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nüìà Quick Statistics:\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline Overview\n",
    "\n",
    "This section provides an overview of the complete POD post-processing workflow and data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä POD POST-PROCESSING PIPELINE FLOW\n",
      "==================================================\n",
      "1Ô∏è‚É£ Load data (~1.6M records from unique_penn.parquet)\n",
      "2Ô∏è‚É£ Apply ML filter ‚Üí ~1M BD-unique records\n",
      "3Ô∏è‚É£ Map BorrowDirect IDs (using sample)\n",
      "4Ô∏è‚É£ Selenium verification (1,000 sample)\n",
      "5Ô∏è‚É£ Extrapolate results to full ~1M dataset\n",
      "6Ô∏è‚É£ HathiTrust check (5,000 sample)\n",
      "7Ô∏è‚É£ Export final estimates\n",
      "\n",
      "üìå Key: Samples are used for API calls due to rate limits\n",
      "        Results are extrapolated with confidence intervals\n"
     ]
    }
   ],
   "source": [
    "# Data Processing Pipeline Overview - CORRECTED\n",
    "print(\"üìä POD POST-PROCESSING PIPELINE FLOW\")\n",
    "print(\"=\"*50)\n",
    "print(\"1Ô∏è‚É£ Load data (~1.6M records from unique_penn.parquet)\")\n",
    "print(\"2Ô∏è‚É£ Apply ML filter ‚Üí ~1M BD-unique records\")\n",
    "print(\"3Ô∏è‚É£ Map BorrowDirect IDs (using sample)\")\n",
    "print(\"4Ô∏è‚É£ Selenium verification (1,000 sample)\")\n",
    "print(\"5Ô∏è‚É£ Extrapolate results to full ~1M dataset\")\n",
    "print(\"6Ô∏è‚É£ HathiTrust check (5,000 sample)\")\n",
    "print(\"7Ô∏è‚É£ Export final estimates\")\n",
    "print(\"\\nüìå Key: Samples are used for API calls due to rate limits\")\n",
    "print(\"        Results are extrapolated with confidence intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available columns:\n",
      "   1. F001                           (1,596,684 non-null, 0.0% null)\n",
      "   2. source                         (1,596,684 non-null, 0.0% null)\n",
      "   3. match_key                      (1,596,684 non-null, 0.0% null)\n",
      "   4. id_list                        (0 non-null, 100.0% null)\n",
      "   5. is_valid_match_key             (1,596,684 non-null, 0.0% null)\n",
      "   6. match_key_message              (1,596,684 non-null, 0.0% null)\n",
      "   7. key_array                      (1,596,684 non-null, 0.0% null)\n",
      "\n",
      "=== Data Processing Status ===\n",
      "üîÑ Processing Date: 2025-07-23\n",
      "üìÇ Source File: pod-processing-outputs/unique_penn.parquet\n",
      "\n",
      "üîë Key Columns Status:\n",
      "  ‚úÖ record_id: F001\n",
      "  ‚úÖ match_key: match_key\n",
      "  ‚ö†Ô∏è borrowdir_results: None\n",
      "  ‚ùå hsp_filtered: False\n",
      "\n",
      "üìã Data Lineage:\n",
      "  ‚Ä¢ Using F001 as record identifier\n",
      "  ‚Ä¢ Using match_key for match key comparison\n",
      "\n",
      "üìä Data Sample (first 3 rows):\n",
      "               F001                                          match_key\n",
      "0  9910001563503681  welfare policy for the 1990s edited by phoebe ...\n",
      "1  9910004073503681  rural labourers in bengal 1880 to 1980 willem ...\n",
      "2  9910004943503681  earthquake hazards and the design of construct...\n",
      "   6. match_key_message              (1,596,684 non-null, 0.0% null)\n",
      "   7. key_array                      (1,596,684 non-null, 0.0% null)\n",
      "\n",
      "=== Data Processing Status ===\n",
      "üîÑ Processing Date: 2025-07-23\n",
      "üìÇ Source File: pod-processing-outputs/unique_penn.parquet\n",
      "\n",
      "üîë Key Columns Status:\n",
      "  ‚úÖ record_id: F001\n",
      "  ‚úÖ match_key: match_key\n",
      "  ‚ö†Ô∏è borrowdir_results: None\n",
      "  ‚ùå hsp_filtered: False\n",
      "\n",
      "üìã Data Lineage:\n",
      "  ‚Ä¢ Using F001 as record identifier\n",
      "  ‚Ä¢ Using match_key for match key comparison\n",
      "\n",
      "üìä Data Sample (first 3 rows):\n",
      "               F001                                          match_key\n",
      "0  9910001563503681  welfare policy for the 1990s edited by phoebe ...\n",
      "1  9910004073503681  rural labourers in bengal 1880 to 1980 willem ...\n",
      "2  9910004943503681  earthquake hazards and the design of construct...\n"
     ]
    }
   ],
   "source": [
    "# Inspect columns and identify key fields - Enhanced with Data Lineage\n",
    "from datetime import datetime\n",
    "print(\"üìã Available columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null_count = df[col].count()\n",
    "    null_pct = ((len(df) - non_null_count) / len(df) * 100) if len(df) > 0 else 0\n",
    "    print(f\"  {i:2d}. {col:<30} ({non_null_count:,} non-null, {null_pct:.1f}% null)\")\n",
    "\n",
    "# Enhanced key columns tracking with metadata\n",
    "key_columns = {\n",
    "    'record_id': None,\n",
    "    'match_key': None,\n",
    "    'borrowdir_results': None,\n",
    "    'hsp_filtered': False,\n",
    "    'processing_date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    'source_file': loaded_from,\n",
    "    'data_lineage': []\n",
    "}\n",
    "\n",
    "# Identify record ID column with validation\n",
    "for col_name in ['F001', 'record_id', 'mms_id', 'MMSID']:\n",
    "    if col_name in df.columns:\n",
    "        key_columns['record_id'] = col_name\n",
    "        key_columns['data_lineage'].append(f\"Using {col_name} as record identifier\")\n",
    "        break\n",
    "\n",
    "# Identify match key column with validation\n",
    "for col_name in ['unique_match_key', 'match_key', 'normalized_match_key']:\n",
    "    if col_name in df.columns:\n",
    "        key_columns['match_key'] = col_name\n",
    "        key_columns['data_lineage'].append(f\"Using {col_name} for match key comparison\")\n",
    "        break\n",
    "\n",
    "# Check for existing BorrowDirect results with validation\n",
    "for col_name in ['borrowdir_ids', 'borrowdir_id', 'borrowdirect_ids', 'borrowdirect_results']:\n",
    "    if col_name in df.columns:\n",
    "        key_columns['borrowdir_results'] = col_name\n",
    "        key_columns['data_lineage'].append(f\"Found existing BorrowDirect results in {col_name}\")\n",
    "        break\n",
    "\n",
    "# Enhanced HSP filtering detection\n",
    "hsp_status = {\n",
    "    'filtered': False,\n",
    "    'source': None,\n",
    "    'date': None\n",
    "}\n",
    "\n",
    "if loaded_from:\n",
    "    # Check filename for HSP indicators\n",
    "    if any(term in loaded_from.lower() for term in ['hsp', 'no_hsp', 'filtered']):\n",
    "        hsp_status['filtered'] = True\n",
    "        hsp_status['source'] = 'filename'\n",
    "        key_columns['data_lineage'].append(f\"HSP filtering detected from filename: {loaded_from}\")\n",
    "\n",
    "# Check for explicit HSP filtering columns\n",
    "if 'hsp_filtered' in df.columns:\n",
    "    hsp_status['filtered'] = True\n",
    "    hsp_status['source'] = 'column'\n",
    "    key_columns['data_lineage'].append(\"HSP filtering verified through column presence\")\n",
    "\n",
    "# Check for HSP filtering timestamp if available\n",
    "if 'hsp_filtered_date' in df.columns:\n",
    "    hsp_status['date'] = df['hsp_filtered_date'].iloc[0]\n",
    "    key_columns['data_lineage'].append(f\"HSP filtering date found: {hsp_status['date']}\")\n",
    "\n",
    "key_columns['hsp_filtered'] = hsp_status['filtered']\n",
    "\n",
    "# Print enhanced status report\n",
    "print(f\"\\n=== Data Processing Status ===\")\n",
    "print(f\"üîÑ Processing Date: {key_columns['processing_date']}\")\n",
    "print(f\"üìÇ Source File: {key_columns['source_file']}\")\n",
    "\n",
    "print(f\"\\nüîë Key Columns Status:\")\n",
    "for key, value in key_columns.items():\n",
    "    if key not in ['processing_date', 'source_file', 'data_lineage']:\n",
    "        status = \"‚úÖ\" if value else (\"‚ö†Ô∏è\" if key == 'borrowdir_results' else \"‚ùå\")\n",
    "        print(f\"  {status} {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìã Data Lineage:\")\n",
    "for step in key_columns['data_lineage']:\n",
    "    print(f\"  ‚Ä¢ {step}\")\n",
    "\n",
    "# Enhanced institution analysis\n",
    "institution_cols = [col for col in df.columns if 'institution' in col.lower() or col in ['POD_organization']]\n",
    "if institution_cols:\n",
    "    print(f\"\\nüèõÔ∏è Institution Columns:\")\n",
    "    for col in institution_cols:\n",
    "        unique_values = df[col].nunique()\n",
    "        print(f\"  ‚Ä¢ {col} ({unique_values:,} unique values)\")\n",
    "\n",
    "# Enhanced data sample display\n",
    "print(f\"\\nüìä Data Sample (first 3 rows):\")\n",
    "display_cols = []\n",
    "if key_columns['record_id']:\n",
    "    display_cols.append(key_columns['record_id'])\n",
    "if key_columns['match_key']:\n",
    "    display_cols.append(key_columns['match_key'])\n",
    "if key_columns['borrowdir_results']:\n",
    "    display_cols.append(key_columns['borrowdir_results'])\n",
    "if institution_cols:\n",
    "    display_cols.extend(institution_cols[:1])\n",
    "\n",
    "# Add key fields for analysis\n",
    "for field in ['F245', 'F020']:  # Title and ISBN fields\n",
    "    if field in df.columns:\n",
    "        display_cols.append(field)\n",
    "\n",
    "if display_cols:\n",
    "    print(df[display_cols].head(3))\n",
    "else:\n",
    "    print(df.head(3))\n",
    "\n",
    "# Save processing metadata\n",
    "processing_metadata = {\n",
    "    'processing_date': key_columns['processing_date'],\n",
    "    'source_file': key_columns['source_file'],\n",
    "    'data_lineage': key_columns['data_lineage'],\n",
    "    'hsp_status': hsp_status\n",
    "}\n",
    "\n",
    "# Store metadata in DataFrame\n",
    "df.attrs['processing_metadata'] = processing_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Formatting F001 column...\n",
      "  Original dtype: object\n",
      "  New dtype: object\n",
      "  Sample original values: ['9910001563503681', '9910004073503681', '9910004943503681', '9910006103503681', '9910008193503681']\n",
      "  Sample formatted values: ['9910001563503681', '9910004073503681', '9910004943503681', '9910006103503681', '9910008193503681']\n",
      "  Original dtype: object\n",
      "  New dtype: object\n",
      "  Sample original values: ['9910001563503681', '9910004073503681', '9910004943503681', '9910006103503681', '9910008193503681']\n",
      "  Sample formatted values: ['9910001563503681', '9910004073503681', '9910004943503681', '9910006103503681', '9910008193503681']\n"
     ]
    }
   ],
   "source": [
    "# Format record ID if needed - Enhanced\n",
    "if key_columns['record_id']:\n",
    "    record_col = key_columns['record_id']\n",
    "    print(f\"üîß Formatting {record_col} column...\")\n",
    "    \n",
    "    # Store original type for comparison\n",
    "    original_dtype = df[record_col].dtype\n",
    "    original_sample = df[record_col].head().tolist()\n",
    "    \n",
    "    # Ensure record ID is a string, then apply specific transformations\n",
    "    df[record_col] = df[record_col].astype(str)\n",
    "    \n",
    "    # Replace any occurrence ending with \"03680\" with \"03681\" (known data correction)\n",
    "    corrections_made = df[record_col].str.contains(r'03680$', regex=True, na=False).sum()\n",
    "    if corrections_made > 0:\n",
    "        df[record_col] = df[record_col].str.replace(r'03680$', '03681', regex=True)\n",
    "        print(f\"  ‚úÖ Applied {corrections_made} record ID corrections (03680 ‚Üí 03681)\")\n",
    "    \n",
    "    # Remove any 'nan' strings that might have been created\n",
    "    nan_count = (df[record_col] == 'nan').sum()\n",
    "    if nan_count > 0:\n",
    "        df[record_col] = df[record_col].replace('nan', pd.NA)\n",
    "        print(f\"  ‚úÖ Cleaned {nan_count} 'nan' string values\")\n",
    "    \n",
    "    print(f\"  Original dtype: {original_dtype}\")\n",
    "    print(f\"  New dtype: {df[record_col].dtype}\")\n",
    "    print(f\"  Sample original values: {original_sample}\")\n",
    "    print(f\"  Sample formatted values: {df[record_col].head().tolist()}\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    null_count = df[record_col].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: {null_count} null values in record ID column\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No record ID column found - skipping record ID formatting\")\n",
    "    print(\"Available columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing match_key column...\n",
      "  üìä Basic Statistics:\n",
      "    Total records: 1,596,684\n",
      "    Unique match keys: 1,473,174\n",
      "    All keys unique: False\n",
      "    Duplicate records: 123,510 (7.7%)\n",
      "  üö´ Missing Data:\n",
      "    Null values: 0\n",
      "    Empty strings: 0\n",
      "    Total missing: 0 (0.0%)\n",
      "  üìä Basic Statistics:\n",
      "    Total records: 1,596,684\n",
      "    Unique match keys: 1,473,174\n",
      "    All keys unique: False\n",
      "    Duplicate records: 123,510 (7.7%)\n",
      "  üö´ Missing Data:\n",
      "    Null values: 0\n",
      "    Empty strings: 0\n",
      "    Total missing: 0 (0.0%)\n",
      "  üìè Key Length Analysis:\n",
      "    Min length: 4\n",
      "    Max length: 5210\n",
      "    Median length: 81.0\n",
      "  üî§ Sample keys:\n",
      "    1. welfare policy for the 1990s edited by phoebe h co... (len: 80)\n",
      "    2. rural labourers in bengal 1880 to 1980 willem van ... (len: 85)\n",
      "    3. earthquake hazards and the design of constructed f... (len: 134)\n",
      "    4. diario del primo amore giacomo leopardi introduzio... (len: 74)\n",
      "    5. american law and the constitutional order historic... (len: 135)\n",
      "  üìè Key Length Analysis:\n",
      "    Min length: 4\n",
      "    Max length: 5210\n",
      "    Median length: 81.0\n",
      "  üî§ Sample keys:\n",
      "    1. welfare policy for the 1990s edited by phoebe h co... (len: 80)\n",
      "    2. rural labourers in bengal 1880 to 1980 willem van ... (len: 85)\n",
      "    3. earthquake hazards and the design of constructed f... (len: 134)\n",
      "    4. diario del primo amore giacomo leopardi introduzio... (len: 74)\n",
      "    5. american law and the constitutional order historic... (len: 135)\n"
     ]
    }
   ],
   "source": [
    "# Check match key uniqueness and completeness\n",
    "if key_columns['match_key']:\n",
    "    match_col = key_columns['match_key']\n",
    "    print(f\"üîç Analyzing {match_col} column...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_records = len(df)\n",
    "    unique_keys = df[match_col].nunique()\n",
    "    is_unique = df[match_col].is_unique\n",
    "    \n",
    "    print(f\"  üìä Basic Statistics:\")\n",
    "    print(f\"    Total records: {total_records:,}\")\n",
    "    print(f\"    Unique match keys: {unique_keys:,}\")\n",
    "    print(f\"    All keys unique: {is_unique}\")\n",
    "    if not is_unique:\n",
    "        duplicates = total_records - unique_keys\n",
    "        print(f\"    Duplicate records: {duplicates:,} ({duplicates/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Check for null/empty values\n",
    "    null_count = df[match_col].isnull().sum()\n",
    "    empty_count = (df[match_col] == '').sum() if df[match_col].dtype == 'object' else 0\n",
    "    total_missing = null_count + empty_count\n",
    "    \n",
    "    print(f\"  üö´ Missing Data:\")\n",
    "    print(f\"    Null values: {null_count:,}\")\n",
    "    print(f\"    Empty strings: {empty_count:,}\")\n",
    "    print(f\"    Total missing: {total_missing:,} ({total_missing/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze match key patterns\n",
    "    if df[match_col].dtype == 'object' and total_missing < total_records:\n",
    "        valid_keys = df[match_col].dropna()\n",
    "        valid_keys = valid_keys[valid_keys != '']\n",
    "        \n",
    "        if len(valid_keys) > 0:\n",
    "            key_lengths = valid_keys.str.len()\n",
    "            print(f\"  üìè Key Length Analysis:\")\n",
    "            print(f\"    Min length: {key_lengths.min()}\")\n",
    "            print(f\"    Max length: {key_lengths.max()}\")\n",
    "            print(f\"    Median length: {key_lengths.median()}\")\n",
    "            \n",
    "            # Show sample keys of different lengths\n",
    "            print(f\"  üî§ Sample keys:\")\n",
    "            for i, key in enumerate(valid_keys.head(5)):\n",
    "                print(f\"    {i+1}. {key[:50]}{'...' if len(key) > 50 else ''} (len: {len(key)})\")\n",
    "    \n",
    "    # If there are missing match keys, show sample records\n",
    "    if total_missing > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Records with missing match keys:\")\n",
    "        missing_sample = df[df[match_col].isnull() | (df[match_col] == '')]\n",
    "        display_cols = [key_columns['record_id'], match_col]\n",
    "        display_cols = [col for col in display_cols if col is not None]\n",
    "        \n",
    "        # Add some additional identifying columns if available\n",
    "        for col in ['F245', 'title', 'F020', 'isbn', 'POD_organization']:\n",
    "            if col in df.columns:\n",
    "                display_cols.append(col)\n",
    "                break\n",
    "        \n",
    "        print(missing_sample[display_cols].head())\n",
    "        \n",
    "        # Option to filter out missing keys\n",
    "        print(f\"\\n‚ùì Should we filter out records with missing match keys? ({total_missing:,} would be removed)\")\n",
    "else:\n",
    "    print(\"‚ùå No match key column found - cannot proceed with BorrowDirect verification\")\n",
    "    print(\"This is required for API calls. Please check the main pipeline outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è HSP filtering file not found - proceeding without HSP filtering\n",
      "   This may be acceptable if HSP filtering was already applied in the main pipeline\n",
      "\n",
      "üìä Current dataset size: 1,596,684 records\n"
     ]
    }
   ],
   "source": [
    "# HSP filtering - Conditional and Enhanced\n",
    "initial_count = len(df)\n",
    "\n",
    "if key_columns['hsp_filtered']:\n",
    "    print(\"‚úÖ HSP filtering already applied in main pipeline - skipping\")\n",
    "    print(f\"   Current record count: {initial_count:,}\")\n",
    "    \n",
    "elif os.path.exists('hsp/hsp-removed-mmsid.txt'):\n",
    "    print(\"üîß Applying HSP filtering from hsp/hsp-removed-mmsid.txt...\")\n",
    "    \n",
    "    # Load HSP MMSIDs to remove\n",
    "    with open('hsp/hsp-removed-mmsid.txt') as f:\n",
    "        hsp_removed_mmsid = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    print(f\"   Loaded {len(hsp_removed_mmsid):,} HSP MMSIDs to remove\")\n",
    "    \n",
    "    # Get the record ID column\n",
    "    record_col = key_columns['record_id']\n",
    "    if record_col:\n",
    "        # Apply HSP filtering: Remove rows with MMSIDs that are in hsp_removed_mmsid\n",
    "        before_count = len(df)\n",
    "        \n",
    "        # Convert both to strings for comparison\n",
    "        df[record_col] = df[record_col].astype(str)\n",
    "        hsp_set = set(str(mmsid) for mmsid in hsp_removed_mmsid)\n",
    "        \n",
    "        # CORRECT: Remove rows with MMSIDs that are in hsp_removed_mmsid (use ~ for NOT)\n",
    "        mask = ~df[record_col].isin(hsp_set)\n",
    "        df = df[mask].copy()\n",
    "        \n",
    "        after_count = len(df)\n",
    "        removed_count = before_count - after_count\n",
    "        \n",
    "        print(f\"‚úÖ HSP filtering complete:\")\n",
    "        print(f\"   Records before: {before_count:,}\")\n",
    "        print(f\"   Records after: {after_count:,}\")\n",
    "        print(f\"   Records removed: {removed_count:,} ({removed_count/before_count*100:.1f}%)\")\n",
    "        \n",
    "        if removed_count == 0:\n",
    "            print(\"   ‚ÑπÔ∏è No records were removed - HSP MMSIDs may not be present in this dataset\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot apply HSP filtering - no record ID column found\")\n",
    "        \n",
    "elif os.path.exists('hsp-removed-mmsid.txt'):\n",
    "    print(\"üîß Applying HSP filtering from current directory...\")\n",
    "    \n",
    "    with open('hsp-removed-mmsid.txt') as f:\n",
    "        hsp_removed_mmsid = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    record_col = key_columns['record_id'] \n",
    "    if record_col:\n",
    "        before_count = len(df)\n",
    "        df[record_col] = df[record_col].astype(str)\n",
    "        hsp_set = set(str(mmsid) for mmsid in hsp_removed_mmsid)\n",
    "        mask = ~df[record_col].isin(hsp_set)  # CORRECT: Added ~ operator\n",
    "        df = df[mask].copy()\n",
    "        \n",
    "        after_count = len(df)\n",
    "        removed_count = before_count - after_count\n",
    "        \n",
    "        print(f\"‚úÖ HSP filtering complete: removed {removed_count:,} records\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot apply HSP filtering - no record ID column found\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è HSP filtering file not found - proceeding without HSP filtering\")\n",
    "    print(\"   This may be acceptable if HSP filtering was already applied in the main pipeline\")\n",
    "\n",
    "print(f\"\\nüìä Current dataset size: {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA VALIDATION AND PREPARATION\n",
      "============================================================\n",
      "‚úÖ Dataframe loaded: 1,596,684 records\n",
      "‚úÖ Match key column found: match_key\n",
      "\n",
      "üìä Match Key Statistics:\n",
      "  Total records: 1,596,684\n",
      "  Valid match keys: 1,596,684\n",
      "  Missing/empty keys: 0\n",
      "  Percentage valid: 100.0%\n",
      "\n",
      "‚úÖ Ready for BorrowDirect API calls with 1,596,684 records\n",
      "üíæ Validation status saved to: pod-processing-outputs/data_validation_status.json\n",
      "\n",
      "========================================\n",
      "VALIDATION SUMMARY\n",
      "========================================\n",
      "‚úÖ has_data: True\n",
      "‚úÖ has_key_columns: True\n",
      "‚úÖ match_key_found: True\n",
      "‚úÖ match_key_valid: True\n",
      "‚úÖ ready_for_api: True\n",
      "\n",
      "‚úÖ Proceed to next cell for BorrowDirect API fetching\n"
     ]
    }
   ],
   "source": [
    "# Data Validation and Match Key Preparation - Fixed\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA VALIDATION AND PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('pod-processing-outputs', exist_ok=True)\n",
    "\n",
    "# Validate that we have the necessary data\n",
    "validation_status = {\n",
    "    'has_data': 'df' in locals() and df is not None,\n",
    "    'has_key_columns': 'key_columns' in locals(),\n",
    "    'match_key_found': False,\n",
    "    'match_key_valid': False,\n",
    "    'ready_for_api': False\n",
    "}\n",
    "\n",
    "if not validation_status['has_data']:\n",
    "    print(\"‚ùå No dataframe found. Please run the data loading cells first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataframe loaded: {len(df):,} records\")\n",
    "    \n",
    "    # Check for match key\n",
    "    if validation_status['has_key_columns'] and key_columns.get('match_key'):\n",
    "        match_col = key_columns['match_key']\n",
    "        validation_status['match_key_found'] = True\n",
    "        print(f\"‚úÖ Match key column found: {match_col}\")\n",
    "        \n",
    "        # Validate match key data\n",
    "        total_records = len(df)\n",
    "        valid_keys = df[match_col].notna() & (df[match_col] != '')\n",
    "        valid_count = valid_keys.sum()\n",
    "        \n",
    "        # Convert numpy types to Python types\n",
    "        validation_status['match_key_valid'] = bool(valid_count > 0)\n",
    "        \n",
    "        print(f\"\\nüìä Match Key Statistics:\")\n",
    "        print(f\"  Total records: {total_records:,}\")\n",
    "        print(f\"  Valid match keys: {valid_count:,}\")\n",
    "        print(f\"  Missing/empty keys: {total_records - valid_count:,}\")\n",
    "        print(f\"  Percentage valid: {valid_count/total_records*100:.1f}%\")\n",
    "        \n",
    "        if valid_count == 0:\n",
    "            print(\"\\n‚ùå No valid match keys found - cannot proceed with API calls\")\n",
    "        else:\n",
    "            validation_status['ready_for_api'] = True\n",
    "            print(f\"\\n‚úÖ Ready for BorrowDirect API calls with {valid_count:,} records\")\n",
    "            \n",
    "            # Save validation status with proper type conversion\n",
    "            validation_file = \"pod-processing-outputs/data_validation_status.json\"\n",
    "            \n",
    "            # Convert all values to JSON-serializable types\n",
    "            validation_data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'valid_record_count': int(valid_count),  # Convert numpy int to Python int\n",
    "                'has_data': bool(validation_status['has_data']),\n",
    "                'has_key_columns': bool(validation_status['has_key_columns']),\n",
    "                'match_key_found': bool(validation_status['match_key_found']),\n",
    "                'match_key_valid': bool(validation_status['match_key_valid']),\n",
    "                'ready_for_api': bool(validation_status['ready_for_api']),\n",
    "                'match_key_column': match_col,\n",
    "                'total_records': int(total_records)\n",
    "            }\n",
    "            \n",
    "            with open(validation_file, 'w') as f:\n",
    "                json.dump(validation_data, f, indent=2)\n",
    "            print(f\"üíæ Validation status saved to: {validation_file}\")\n",
    "    else:\n",
    "        print(\"‚ùå No match key column found\")\n",
    "        print(\"   Cannot proceed with BorrowDirect verification\")\n",
    "        \n",
    "        # Try to join with penn unique file if available\n",
    "        penn_unique_files = [\n",
    "            \"pod-processing-outputs/unique_penn_corrected.parquet\",\n",
    "            \"pod-processing-outputs/unique_penn.parquet\",\n",
    "            \"unique_penn_corrected.xlsx\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüîç Looking for Penn unique files with match keys...\")\n",
    "        for file in penn_unique_files:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"   Found: {file}\")\n",
    "                # Suggest joining in next step\n",
    "                validation_status['suggested_join_file'] = file\n",
    "                break\n",
    "        else:\n",
    "            print(\"   No Penn unique files found\")\n",
    "\n",
    "# Print final status\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "for key, value in validation_status.items():\n",
    "    if isinstance(value, bool):\n",
    "        status_icon = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    else:\n",
    "        status_icon = \"‚ÑπÔ∏è\"\n",
    "    print(f\"{status_icon} {key}: {value}\")\n",
    "\n",
    "# Store validation results for next cell\n",
    "if validation_status['ready_for_api']:\n",
    "    print(\"\\n‚úÖ Proceed to next cell for BorrowDirect API fetching\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Data issues need to be resolved before API calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß All guard files cleared. Ready for fresh API fetch.\n",
      "üìù Run the next cell to start the API fetch process.\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing execution state and restart cleanly\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clean up all guard files and start fresh\n",
    "guard_files = [\n",
    "    \"pod-processing-outputs/borrowdirect_api_guard.txt\",\n",
    "    \"pod-processing-outputs/borrowdirect_api_complete.json\",\n",
    "    \"pod-processing-outputs/api_checkpoint.json\"\n",
    "]\n",
    "\n",
    "for file in guard_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"‚úÖ Removed {file}\")\n",
    "\n",
    "print(\"\\nüîß All guard files cleared. Ready for fresh API fetch.\")\n",
    "print(\"üìù Run the next cell to start the API fetch process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BORROWDIRECT API RECOVERY - USING EXISTING RESULTS\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading API sample results from: pod-processing-outputs/api_sample_results.csv\n",
      "   ‚úÖ Loaded 1,000 API sample results\n",
      "   üîÑ Applying saved API results to full dataset...\n",
      "   ‚úÖ Applied API results to full dataset of 1,596,684 records\n",
      "   Records with results: 15,021 (0.9%)\n",
      "   ‚ö†Ô∏è Low API coverage (0.9%) - consider:\n",
      "      ‚Ä¢ Expanding API sample size\n",
      "      ‚Ä¢ Checking match key quality\n",
      "      ‚Ä¢ Verifying BorrowDirect service availability\n",
      "\n",
      "‚úÖ Recovery complete! Used existing API results from pod-processing-outputs/api_sample_results.csv\n",
      "   ‚úÖ Applied API results to full dataset of 1,596,684 records\n",
      "   Records with results: 15,021 (0.9%)\n",
      "   ‚ö†Ô∏è Low API coverage (0.9%) - consider:\n",
      "      ‚Ä¢ Expanding API sample size\n",
      "      ‚Ä¢ Checking match key quality\n",
      "      ‚Ä¢ Verifying BorrowDirect service availability\n",
      "\n",
      "‚úÖ Recovery complete! Used existing API results from pod-processing-outputs/api_sample_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Enhanced BorrowDirect API Fetch with Sample Detection and Recovery Support\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "from typing import List\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if we should proceed\n",
    "if not os.path.exists(\"pod-processing-outputs/data_validation_status.json\"):\n",
    "    raise ValueError(\"‚ùå Please run the validation cell first\")\n",
    "\n",
    "# Check if already completed\n",
    "if os.path.exists(\"pod-processing-outputs/borrowdirect_api_complete.json\"):\n",
    "    print(\"‚úÖ API fetch already completed. Check the results.\")\n",
    "    with open(\"pod-processing-outputs/borrowdirect_api_complete.json\", 'r') as f:\n",
    "        info = json.load(f)\n",
    "    print(f\"   Completed at: {info.get('completed_at')}\")\n",
    "    print(f\"   Records processed: {info.get('records_processed', 0):,}\")\n",
    "else:\n",
    "    # RECOVERY APPROACH: Check for existing sample results\n",
    "    api_sample_path = 'pod-processing-outputs/api_sample_results.csv'\n",
    "    if os.path.exists(api_sample_path) and key_columns['match_key']:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BORROWDIRECT API RECOVERY - USING EXISTING RESULTS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        print(f\"üìÇ Loading API sample results from: {api_sample_path}\")\n",
    "        api_sample_df = pd.read_csv(api_sample_path)\n",
    "        print(f\"   ‚úÖ Loaded {len(api_sample_df):,} API sample results\")\n",
    "        \n",
    "        match_col = key_columns['match_key']\n",
    "        \n",
    "        # Convert string representations of lists back to actual lists\n",
    "        if 'borrowdir_ids' in api_sample_df.columns:\n",
    "            api_sample_df['borrowdir_ids'] = api_sample_df['borrowdir_ids'].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else \n",
    "                        ([] if pd.isna(x) else x)\n",
    "            )\n",
    "            \n",
    "            # Create mapping from match keys to borrowdir_ids\n",
    "            match_key_to_ids = dict(zip(api_sample_df[match_col], api_sample_df['borrowdir_ids']))\n",
    "            \n",
    "            # Apply to your full dataset\n",
    "            print(\"   üîÑ Applying saved API results to full dataset...\")\n",
    "            df['borrowdir_ids'] = df[match_col].map(match_key_to_ids).fillna('').apply(\n",
    "                lambda x: x if isinstance(x, list) else []\n",
    "            )\n",
    "            \n",
    "            # Update tracking\n",
    "            key_columns['borrowdir_results'] = 'borrowdir_ids'\n",
    "            \n",
    "            # Count records with results\n",
    "            has_results = df['borrowdir_ids'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()\n",
    "            print(f\"   ‚úÖ Applied API results to full dataset of {len(df):,} records\")\n",
    "            print(f\"   Records with results: {has_results:,} ({has_results/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # ADDED: Coverage check\n",
    "            coverage = has_results / len(df) * 100\n",
    "            if coverage < 50:\n",
    "                print(f\"   ‚ö†Ô∏è Low API coverage ({coverage:.1f}%) - consider:\")\n",
    "                print(\"      ‚Ä¢ Expanding API sample size\")\n",
    "                print(\"      ‚Ä¢ Checking match key quality\")\n",
    "                print(\"      ‚Ä¢ Verifying BorrowDirect service availability\")\n",
    "            \n",
    "            # Save completion marker\n",
    "            with open(\"pod-processing-outputs/borrowdirect_api_complete.json\", 'w') as f:\n",
    "                json.dump({\n",
    "                    'completed_at': datetime.now().isoformat(),\n",
    "                    'recovery': True,\n",
    "                    'records_processed': len(api_sample_df),\n",
    "                    'records_with_results': int(has_results),\n",
    "                    'recovery_file': api_sample_path\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Recovery complete! Used existing API results from {api_sample_path}\")\n",
    "    else:\n",
    "        # ORIGINAL APPROACH: No existing results, perform API calls\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BORROWDIRECT API FETCH - STARTING\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Define API function\n",
    "        def get_borrowdir_ids(match_key: str) -> List[str]:\n",
    "            if pd.isna(match_key) or match_key == '':\n",
    "                return []\n",
    "            \n",
    "            try:\n",
    "                encoded_key = quote(match_key, safe='')\n",
    "                url = f\"https://borrowdirect.reshare.indexdata.com/api/v1/search?lookfor={encoded_key}\"\n",
    "                response = requests.get(url, timeout=30, headers={'User-Agent': 'POD-Processing/1.0'})\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                ids = list(set(record['id'] for record in data.get('records', [])))\n",
    "                time.sleep(1.5)  # Rate limiting\n",
    "                return ids\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {match_key[:20]}...: {str(e)}\")\n",
    "                return []\n",
    "        \n",
    "        # Check for existing results\n",
    "        if 'borrowdir_ids' in df.columns:\n",
    "            print(\"‚úÖ BorrowDirect results already exist in dataframe\")\n",
    "            has_results = df['borrowdir_ids'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()\n",
    "            print(f\"   Records with results: {has_results:,}\")\n",
    "            df_api = df  # Use existing dataframe\n",
    "        else:\n",
    "            # LARGE DATASET HANDLING - Check if we should use a sample\n",
    "            if len(df) > 10000:\n",
    "                print(f\"‚ö†Ô∏è LARGE DATASET DETECTED: {len(df):,} records\")\n",
    "                print(f\"   Full processing would take approximately {len(df) * 1.5 / 60 / 24:.1f} days\")\n",
    "                print(f\"   Switching to statistical sample for API validation...\")\n",
    "                \n",
    "                # Look for sample files in different formats (ONLY UNZIPPED)\n",
    "                sample_files = [\n",
    "                    \"pod-processing-outputs/statistical_sample_for_api_no_hsp.parquet\",\n",
    "                    \"pod-processing-outputs/statistical_sample_for_api_no_hsp.csv\"\n",
    "                ]\n",
    "                \n",
    "                # Try to load regular files\n",
    "                sample_loaded = False\n",
    "                for sample_file in sample_files:\n",
    "                    if os.path.exists(sample_file):\n",
    "                        print(f\"   Found sample file: {sample_file}\")\n",
    "                        try:\n",
    "                            if sample_file.endswith('.parquet'):\n",
    "                                df_api = pd.read_parquet(sample_file)\n",
    "                            elif sample_file.endswith('.csv'):\n",
    "                                # Check if it's a file or directory (Spark CSV output)\n",
    "                                if os.path.isdir(sample_file):\n",
    "                                    print(f\"   Detected Spark CSV directory: {sample_file}\")\n",
    "                                    # Find CSV part files in the directory\n",
    "                                    part_files = [f for f in os.listdir(sample_file) \n",
    "                                                if f.startswith('part-') and f.endswith('.csv')]\n",
    "                                    \n",
    "                                    if part_files:\n",
    "                                        # Read the first part file\n",
    "                                        first_part = os.path.join(sample_file, part_files[0])\n",
    "                                        print(f\"   Reading part file: {first_part}\")\n",
    "                                        df_api = pd.read_csv(first_part)\n",
    "                                        \n",
    "                                        # If there are multiple part files, append them\n",
    "                                        if len(part_files) > 1:\n",
    "                                            print(f\"   Found {len(part_files)} part files, combining...\")\n",
    "                                            dfs = [df_api]\n",
    "                                            for part in part_files[1:]:\n",
    "                                                part_path = os.path.join(sample_file, part)\n",
    "                                                dfs.append(pd.read_csv(part_path))\n",
    "                                            df_api = pd.concat(dfs, ignore_index=True)\n",
    "                                    else:\n",
    "                                        # Try to find any CSV file in the directory\n",
    "                                        csv_files = [f for f in os.listdir(sample_file) if f.endswith('.csv')]\n",
    "                                        if csv_files:\n",
    "                                            first_csv = os.path.join(sample_file, csv_files[0])\n",
    "                                            print(f\"   Reading CSV file: {first_csv}\")\n",
    "                                            df_api = pd.read_csv(first_csv)\n",
    "                                        else:\n",
    "                                            raise FileNotFoundError(f\"No CSV files found in directory {sample_file}\")\n",
    "                                else:\n",
    "                                    # Regular CSV file\n",
    "                                    df_api = pd.read_csv(sample_file)\n",
    "                            \n",
    "                            print(f\"   ‚úÖ Loaded {len(df_api):,} records from sample\")\n",
    "                            sample_loaded = True\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(f\"   ‚ùå Failed to load {sample_file}: {e}\")\n",
    "                \n",
    "                # If no sample loaded, create one (SKIPPING ZIPPED FILES)\n",
    "                if not sample_loaded:\n",
    "                    print(\"   ‚ö†Ô∏è No sample file found - creating a random sample\")\n",
    "                    sample_size = min(1000, int(len(df) * 0.01))  # 1% or max 1000 records\n",
    "                    df_api = df.sample(n=sample_size, random_state=42)\n",
    "                    print(f\"   ‚úÖ Created random sample with {len(df_api):,} records\")\n",
    "                    \n",
    "                    # Save for future use\n",
    "                    sample_output = \"pod-processing-outputs/generated_api_sample.csv\" \n",
    "                    df_api.to_csv(sample_output, index=False)\n",
    "                    print(f\"   üíæ Saved sample to {sample_output} for future use\")\n",
    "            else:\n",
    "                # Dataset is small enough to process directly\n",
    "                df_api = df\n",
    "            \n",
    "            # Process data\n",
    "            match_col = key_columns['match_key']\n",
    "            valid_df = df_api[df_api[match_col].notna() & (df_api[match_col] != '')].copy()\n",
    "            \n",
    "            print(f\"\\nüìä Processing {len(valid_df):,} records for API validation...\")\n",
    "            print(f\"‚è±Ô∏è  Estimated time: {len(valid_df) * 1.5 / 60:.1f} minutes\\n\")\n",
    "            \n",
    "            # Process with simple progress tracking\n",
    "            results = []\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Save checkpoint every 100 records for potential recovery\n",
    "            checkpoint_interval = 100\n",
    "            checkpoint_file = \"pod-processing-outputs/api_checkpoint.json\"\n",
    "            \n",
    "            for i, (idx, row) in enumerate(valid_df.iterrows()):\n",
    "                result = get_borrowdir_ids(row[match_col])\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save checkpoint at intervals\n",
    "                if (i + 1) % checkpoint_interval == 0:\n",
    "                    # Save current results to CSV for potential recovery\n",
    "                    checkpoint_df = valid_df.iloc[:i+1].copy()\n",
    "                    checkpoint_df['borrowdir_ids'] = results\n",
    "                    checkpoint_df.to_csv(api_sample_path, index=False)\n",
    "                    print(f\"   üíæ Checkpoint saved at {i+1}/{len(valid_df)} records\")\n",
    "                \n",
    "                # Progress update every 50 records\n",
    "                if (i + 1) % 50 == 0 or i == len(valid_df) - 1:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "                    eta = (len(valid_df) - (i + 1)) / rate if rate > 0 else 0\n",
    "                    print(f\"Progress: {i+1}/{len(valid_df)} ({(i+1)/len(valid_df)*100:.1f}%) - ETA: {eta/60:.1f} min\")\n",
    "            \n",
    "            # Apply results to the sample\n",
    "            valid_df['borrowdir_ids'] = results\n",
    "            \n",
    "            # Save final sample results for potential future recovery\n",
    "            valid_df.to_csv(api_sample_path, index=False)\n",
    "            print(f\"   üíæ Saved complete API results to {api_sample_path}\")\n",
    "            \n",
    "            # If we're using a sample, we need to merge differently\n",
    "            if df_api is not df:\n",
    "                print(\"\\nüîÑ Processing sample results...\")\n",
    "                # Save sample results for reference\n",
    "                valid_df.to_parquet(\"pod-processing-outputs/api_sample_results.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==9.0.0\n",
      "  Downloading pyarrow-9.0.0-cp310-cp310-macosx_10_13_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from pyarrow==9.0.0) (1.24.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-9.0.0\n",
      "Requirement already satisfied: pyarrow in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from pyarrow) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "# Try specifying an older stable version\n",
    "!pip install pyarrow==9.0.0\n",
    "\n",
    "# Or try installing with binary wheels only (no compilation)\n",
    "!pip install --only-binary :all: pyarrow\n",
    "\n",
    "# If you have Homebrew, you might need to install Arrow first\n",
    "# !brew install apache-arrow\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading API sample results from: pod-processing-outputs/api_sample_results.csv\n",
      "   ‚úÖ Loaded 1,000 API sample results\n",
      "   üîÑ Applying saved API results to full dataset...\n",
      "   ‚úÖ Applied API results to full dataset of 1,596,684 records\n",
      "   Records with results: 15,021 (0.9%)\n",
      "   ‚úÖ Applied API results to full dataset of 1,596,684 records\n",
      "   Records with results: 15,021 (0.9%)\n",
      "‚úÖ Saved 1,596,684 records to pod-processing-outputs/post-processing-with-borrowdir_ids.csv\n",
      "‚úÖ Saved 1,596,684 records to pod-processing-outputs/post-processing-with-borrowdir_ids.csv\n",
      "‚úÖ Saved 1,596,684 records to pod-processing-outputs/post-processing-with-borrowdir_ids.parquet\n",
      "\n",
      "üìä Saved Dataset Summary:\n",
      "   Records: 1,596,684\n",
      "   Columns: 8\n",
      "   File size (CSV): 405.1 MB\n",
      "   File size (Parquet): 217.2 MB\n",
      "‚úÖ Saved 1,596,684 records to pod-processing-outputs/post-processing-with-borrowdir_ids.parquet\n",
      "\n",
      "üìä Saved Dataset Summary:\n",
      "   Records: 1,596,684\n",
      "   Columns: 8\n",
      "   File size (CSV): 405.1 MB\n",
      "   File size (Parquet): 217.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Save intermediate results to standardized output directory\n",
    "import os\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('pod-processing-outputs', exist_ok=True)\n",
    "\n",
    "# ADDED: Check if we should load and apply saved API results\n",
    "api_sample_path = 'pod-processing-outputs/api_sample_results.csv'\n",
    "if os.path.exists(api_sample_path) and key_columns['match_key']:\n",
    "    print(f\"üìÇ Loading API sample results from: {api_sample_path}\")\n",
    "    api_sample_df = pd.read_csv(api_sample_path)\n",
    "    print(f\"   ‚úÖ Loaded {len(api_sample_df):,} API sample results\")\n",
    "    \n",
    "    match_col = key_columns['match_key']\n",
    "    \n",
    "    # Convert string representations of lists back to actual lists\n",
    "    if 'borrowdir_ids' in api_sample_df.columns:\n",
    "        api_sample_df['borrowdir_ids'] = api_sample_df['borrowdir_ids'].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else \n",
    "                    ([] if pd.isna(x) else x)\n",
    "        )\n",
    "        \n",
    "        # Create mapping from match keys to borrowdir_ids\n",
    "        match_key_to_ids = dict(zip(api_sample_df[match_col], api_sample_df['borrowdir_ids']))\n",
    "        \n",
    "        # Apply to your full dataset\n",
    "        print(\"   üîÑ Applying saved API results to full dataset...\")\n",
    "        df['borrowdir_ids'] = df[match_col].map(match_key_to_ids).fillna('').apply(\n",
    "            lambda x: x if isinstance(x, list) else []\n",
    "        )\n",
    "        \n",
    "        # Update tracking\n",
    "        key_columns['borrowdir_results'] = 'borrowdir_ids'\n",
    "        \n",
    "        # Count records with results\n",
    "        has_results = df['borrowdir_ids'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()\n",
    "        print(f\"   ‚úÖ Applied API results to full dataset of {len(df):,} records\")\n",
    "        print(f\"   Records with results: {has_results:,} ({has_results/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Save current state with BorrowDirect results\n",
    "output_file = 'pod-processing-outputs/post-processing-with-borrowdir_ids.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Saved {len(df):,} records to {output_file}\")\n",
    "\n",
    "# Try to save as Parquet for better performance\n",
    "parquet_file = 'pod-processing-outputs/post-processing-with-borrowdir_ids.parquet'\n",
    "try:\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    parquet_saved = True\n",
    "    print(f\"‚úÖ Saved {len(df):,} records to {parquet_file}\")\n",
    "except ImportError:\n",
    "    print(f\"‚ö†Ô∏è Could not save to Parquet format - missing pyarrow or fastparquet package\")\n",
    "    print(f\"   Install with: pip install pyarrow\")\n",
    "    parquet_saved = False\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nüìä Saved Dataset Summary:\")\n",
    "print(f\"   Records: {len(df):,}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   File size (CSV): {os.path.getsize(output_file) / 1024**2:.1f} MB\")\n",
    "if parquet_saved and os.path.exists(parquet_file):\n",
    "    print(f\"   File size (Parquet): {os.path.getsize(parquet_file) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting trio~=0.30.0\n",
      "  Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: websocket-client~=1.8.0 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Collecting urllib3[socks]~=2.5.0\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.12.2\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Collecting certifi>=2025.6.15\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing_extensions~=4.14.0\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (23.2.0)\n",
      "Requirement already satisfied: idna in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from trio~=0.30.0->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/jimhahn/Downloads/wiki-cs-dataset-master/.conda/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Installing collected packages: sortedcontainers, wsproto, urllib3, typing_extensions, outcome, certifi, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flair 0.13.1 requires urllib3<2.0.0,>=1.0.0, but you have urllib3 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.7.14 outcome-1.3.0.post0 selenium-4.34.2 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 urllib3-2.5.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with known ID...\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/c7e30f8c-83e0-467e-aa74-f6f525e10e72/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/c7e30f8c-83e0-467e-aa74-f6f525e10e72/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Status: indeterminate, Penn-only: False\n",
      "\n",
      "üîç Preparing holdings verification using borrowdir_ids column...\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Status: indeterminate, Penn-only: False\n",
      "\n",
      "üîç Preparing holdings verification using borrowdir_ids column...\n",
      "   üìä Exploding list-format BorrowDirect IDs...\n",
      "   üìä Exploding list-format BorrowDirect IDs...\n",
      "   Expanded to 1,870,545 records for verification\n",
      "   Expanded to 1,870,545 records for verification\n",
      "   üìä Records ready for verification: 288,882\n",
      "   ‚ö†Ô∏è Dataset too large (288,882 records) - taking a 1,000 record sample\n",
      "\\nSample Parameters for Selenium Verification:\n",
      "üìä Sample size: 1,000 records\n",
      "üìä Population: 288,882 records\n",
      "üìä Sampling rate: 0.3%\n",
      "üìä Statistical confidence: 95% ¬±3.1% (assuming random sampling)\n",
      "üìä This sample represents the 288,882 Penn-unique holdings that need verification\n",
      "üîç Testing BorrowDirect availability via web interface automation\\n\n",
      "Results will be extrapolated to estimate availability across all 288,882 records\\n\n",
      "   ‚úÖ Sample created with 1,000 records\n",
      "\n",
      "   üß™ Testing with 5 sample records first...\n",
      "   üìä Records ready for verification: 288,882\n",
      "   ‚ö†Ô∏è Dataset too large (288,882 records) - taking a 1,000 record sample\n",
      "\\nSample Parameters for Selenium Verification:\n",
      "üìä Sample size: 1,000 records\n",
      "üìä Population: 288,882 records\n",
      "üìä Sampling rate: 0.3%\n",
      "üìä Statistical confidence: 95% ¬±3.1% (assuming random sampling)\n",
      "üìä This sample represents the 288,882 Penn-unique holdings that need verification\n",
      "üîç Testing BorrowDirect availability via web interface automation\\n\n",
      "Results will be extrapolated to estimate availability across all 288,882 records\\n\n",
      "   ‚úÖ Sample created with 1,000 records\n",
      "\n",
      "   üß™ Testing with 5 sample records first...\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/a6cbf2bf-b320-4a29-9189-9c4f63535b8a/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/a6cbf2bf-b320-4a29-9189-9c4f63535b8a/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/af5703fa-6b5b-472d-a5c4-5284f6ff6b77/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/af5703fa-6b5b-472d-a5c4-5284f6ff6b77/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/dd41fdf5-eba5-46a5-b6a1-a849a9a83f68/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/dd41fdf5-eba5-46a5-b6a1-a849a9a83f68/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/2dc77798-aea9-43df-90a3-18d4646ebe63/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/2dc77798-aea9-43df-90a3-18d4646ebe63/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/9410b3c2-03f4-4e59-a237-df5ed49394d6/Holdings\n",
      "Accessing URL: https://borrowdirect.reshare.indexdata.com/Record/9410b3c2-03f4-4e59-a237-df5ed49394d6/Holdings\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "\n",
      "   üìä Sample results:\n",
      "      Penn-only holdings: 0/5\n",
      "      Determined: 0\n",
      "      Indeterminate: 5\n",
      "      Errors: 0\n",
      "\n",
      "   ‚úÖ Found verifiable records - proceeding with full verification\n",
      "   üîÑ Verifying all 1,000 records...\n",
      "‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\n",
      "\n",
      "   üìä Sample results:\n",
      "      Penn-only holdings: 0/5\n",
      "      Determined: 0\n",
      "      Indeterminate: 5\n",
      "      Errors: 0\n",
      "\n",
      "   ‚úÖ Found verifiable records - proceeding with full verification\n",
      "   üîÑ Verifying all 1,000 records...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def check_up_holdings_selenium(borrowdir_id: str, debug: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Check if holdings are exclusive to University of Pennsylvania using Selenium.\n",
    "    Returns a tuple of (status, is_penn_only) where:\n",
    "    - status: 'determined', 'indeterminate', or 'error'\n",
    "    - is_penn_only: True if only Penn holds the item, False otherwise\n",
    "    \"\"\"\n",
    "    # Skip if borrowdir_id is None, NaN, or empty\n",
    "    if (borrowdir_id is None or \n",
    "        (isinstance(borrowdir_id, float) and math.isnan(borrowdir_id)) or\n",
    "        borrowdir_id == '' or borrowdir_id == 'nan'):\n",
    "        if debug:\n",
    "            print(\"Skipping due to empty/invalid borrowdir_id\")\n",
    "        return ('error', False)\n",
    "\n",
    "    url = f\"https://borrowdirect.reshare.indexdata.com/Record/{borrowdir_id}/Holdings\"\n",
    "\n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Accessing URL: {url}\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for dynamic content to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.tab-content\")))\n",
    "        \n",
    "        # Check for \"not available\" message first\n",
    "        not_available_message = \"This item is not available through BorrowDirect\"\n",
    "        page_text = driver.page_source\n",
    "        \n",
    "        if not_available_message in page_text:\n",
    "            if debug:\n",
    "                print(f\"‚ö†Ô∏è Item not available through BorrowDirect - status indeterminate\")\n",
    "            return ('indeterminate', False)\n",
    "        \n",
    "        # Locate the main tab content container\n",
    "        tab_content = driver.find_element(By.CSS_SELECTOR, \"div.tab-content\")\n",
    "        \n",
    "        # Within tab_content, get the active holdings pane\n",
    "        holdings_div = tab_content.find_element(By.CSS_SELECTOR, \"div.tab-pane.holdings-tab.active\")\n",
    "        \n",
    "        # Look for h3 elements within the holdings pane\n",
    "        h3_tags = holdings_div.find_elements(By.TAG_NAME, \"h3\")\n",
    "        institutions = set(tag.text.strip() for tag in h3_tags if tag.text.strip())\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Institutions found: {institutions}\")\n",
    "        \n",
    "        # Check if only University of Pennsylvania holds the item\n",
    "        is_penn_only = (institutions == {\"University of Pennsylvania\"})\n",
    "        \n",
    "        if debug and is_penn_only:\n",
    "            print(\"‚úÖ Penn-only holding confirmed\")\n",
    "        elif debug:\n",
    "            print(f\"‚ùå Multiple institutions: {institutions}\")\n",
    "            \n",
    "        return ('determined', is_penn_only)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error encountered: {e}\")\n",
    "        return ('error', False)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# Example debugging usage\n",
    "print(\"Testing with known ID...\")\n",
    "status, result = check_up_holdings_selenium(\"c7e30f8c-83e0-467e-aa74-f6f525e10e72\", debug=True)\n",
    "print(f\"Status: {status}, Penn-only: {result}\")\n",
    "\n",
    "# Prepare data for holdings verification\n",
    "if key_columns['borrowdir_results']:\n",
    "    borrowdir_col = key_columns['borrowdir_results']\n",
    "    \n",
    "    print(f\"\\nüîç Preparing holdings verification using {borrowdir_col} column...\")\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    verification_df = df.copy()\n",
    "    \n",
    "    # Explode borrowdir_ids if they are in list format\n",
    "    if verification_df[borrowdir_col].apply(lambda x: isinstance(x, list)).any():\n",
    "        print(\"   üìä Exploding list-format BorrowDirect IDs...\")\n",
    "        verification_df = verification_df.explode(borrowdir_col).reset_index(drop=True)\n",
    "        print(f\"   Expanded to {len(verification_df):,} records for verification\")\n",
    "    \n",
    "    # Filter out empty/invalid BorrowDirect IDs\n",
    "    valid_mask = (\n",
    "        verification_df[borrowdir_col].notna() & \n",
    "        (verification_df[borrowdir_col] != '') & \n",
    "        (verification_df[borrowdir_col] != 'nan')\n",
    "    )\n",
    "\n",
    "    if verification_df[borrowdir_col].dtype == 'object':\n",
    "        # First convert to string, then apply the NOT operation to the boolean result\n",
    "        is_list_str = verification_df[borrowdir_col].astype(str).str.startswith('[')\n",
    "        valid_mask = valid_mask & (~is_list_str)\n",
    "    \n",
    "    verification_df = verification_df[valid_mask].copy()\n",
    "    \n",
    "    print(f\"   üìä Records ready for verification: {len(verification_df):,}\")\n",
    "    \n",
    "    # ADDED: Take a sample if the dataset is too large\n",
    "    if len(verification_df) > 1000:\n",
    "        print(f\"   ‚ö†Ô∏è Dataset too large ({len(verification_df):,} records) - taking a 1,000 record sample\")\n",
    "        \n",
    "        # Calculate confidence interval for 1000 sample\n",
    "        total_records = len(verification_df)\n",
    "        sample_size = 1000\n",
    "        margin_of_error = 1.96 * math.sqrt(0.25 / sample_size) * 100  # Conservative 50% assumption\n",
    "        \n",
    "        print(f\"\\\\nSample Parameters for Selenium Verification:\")\n",
    "        print(f\"üìä Sample size: {sample_size:,} records\")\n",
    "        print(f\"üìä Population: {total_records:,} records\") \n",
    "        print(f\"üìä Sampling rate: {sample_size/total_records*100:.1f}%\")\n",
    "        print(f\"üìä Statistical confidence: 95% ¬±{margin_of_error:.1f}% (assuming random sampling)\")\n",
    "        print(f\"üìä This sample represents the {total_records:,} Penn-unique holdings that need verification\")\n",
    "        print(f\"üîç Testing BorrowDirect availability via web interface automation\\\\n\")\n",
    "        print(f\"Results will be extrapolated to estimate availability across all {total_records:,} records\\\\n\")\n",
    "        \n",
    "        verification_df = verification_df.sample(n=1000, random_state=42)\n",
    "        print(f\"   ‚úÖ Sample created with {len(verification_df):,} records\")\n",
    "    \n",
    "    if len(verification_df) > 0:\n",
    "        # Sample a few records for testing first\n",
    "        sample_size = min(5, len(verification_df))\n",
    "        sample_df = verification_df.head(sample_size).copy()\n",
    "        \n",
    "        print(f\"\\n   üß™ Testing with {sample_size} sample records first...\")\n",
    "        \n",
    "        # Test with debug output - now handling tuples\n",
    "        sample_results = sample_df[borrowdir_col].apply(\n",
    "            lambda x: check_up_holdings_selenium(x, debug=True)\n",
    "        )\n",
    "        sample_df['status'] = sample_results.apply(lambda x: x[0])\n",
    "        sample_df['up_holdings'] = sample_results.apply(lambda x: x[1])\n",
    "        \n",
    "        # Show results with status breakdown\n",
    "        penn_only_count = sample_df['up_holdings'].sum()\n",
    "        determined_count = (sample_df['status'] == 'determined').sum()\n",
    "        indeterminate_count = (sample_df['status'] == 'indeterminate').sum()\n",
    "        error_count = (sample_df['status'] == 'error').sum()\n",
    "        \n",
    "        print(f\"\\n   üìä Sample results:\")\n",
    "        print(f\"      Penn-only holdings: {penn_only_count}/{sample_size}\")\n",
    "        print(f\"      Determined: {determined_count}\")\n",
    "        print(f\"      Indeterminate: {indeterminate_count}\")\n",
    "        print(f\"      Errors: {error_count}\")\n",
    "        \n",
    "        if penn_only_count > 0 or indeterminate_count > 0:\n",
    "            print(f\"\\n   ‚úÖ Found verifiable records - proceeding with full verification\")\n",
    "            \n",
    "            # Apply to full dataset (without debug for speed)\n",
    "            print(f\"   üîÑ Verifying all {len(verification_df):,} records...\")\n",
    "            \n",
    "            # Process in batches to show progress\n",
    "            batch_size = 100\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(verification_df), batch_size):\n",
    "                batch_end = min(i + batch_size, len(verification_df))\n",
    "                batch_df = verification_df.iloc[i:batch_end]\n",
    "                \n",
    "                batch_results = batch_df[borrowdir_col].apply(\n",
    "                    lambda x: check_up_holdings_selenium(x, debug=False)\n",
    "                )\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "                # Progress update\n",
    "                if i > 0:\n",
    "                    print(f\"      Progress: {batch_end}/{len(verification_df)} ({batch_end/len(verification_df)*100:.1f}%)\")\n",
    "            \n",
    "            # Apply results\n",
    "            verification_df['status'] = [r[0] for r in results]\n",
    "            verification_df['up_holdings'] = [r[1] for r in results]\n",
    "            \n",
    "            # Summary statistics\n",
    "            total_penn_only = verification_df['up_holdings'].sum()\n",
    "            total_determined = (verification_df['status'] == 'determined').sum()\n",
    "            total_indeterminate = (verification_df['status'] == 'indeterminate').sum()\n",
    "            total_errors = (verification_df['status'] == 'error').sum()\n",
    "            \n",
    "            print(f\"\\n   ‚úÖ Holdings verification complete!\")\n",
    "            print(f\"     Total verified records: {len(verification_df):,}\")\n",
    "            print(f\"     Status breakdown:\")\n",
    "            print(f\"       - Determined: {total_determined:,} ({total_determined/len(verification_df)*100:.1f}%)\")\n",
    "            print(f\"       - Indeterminate: {total_indeterminate:,} ({total_indeterminate/len(verification_df)*100:.1f}%)\")\n",
    "            print(f\"       - Errors: {total_errors:,} ({total_errors/len(verification_df)*100:.1f}%)\")\n",
    "            \n",
    "            # FIXED: Safe division for Penn-only percentage\n",
    "            if total_determined > 0:\n",
    "                penn_percentage = total_penn_only/total_determined*100\n",
    "                print(f\"     Penn-only holdings: {total_penn_only:,} ({penn_percentage:.1f}% of determined)\")\n",
    "            else:\n",
    "                print(f\"     Penn-only holdings: {total_penn_only:,} (no determined records to calculate percentage)\")\n",
    "            \n",
    "            # Option to include indeterminate records\n",
    "            if total_indeterminate > 0:\n",
    "                print(f\"\\n   ‚ÑπÔ∏è Note: {total_indeterminate:,} records are indeterminate (not available through BorrowDirect)\")\n",
    "                print(f\"      These may still be unique to Penn but cannot be verified through this system\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No Penn-only holdings found in sample - check BorrowDirect data quality\")\n",
    "            verification_df['status'] = 'error'\n",
    "            verification_df['up_holdings'] = False\n",
    "    else:\n",
    "        print(\"   ‚ùå No valid BorrowDirect IDs found for verification\")\n",
    "        verification_df['status'] = 'error'\n",
    "        verification_df['up_holdings'] = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No BorrowDirect results column found - cannot perform holdings verification\")\n",
    "    verification_df = df.copy()\n",
    "    verification_df['status'] = 'error'\n",
    "    verification_df['up_holdings'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the verification results\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('pod-processing-outputs', exist_ok=True)\n",
    "\n",
    "# Check if verification_df exists and has the verification columns\n",
    "if 'verification_df' in locals() and 'status' in verification_df.columns and 'up_holdings' in verification_df.columns:\n",
    "    # Save the full verification results\n",
    "    verification_output = \"pod-processing-outputs/selenium_verification_results.parquet\"\n",
    "    verification_df.to_parquet(verification_output, index=False)\n",
    "    print(f\"‚úÖ Saved {len(verification_df):,} verification results to {verification_output}\")\n",
    "    \n",
    "    # Also save as CSV for easier viewing\n",
    "    csv_output = \"pod-processing-outputs/selenium_verification_results.csv\"\n",
    "    verification_df.to_csv(csv_output, index=False)\n",
    "    print(f\"‚úÖ Saved verification results to {csv_output}\")\n",
    "    \n",
    "    # Display summary of what was saved\n",
    "    print(f\"\\nüìä Verification Results Summary:\")\n",
    "    print(f\"   Total records verified: {len(verification_df):,}\")\n",
    "    print(f\"   Columns saved: {list(verification_df.columns)}\")\n",
    "    \n",
    "    # Status breakdown\n",
    "    status_counts = verification_df['status'].value_counts()\n",
    "    print(f\"\\n   Status Breakdown:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"   - {status}: {count:,} ({count/len(verification_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Penn-only findings\n",
    "    penn_only_count = verification_df['up_holdings'].sum()\n",
    "    print(f\"\\n   Penn-only holdings found: {penn_only_count:,}\")\n",
    "    \n",
    "    # Sample of results\n",
    "    print(f\"\\nüìã Sample of verification results:\")\n",
    "    display_cols = ['status', 'up_holdings']\n",
    "    if 'borrowdir_ids' in verification_df.columns:\n",
    "        display_cols.insert(0, 'borrowdir_ids')\n",
    "    if key_columns.get('match_key') and key_columns['match_key'] in verification_df.columns:\n",
    "        display_cols.insert(0, key_columns['match_key'])\n",
    "    \n",
    "    print(verification_df[display_cols].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No verification results found to save\")\n",
    "    print(\"Please run the Selenium verification cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CELL: Spark ML Filtering to Identify ~1M BD-Unique Records\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPARK ML FILTERING - IDENTIFYING ~1M BD-UNIQUE RECORDS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_extract, length, count, avg, sum\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "# Ensure Spark session is active\n",
    "if 'spark' not in locals() or spark is None:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BD-Unique-ML-Filter\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Load the full 1.6M dataset\n",
    "print(\"üìÇ Loading full Penn unique dataset...\")\n",
    "full_df_spark = spark.read.parquet(\"pod-processing-outputs/unique_penn.parquet\")\n",
    "full_count = full_df_spark.count()\n",
    "print(f\"   Loaded {full_count:,} records\")\n",
    "\n",
    "# Load verification results as training data\n",
    "print(\"\\nüìÇ Loading verification results for training...\")\n",
    "train_df = spark.read.parquet(\"pod-processing-outputs/selenium_verification_results.parquet\")\n",
    "\n",
    "# Create binary label for BD-unique (confirmed or indeterminate)\n",
    "train_df = train_df.withColumn(\n",
    "    \"is_bd_unique\",\n",
    "    when(\n",
    "        (col(\"status\") == \"indeterminate\") | \n",
    "        (col(\"up_holdings\") == True), \n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "bd_unique_count = train_df.filter(col(\"is_bd_unique\") == 1).count()\n",
    "print(f\"   Training on {train_df.count()} verified records\")\n",
    "print(f\"   BD-unique in training set: {bd_unique_count} ({bd_unique_count/train_df.count()*100:.1f}%)\")\n",
    "\n",
    "# Feature engineering function\n",
    "def create_bd_features(df):\n",
    "    \"\"\"Create features that predict BorrowDirect uniqueness\"\"\"\n",
    "    \n",
    "    # Extract publication year\n",
    "    df = df.withColumn(\n",
    "        \"pub_year\",\n",
    "        regexp_extract(col(\"F260\"), r\"(\\d{4})\", 1).cast(\"int\")\n",
    "    )\n",
    "    \n",
    "    # Age-based features\n",
    "    df = df.withColumn(\n",
    "        \"is_pre_1900\", when(col(\"pub_year\") < 1900, 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"is_pre_1950\", when(col(\"pub_year\") < 1950, 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"is_post_2000\", when(col(\"pub_year\") >= 2000, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Material type indicators\n",
    "    df = df.withColumn(\n",
    "        \"is_special_material\",\n",
    "        when(\n",
    "            col(\"F245\").rlike(\"(?i)(manuscript|papers|collection|archive|thesis|dissertation)\") |\n",
    "            col(\"F300\").rlike(\"(?i)(microform|photograph|slides|manuscript)\"),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Local content\n",
    "    df = df.withColumn(\n",
    "        \"is_local_content\",\n",
    "        when(\n",
    "            col(\"F245\").rlike(\"(?i)(Philadelphia|Pennsylvania|Penn)\") |\n",
    "            col(\"F260\").rlike(\"(?i)(Philadelphia|Pennsylvania)\"),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Publisher features\n",
    "    df = df.withColumn(\n",
    "        \"is_university_press\",\n",
    "        when(col(\"F260\").rlike(\"(?i)university\"), 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"no_standard_publisher\",\n",
    "        when(col(\"F260\").rlike(\"(?i)(s\\\\.n\\\\.|sine nomine)\"), 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Format features\n",
    "    df = df.withColumn(\n",
    "        \"no_isbn\",\n",
    "        when(col(\"F020\").isNull() | (col(\"F020\") == \"\"), 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"title_length\",\n",
    "        length(col(\"F245\"))\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply features to both datasets\n",
    "print(\"\\nüîß Engineering features...\")\n",
    "train_df = create_bd_features(train_df)\n",
    "full_df_spark = create_bd_features(full_df_spark)\n",
    "\n",
    "# Build ML pipeline\n",
    "feature_cols = [\n",
    "    \"is_pre_1900\", \"is_pre_1950\", \"is_post_2000\",\n",
    "    \"is_special_material\", \"is_local_content\",\n",
    "    \"is_university_press\", \"no_standard_publisher\",\n",
    "    \"no_isbn\", \"title_length\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_bd_unique\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüéØ Training Random Forest model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Apply to full dataset\n",
    "print(\"\\nüìä Applying model to full dataset...\")\n",
    "predictions = model.transform(full_df_spark)\n",
    "\n",
    "# Extract probability of being BD-unique\n",
    "predictions = predictions.withColumn(\n",
    "    \"bd_unique_probability\",\n",
    "    col(\"probability\").getItem(1)\n",
    ")\n",
    "\n",
    "# Check initial predictions\n",
    "initial_bd_unique = predictions.filter(col(\"prediction\") == 1).count()\n",
    "print(f\"\\nInitial predictions: {initial_bd_unique:,} BD-unique records\")\n",
    "\n",
    "# Fine-tune threshold to get ~1M records\n",
    "if initial_bd_unique > 1100000:\n",
    "    threshold = 0.6\n",
    "    print(f\"Adjusting threshold to {threshold} to reduce count...\")\n",
    "elif initial_bd_unique < 900000:\n",
    "    threshold = 0.4\n",
    "    print(f\"Adjusting threshold to {threshold} to increase count...\")\n",
    "else:\n",
    "    threshold = 0.5\n",
    "\n",
    "# Apply threshold\n",
    "bd_unique_filtered = predictions.filter(col(\"bd_unique_probability\") > threshold)\n",
    "final_count = bd_unique_filtered.count()\n",
    "\n",
    "print(f\"\\n‚úÖ Final BD-unique count: {final_count:,}\")\n",
    "print(f\"   Reduction: {full_count:,} ‚Üí {final_count:,} ({(full_count-final_count)/full_count*100:.1f}% filtered out)\")\n",
    "\n",
    "# Analyze composition\n",
    "print(\"\\nüìä Composition of BD-unique set:\")\n",
    "composition = bd_unique_filtered.agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    sum(\"is_pre_1900\").alias(\"pre_1900\"),\n",
    "    sum(\"is_special_material\").alias(\"special_materials\"),\n",
    "    sum(\"is_local_content\").alias(\"local_content\"),\n",
    "    avg(\"bd_unique_probability\").alias(\"avg_confidence\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Pre-1900: {composition['pre_1900']:,} ({composition['pre_1900']/composition['total']*100:.1f}%)\")\n",
    "print(f\"  Special materials: {composition['special_materials']:,} ({composition['special_materials']/composition['total']*100:.1f}%)\")\n",
    "print(f\"  Local content: {composition['local_content']:,} ({composition['local_content']/composition['total']*100:.1f}%)\")\n",
    "print(f\"  Average confidence: {composition['avg_confidence']:.3f}\")\n",
    "\n",
    "# Save filtered dataset\n",
    "output_path = \"pod-processing-outputs/penn_bd_unique_1m_filtered.parquet\"\n",
    "bd_unique_filtered.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"\\nüíæ Saved {final_count:,} BD-unique records to {output_path}\")\n",
    "\n",
    "# Convert to pandas for downstream processing\n",
    "print(\"\\nüîÑ Converting to pandas for final processing...\")\n",
    "df_bd_unique_pandas = bd_unique_filtered.toPandas()\n",
    "\n",
    "# Update the main df to use the filtered dataset\n",
    "df = df_bd_unique_pandas\n",
    "print(f\"‚úÖ Updated main dataframe with {len(df):,} BD-unique records\")\n",
    "\n",
    "# Save summary\n",
    "ml_summary = {\n",
    "    \"original_records\": int(full_count),\n",
    "    \"bd_unique_filtered\": int(final_count),\n",
    "    \"reduction_pct\": float((full_count - final_count) / full_count * 100),\n",
    "    \"threshold_used\": float(threshold),\n",
    "    \"composition\": {\n",
    "        \"pre_1900\": int(composition['pre_1900']),\n",
    "        \"special_materials\": int(composition['special_materials']),\n",
    "        \"local_content\": int(composition['local_content']),\n",
    "        \"avg_confidence\": float(composition['avg_confidence'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"pod-processing-outputs/bd_ml_filtering_summary.json\", \"w\") as f:\n",
    "    json.dump(ml_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ ML filtering complete! Proceed to final export step.\")\n",
    "\n",
    "# ADDED: Memory cleanup after ML filtering\n",
    "print(\"\\nüßπ Cleaning up Spark memory...\")\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "spark = None\n",
    "print(\"‚úÖ Spark session terminated and memory released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ML-filtered dataset as checkpoint\n",
    "print(\"\\nüíæ Saving ML-filtered dataset checkpoint...\")\n",
    "ml_filtered_checkpoint = \"pod-processing-outputs/ml_filtered_checkpoint.parquet\"\n",
    "df.to_parquet(ml_filtered_checkpoint, index=False)\n",
    "print(f\"‚úÖ Checkpoint saved: {ml_filtered_checkpoint}\")\n",
    "\n",
    "# Also save as CSV for inspection\n",
    "csv_checkpoint = \"pod-processing-outputs/ml_filtered_checkpoint.csv\"\n",
    "df.to_csv(csv_checkpoint, index=False)\n",
    "print(f\"‚úÖ CSV checkpoint saved: {csv_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('pod-processing-outputs', exist_ok=True)\n",
    "\n",
    "# Check if verification_df exists and has the required columns\n",
    "if 'verification_df' not in locals() or 'up_holdings' not in verification_df.columns:\n",
    "    print(\"‚ö†Ô∏è Verification results not found in memory\")\n",
    "    print(\"üîÑ Attempting to reload from saved files...\")\n",
    "    \n",
    "    if os.path.exists(\"pod-processing-outputs/selenium_verification_results.parquet\"):\n",
    "        verification_df = pd.read_parquet(\"pod-processing-outputs/selenium_verification_results.parquet\")\n",
    "        print(f\"‚úÖ Reloaded {len(verification_df):,} verification results\")\n",
    "    else:\n",
    "        print(\"‚ùå No verification results file found\")\n",
    "        raise ValueError(\"Cannot proceed without verification results\")\n",
    "\n",
    "# Filter records based on verification results\n",
    "if 'up_holdings' in verification_df.columns and 'status' in verification_df.columns:\n",
    "    # Get confirmed Penn-only holdings (determined as Penn-only)\n",
    "    df_penn_only = verification_df[verification_df['up_holdings'] == True].copy()\n",
    "    \n",
    "    # Get indeterminate records for separate tracking\n",
    "    df_indeterminate = verification_df[verification_df['status'] == 'indeterminate'].copy()\n",
    "    \n",
    "    # Remove duplicates based on match key or record ID\n",
    "    if key_columns['match_key']:\n",
    "        initial_count = len(df_penn_only)\n",
    "        df_penn_only = df_penn_only.drop_duplicates(subset=[key_columns['match_key']])\n",
    "        dedup_count = initial_count - len(df_penn_only)\n",
    "        if dedup_count > 0:\n",
    "            print(f\"üîß Removed {dedup_count:,} duplicate records based on match key\")\n",
    "            \n",
    "        if len(df_indeterminate) > 0:\n",
    "            initial_indet = len(df_indeterminate)\n",
    "            df_indeterminate = df_indeterminate.drop_duplicates(subset=[key_columns['match_key']])\n",
    "            indet_dedup = initial_indet - len(df_indeterminate)\n",
    "            if indet_dedup > 0:\n",
    "                print(f\"üîß Removed {indet_dedup:,} duplicate indeterminate records\")\n",
    "                \n",
    "    elif key_columns['record_id']:\n",
    "        initial_count = len(df_penn_only)\n",
    "        df_penn_only = df_penn_only.drop_duplicates(subset=[key_columns['record_id']])\n",
    "        dedup_count = initial_count - len(df_penn_only)\n",
    "        if dedup_count > 0:\n",
    "            print(f\"üîß Removed {dedup_count:,} duplicate records based on record ID\")\n",
    "            \n",
    "        if len(df_indeterminate) > 0:\n",
    "            initial_indet = len(df_indeterminate)\n",
    "            df_indeterminate = df_indeterminate.drop_duplicates(subset=[key_columns['record_id']])\n",
    "            indet_dedup = initial_indet - len(df_indeterminate)\n",
    "            if indet_dedup > 0:\n",
    "                print(f\"üîß Removed {indet_dedup:,} duplicate indeterminate records\")\n",
    "    \n",
    "    # Calculate status breakdown\n",
    "    total_determined = (verification_df['status'] == 'determined').sum()\n",
    "    total_indeterminate = len(df_indeterminate)\n",
    "    total_errors = (verification_df['status'] == 'error').sum()\n",
    "    \n",
    "    # NEW: Check if ML filtering was applied\n",
    "    ml_filtered = os.path.exists(\"pod-processing-outputs/penn_bd_unique_1m_filtered.parquet\")\n",
    "    if ml_filtered:\n",
    "        # Load ML summary for accurate counts\n",
    "        if os.path.exists(\"pod-processing-outputs/bd_ml_filtering_summary.json\"):\n",
    "            with open(\"pod-processing-outputs/bd_ml_filtering_summary.json\", 'r') as f:\n",
    "                ml_summary = json.load(f)\n",
    "            original_dataset_size = ml_summary['original_records']\n",
    "            ml_filtered_size = ml_summary['bd_unique_filtered']\n",
    "        else:\n",
    "            original_dataset_size = 1600000  # Approximate\n",
    "            ml_filtered_size = len(df) if 'df' in locals() else 1000000\n",
    "    else:\n",
    "        original_dataset_size = len(df) if 'df' in locals() else len(verification_df)\n",
    "        ml_filtered_size = original_dataset_size\n",
    "    \n",
    "    # Calculate sample rate based on verification vs ML-filtered dataset\n",
    "    sample_rate = len(verification_df) / ml_filtered_size if ml_filtered_size > 0 else 1\n",
    "    is_sample = len(verification_df) < ml_filtered_size\n",
    "    \n",
    "    print(f\"\\nüìä Final Results Summary:\")\n",
    "    if ml_filtered:\n",
    "        print(f\"   Original dataset: {original_dataset_size:,} records\")\n",
    "        print(f\"   ML-filtered to: {ml_filtered_size:,} BD-unique records\")\n",
    "    print(f\"   Verification sample: {len(verification_df):,} records ({sample_rate*100:.1f}% of {'ML-filtered' if ml_filtered else 'total'})\")\n",
    "    \n",
    "    print(f\"\\n   Status Breakdown (from verification sample):\")\n",
    "    print(f\"   - Determined: {total_determined:,} ({total_determined/len(verification_df)*100:.1f}%)\")\n",
    "    print(f\"   - Indeterminate: {total_indeterminate:,} ({total_indeterminate/len(verification_df)*100:.1f}%)\")\n",
    "    print(f\"   - Errors: {total_errors:,} ({total_errors/len(verification_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Enhanced reporting with ML context\n",
    "    if is_sample:\n",
    "        # Calculate estimations for ML-filtered dataset\n",
    "        est_confirmed_min = int(ml_filtered_size * len(df_penn_only) / len(verification_df)) if len(verification_df) > 0 else 0\n",
    "        est_indeterminate = int(ml_filtered_size * len(df_indeterminate) / len(verification_df)) if len(verification_df) > 0 else 0\n",
    "        est_potential_max = est_confirmed_min + est_indeterminate\n",
    "        \n",
    "        print(f\"\\n   üìà Estimated BD-Unique Penn Holdings (extrapolated to ML-filtered ~1M):\")\n",
    "        print(f\"   Confirmed minimum: ~{est_confirmed_min:,} holdings\")\n",
    "        print(f\"   Indeterminate (possibly unique): ~{est_indeterminate:,} holdings\") \n",
    "        print(f\"   Potential maximum: ~{est_potential_max:,} holdings\")\n",
    "        print(f\"   (if all indeterminate records are also unique to Penn)\")\n",
    "        \n",
    "        # ADDED: Confidence intervals for extrapolated estimates\n",
    "        # Using 95% confidence interval for 1,000 sample\n",
    "        import math\n",
    "        margin_of_error = 1.96 * math.sqrt(0.25 / len(verification_df)) * ml_filtered_size\n",
    "        est_confirmed_low = max(0, est_confirmed_min - margin_of_error)\n",
    "        est_confirmed_high = est_confirmed_min + margin_of_error\n",
    "        \n",
    "        print(f\"   üìä 95% Confidence Interval for confirmed: {est_confirmed_low:,.0f} - {est_confirmed_high:,.0f} holdings\")\n",
    "        \n",
    "        # Show verification sample breakdown\n",
    "        print(f\"\\n   üìä From verification sample:\")\n",
    "        print(f\"   - Confirmed Penn-only: {len(df_penn_only):,} ({len(df_penn_only)/len(verification_df)*100:.1f}% of sample)\")\n",
    "        print(f\"   - Indeterminate: {len(df_indeterminate):,} ({len(df_indeterminate)/len(verification_df)*100:.1f}% of sample)\")\n",
    "        if total_determined > 0:\n",
    "            print(f\"   - Of determined records, {len(df_penn_only)/total_determined*100:.1f}% were Penn-only\")\n",
    "    \n",
    "    # Export indeterminate records separately if they exist\n",
    "    if len(df_indeterminate) > 0:\n",
    "        indet_excel = \"pod-processing-outputs/penn_indeterminate_holdings.xlsx\"\n",
    "        indet_parquet = \"pod-processing-outputs/penn_indeterminate_holdings.parquet\"\n",
    "        df_indeterminate.to_excel(indet_excel, index=False)\n",
    "        df_indeterminate.to_parquet(indet_parquet, index=False)\n",
    "        print(f\"\\nüìù Exported {len(df_indeterminate):,} indeterminate records:\")\n",
    "        print(f\"   - {indet_excel}\")\n",
    "        print(f\"   - {indet_parquet}\")\n",
    "        print(f\"   ‚ÑπÔ∏è These may be unique to Penn but cannot be verified through BorrowDirect\")\n",
    "    \n",
    "    if len(df_penn_only) > 0:\n",
    "        # Export confirmed Penn-only holdings\n",
    "        excel_output = \"pod-processing-outputs/penn_unique_confirmed.xlsx\"\n",
    "        df_penn_only.to_excel(excel_output, index=False)\n",
    "        print(f\"\\n‚úÖ Exported {len(df_penn_only):,} confirmed Penn-only records to {excel_output}\")\n",
    "        \n",
    "        # Also save as Parquet\n",
    "        parquet_output = \"pod-processing-outputs/penn_unique_confirmed.parquet\"\n",
    "        df_penn_only.to_parquet(parquet_output, index=False)\n",
    "        print(f\"‚úÖ Exported {len(df_penn_only):,} Penn-only records to {parquet_output}\")\n",
    "        \n",
    "        # Save detailed verification results\n",
    "        verification_output = \"pod-processing-outputs/holdings_verification_results.parquet\"\n",
    "        verification_df.to_parquet(verification_output, index=False)\n",
    "        print(f\"‚úÖ Saved full verification results to {verification_output}\")\n",
    "        \n",
    "        # Display sample of Penn-only records\n",
    "        print(f\"\\nüìã Sample Penn-only holdings:\")\n",
    "        display_cols = []\n",
    "        if key_columns['record_id']:\n",
    "            display_cols.append(key_columns['record_id'])\n",
    "        if key_columns['match_key']:\n",
    "            display_cols.append(key_columns['match_key'])\n",
    "        \n",
    "        # Add title/isbn columns if available\n",
    "        for col in ['F245', 'title', 'F020', 'isbn']:\n",
    "            if col in df_penn_only.columns:\n",
    "                display_cols.append(col)\n",
    "                break\n",
    "        \n",
    "        print(df_penn_only[display_cols].head() if display_cols else df_penn_only.head())\n",
    "        \n",
    "        # Enhanced summary statistics with ML context\n",
    "        summary = {\n",
    "            'processing_pipeline': {\n",
    "                'original_dataset': original_dataset_size,\n",
    "                'ml_filtered': ml_filtered,\n",
    "                'ml_filtered_size': ml_filtered_size,\n",
    "                'verification_sample_size': len(verification_df)\n",
    "            },\n",
    "            'verification_results': {\n",
    "                'sample_rate': round(sample_rate * 100, 2),\n",
    "                'status_breakdown': {\n",
    "                    'determined': int(total_determined),\n",
    "                    'indeterminate': int(total_indeterminate),\n",
    "                    'errors': int(total_errors)\n",
    "                },\n",
    "                'penn_only_confirmed': len(df_penn_only),\n",
    "                'penn_only_pct_of_determined': round(len(df_penn_only)/total_determined*100, 2) if total_determined > 0 else 0,\n",
    "                'indeterminate_records': len(df_indeterminate)\n",
    "            },\n",
    "            'extrapolation': {\n",
    "                'target_dataset': 'ML-filtered ~1M BD-unique records' if ml_filtered else 'Full dataset',\n",
    "                'estimated_penn_only_min': est_confirmed_min if is_sample else len(df_penn_only),\n",
    "                'estimated_indeterminate': est_indeterminate if is_sample else len(df_indeterminate),\n",
    "                'estimated_penn_only_max': est_potential_max if is_sample else (len(df_penn_only) + len(df_indeterminate)),\n",
    "                'is_extrapolated': is_sample\n",
    "            },\n",
    "            'output_files': {\n",
    "                'confirmed': [excel_output, parquet_output],\n",
    "                'indeterminate': [indet_excel, indet_parquet] if len(df_indeterminate) > 0 else [],\n",
    "                'verification_results': verification_output\n",
    "            },\n",
    "            'input_file': loaded_from if 'loaded_from' in locals() else 'unknown'\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        summary_output = \"pod-processing-outputs/final_verification_summary.json\"\n",
    "        with open(summary_output, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        print(f\"\\n‚úÖ Saved processing summary to {summary_output}\")\n",
    "        \n",
    "        # Final interpretation note\n",
    "        print(f\"\\nüìå FINAL INTERPRETATION:\")\n",
    "        if ml_filtered and is_sample:\n",
    "            print(f\"   From the original {original_dataset_size:,} Penn records:\")\n",
    "            print(f\"   ‚Ä¢ ML identified ~{ml_filtered_size:,} as likely BD-unique\")\n",
    "            print(f\"   ‚Ä¢ Verification sample suggests ~{est_confirmed_min:,}-{est_potential_max:,} are Penn-only\")\n",
    "            print(f\"   ‚Ä¢ This represents {est_confirmed_min/original_dataset_size*100:.1f}%-{est_potential_max/original_dataset_size*100:.1f}% of the original dataset\")\n",
    "        elif is_sample:\n",
    "            print(f\"   These results are based on a {sample_rate*100:.1f}% sample.\")\n",
    "            print(f\"   The full dataset likely contains ~{est_confirmed_min:,}-{est_potential_max:,} Penn-only holdings.\")\n",
    "        else:\n",
    "            print(f\"   Verified {len(verification_df):,} records directly.\")\n",
    "            print(f\"   Found {len(df_penn_only):,} confirmed Penn-only holdings.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No confirmed Penn-only holdings found in the verification sample\")\n",
    "        if len(df_indeterminate) > 0:\n",
    "            print(f\"   However, {len(df_indeterminate):,} indeterminate records were exported for review\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No holdings verification was performed - cannot create Penn-only export\")\n",
    "    print(\"Please ensure the holdings verification step completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HathiTrust Digital Availability Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which unique Penn holdings are already digitized in HathiTrust\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HATHITRUST DIGITAL AVAILABILITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add HathiTrust directory to path\n",
    "sys.path.append('hathitrust')\n",
    "\n",
    "try:\n",
    "    from hathitrust_availability_checker_excel import HathiTrustFullScanner\n",
    "    \n",
    "    # UPDATED: Check ML-filtered dataset if available\n",
    "    datasets_to_check = []\n",
    "    \n",
    "    # First priority: Check the ML-filtered ~1M dataset\n",
    "    if os.path.exists(\"pod-processing-outputs/ml_filtered_checkpoint.parquet\"):\n",
    "        print(\"üìÇ Loading ML-filtered ~1M BD-unique dataset for HathiTrust check...\")\n",
    "        df_ml_filtered = pd.read_parquet(\"pod-processing-outputs/ml_filtered_checkpoint.parquet\")\n",
    "        datasets_to_check.append(('ml_filtered_1m', df_ml_filtered, 'ML-filtered BD-unique (~1M)'))\n",
    "        print(f\"   ‚úÖ Loaded {len(df_ml_filtered):,} ML-filtered records\")\n",
    "        \n",
    "        # For large dataset, use sampling approach\n",
    "        if len(df_ml_filtered) > 10000:\n",
    "            print(f\"   ‚ö†Ô∏è Large dataset ({len(df_ml_filtered):,} records) - will check a representative sample\")\n",
    "            sample_size = min(5000, int(len(df_ml_filtered) * 0.005))  # 0.5% or max 5000\n",
    "            df_sample = df_ml_filtered.sample(n=sample_size, random_state=42)\n",
    "            datasets_to_check = [('ml_filtered_sample', df_sample, f'ML-filtered sample ({sample_size:,} of {len(df_ml_filtered):,})')]\n",
    "            \n",
    "            # Save sample info for reporting\n",
    "            sample_info = {\n",
    "                'full_dataset_size': len(df_ml_filtered),\n",
    "                'sample_size': sample_size,\n",
    "                'sample_rate': sample_size / len(df_ml_filtered)\n",
    "            }\n",
    "    else:\n",
    "        # Fallback to verification results if ML-filtered not available\n",
    "        print(\"‚ö†Ô∏è ML-filtered dataset not found, checking verification results instead...\")\n",
    "        \n",
    "        if 'df_penn_only' in locals() and len(df_penn_only) > 0:\n",
    "            datasets_to_check.append(('df_penn_only', df_penn_only, 'Penn-only confirmed'))\n",
    "        \n",
    "        if 'df_indeterminate' in locals() and len(df_indeterminate) > 0:\n",
    "            datasets_to_check.append(('df_indeterminate', df_indeterminate, 'indeterminate'))\n",
    "    \n",
    "    if not datasets_to_check:\n",
    "        print(\"‚ùå No holdings found to check\")\n",
    "        print(\"Please ensure the ML filtering or holdings verification step completed successfully\")\n",
    "    else:\n",
    "        for df_name, df_to_check, description in datasets_to_check:\n",
    "            print(f\"\\nChecking {len(df_to_check):,} {description} holdings for HathiTrust availability...\")\n",
    "            \n",
    "            # Save temporary Excel file with proper column names\n",
    "            temp_file = f'pod-processing-outputs/temp_hathitrust_input_{df_name}.xlsx'\n",
    "            \n",
    "            # FIXED: Use key_columns['record_id'] instead of hardcoded 'F001'\n",
    "            record_id_col = key_columns['record_id'] if 'key_columns' in locals() and key_columns.get('record_id') else 'F001'\n",
    "            \n",
    "            # Ensure borrowdir_col is defined\n",
    "            borrowdir_col = key_columns.get('borrowdir_results', 'borrowdir_ids') if 'key_columns' in locals() else 'borrowdir_ids'\n",
    "            \n",
    "            # Prepare columns for HathiTrust checker\n",
    "            hathi_df = pd.DataFrame({\n",
    "                'MMS_ID': df_to_check[record_id_col] if record_id_col in df_to_check.columns else df_to_check.index,\n",
    "                'F245': df_to_check['F245'] if 'F245' in df_to_check.columns else '',\n",
    "                'F020_str': df_to_check['F020'].astype(str) if 'F020' in df_to_check.columns else '',\n",
    "                'F010_str': df_to_check['F010'].astype(str) if 'F010' in df_to_check.columns else '',\n",
    "                'F260_str': df_to_check['F260'].astype(str) if 'F260' in df_to_check.columns else '',\n",
    "                'id_list_str': df_to_check['F035'].astype(str) if 'F035' in df_to_check.columns else '',\n",
    "                'borrowdir_id': df_to_check[borrowdir_col] if borrowdir_col in df_to_check.columns else ''\n",
    "            })\n",
    "            \n",
    "            # Save to Excel\n",
    "            hathi_df.to_excel(temp_file, index=False)\n",
    "            print(f\"‚úÖ Prepared data saved to: {temp_file}\")\n",
    "            \n",
    "            # Initialize scanner with conservative rate limiting\n",
    "            scanner = HathiTrustFullScanner(rate_limit_delay=0.3, max_workers=3)\n",
    "            \n",
    "            # Run the scan\n",
    "            print(f\"\\nStarting HathiTrust scan for {description} holdings...\")\n",
    "            print(\"This may take several minutes depending on the number of records...\")\n",
    "            \n",
    "            # ADDED: Create output filename based on dataset type\n",
    "            output_suffix = f\"_{df_name}\" if df_name else \"\"\n",
    "            scanner.scan_full_file(temp_file, batch_size=50, output_suffix=output_suffix)\n",
    "            \n",
    "            # Results are automatically saved by the scanner\n",
    "            print(f\"\\n‚úÖ HathiTrust check complete for {description} holdings!\")\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "            \n",
    "            # If this was a sample, extrapolate results\n",
    "            if 'sample_info' in locals() and df_name == 'ml_filtered_sample':\n",
    "                print(f\"\\nüìä Extrapolating results to full ML-filtered dataset:\")\n",
    "                print(f\"   Sample checked: {sample_info['sample_size']:,} records\")\n",
    "                print(f\"   Full dataset: {sample_info['full_dataset_size']:,} records\")\n",
    "                print(f\"   Results in 'hathitrust/reports' can be extrapolated by factor of {1/sample_info['sample_rate']:.1f}\")\n",
    "        \n",
    "        print(\"\\nüìÅ Check the 'hathitrust/reports' directory for detailed results\")\n",
    "        if 'sample_info' in locals():\n",
    "            print(\"   üìå Note: Results are from a representative sample of the ~1M ML-filtered dataset\")\n",
    "            print(\"   Multiply findings by the extrapolation factor for full dataset estimates\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Could not import HathiTrust scanner\")\n",
    "    print(\"Please ensure hathitrust_availability_checker_excel.py is in the hathitrust/ directory\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during HathiTrust check: {str(e)}\")\n",
    "    \n",
    "    # Clean up on error - check for any temp files\n",
    "    if 'temp_file' in locals() and os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "    # ADDED: Clean up any other temp files that might exist\n",
    "    for temp_pattern in ['temp_hathitrust_input_df_penn_only.xlsx', 'temp_hathitrust_input_df_indeterminate.xlsx', 'temp_hathitrust_input_ml_filtered_1m.xlsx', 'temp_hathitrust_input_ml_filtered_sample.xlsx']:\n",
    "        temp_path = os.path.join('pod-processing-outputs', temp_pattern)\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
