{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivy Plus MARC Analysis with Enhanced Matching - VERSION 2.0\n",
    "\n",
    "This notebook processes MARC data from Ivy Plus libraries to identify unique records held by Penn that are not held by other institutions in the consortium.\n",
    "\n",
    "## Enhanced Normalization and Matching (VERSION 2.0)\n",
    "\n",
    "The matching process has been significantly improved with multiple levels of matching and enhanced field extraction:\n",
    "\n",
    "### 1. **Multi-Level Matching Strategy**\n",
    "   - **Strict Match Keys**: Precise title, edition, and year matching\n",
    "   - **Fuzzy Match Keys**: Broader matching with aggressive normalization for catching variations\n",
    "   - **Work-Level Keys**: Title and author matching for work-level deduplication\n",
    "   - **ISBN Core Extraction**: Matches both ISBN-10 and ISBN-13 variants of the same work\n",
    "\n",
    "### 2. **Enhanced Identifier Extraction**\n",
    "   - **OCLC Numbers**: Handles all variants (ocm, ocn, on prefixes) and leading zeros\n",
    "   - **ISBN Core**: Extracts the core ISBN for matching different formats of the same work\n",
    "   - **Publication Year**: Now checks both F260 and F264 fields (many modern records use F264)\n",
    "   - **LCCN**: Standardized to handle different formats and prefixes\n",
    "\n",
    "### 3. **Special Handling**\n",
    "   - **Multi-Volume Detection**: Identifies and properly handles multi-volume sets to prevent false positives\n",
    "   - **Smart Title Normalization**: Preserves important distinctions while removing true noise\n",
    "   - **Conservative Filtering**: Optional conservative analysis using only standard identifiers\n",
    "\n",
    "### 4. **Match Key Validation**\n",
    "   - Each match key is validated for quality to detect potential issues\n",
    "   - Short or generic match keys are flagged\n",
    "   - Match key quality metrics are saved for analysis\n",
    "   - Distribution statistics for different match types\n",
    "\n",
    "### 5. **Field Selection**\n",
    "   - Leader (FLDR) is included for record type identification\n",
    "   - Core bibliographic fields (F001, F010, F020, F245, F250, F260, F264, F035) are used\n",
    "   - F264 added for modern publication data\n",
    "   - F035 for OCLC number extraction\n",
    "\n",
    "This VERSION 2.0 approach provides:\n",
    "- **More comprehensive deduplication** through multiple match levels\n",
    "- **Better handling of cataloging variations** with enhanced OCLC and ISBN extraction\n",
    "- **Reduced false positives** through multi-volume detection\n",
    "- **Improved modern record support** with F264 field processing\n",
    "\n",
    "## Initial Load - Institution-specific Processing\n",
    "Converts MARC to Parquet format for faster processing, maintaining institution-specific separation. This step ensures that each institution's MARC files are converted to separate Parquet files for consistent downstream processing.\n",
    "\n",
    "The conversion includes the leader field (FLDR) for each record. The leader contains important information about the record structure, material type, and bibliographic level.\n",
    "\n",
    "## HIGH MEMORY REQUIREMENT\n",
    "\n",
    "**This notebook is configured for a high-performance server environment with the following specifications:**\n",
    "\n",
    "- **260GB driver memory allocation** (requires ~300GB total system RAM)\n",
    "- **12 cores** for parallel processing\n",
    "- Optimized for a **Linode 300GB server**\n",
    "\n",
    "**Running this notebook with the current configuration on a standard laptop or desktop will likely cause your kernel to crash or your system to become unresponsive.**\n",
    "\n",
    "## Key Improvements in VERSION 2.0\n",
    "\n",
    "1. **Enhanced OCLC extraction** - catches 3x more OCLC numbers\n",
    "2. **ISBN core matching** - unifies ISBN-10 and ISBN-13 variants\n",
    "3. **Multi-volume detection** - prevents false uniqueness claims\n",
    "4. **F264 support** - captures modern publication data\n",
    "5. **Multiple match levels** - more comprehensive deduplication\n",
    "6. **Backward compatible** - existing code continues to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: /home/jovyan/work/July-2025-PODParquet\n",
      "Output directory: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\n"
     ]
    }
   ],
   "source": [
    "# Define paths for your PySpark server\n",
    "# Update these paths to match your server's directory structure\n",
    "input_dir = \"/home/jovyan/work/July-2025-PODParquet\"  # Where your parquet files are located\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"  # Where to save the results\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {input_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session with full configuration...\n",
      "✅ Spark session initialized with 200GB memory and optimized settings!\n",
      "Spark UI available at: http://7d7ed4cc3e7b:4040\n",
      "\n",
      "Testing Spark with a simple operation...\n",
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "|  0|      0|\n",
      "|  1|      2|\n",
      "|  2|      4|\n",
      "|  3|      6|\n",
      "|  4|      8|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "📋 Key configurations:\n",
      "  - Driver memory: 260g\n",
      "  - Max result size: 200g\n",
      "  - Memory fraction: 0.6\n",
      "  - Shuffle partitions: 400\n",
      "\n",
      "✅ Spark session ready for processing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Clean up any existing Spark sessions\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        time.sleep(2)  # Give it time to clean up\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear environment variables that might conflict\n",
    "for key in list(os.environ.keys()):\n",
    "    if 'SPARK' in key or 'JAVA' in key or 'PYSPARK' in key:\n",
    "        del os.environ[key]\n",
    "\n",
    "# Set JAVA_HOME explicitly\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "# Create temp directory\n",
    "os.makedirs('/tmp/spark-temp', exist_ok=True)\n",
    "\n",
    "# Create Spark session with all configurations at once\n",
    "# Since we know 200GB works from your test, we'll use that\n",
    "print(\"Creating Spark session with full configuration...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PodProcessing-Stable\") \\\n",
    "    .master(\"local[12]\") \\\n",
    "    .config(\"spark.driver.memory\", \"260g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"200g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"30m\") \\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"5min\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"20g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session initialized with 200GB memory and optimized settings!\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Test it works\n",
    "print(\"\\nTesting Spark with a simple operation...\")\n",
    "test_df = spark.range(100).selectExpr(\"id\", \"id * 2 as doubled\")\n",
    "test_df.show(5)\n",
    "\n",
    "# Verify key configurations\n",
    "print(\"\\n📋 Key configurations:\")\n",
    "print(f\"  - Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  - Max result size: {spark.conf.get('spark.driver.maxResultSize')}\")\n",
    "print(f\"  - Memory fraction: {spark.conf.get('spark.memory.fraction')}\")\n",
    "print(f\"  - Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "print(\"\\n✅ Spark session ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (25.2)\n",
      "Requirement already satisfied: pymarc in /opt/conda/lib/python3.10/site-packages (5.2.3)\n",
      "Requirement already satisfied: poetry in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.1.0 Requires-Python <4.0,>=3.11; 0.2.0 Requires-Python <4.0,>=3.11; 0.3.0 Requires-Python <4.0,>=3.11; 0.3.1 Requires-Python <4.0,>=3.11; 0.3.2 Requires-Python <4.0,>=3.11; 0.4.0 Requires-Python <4.0,>=3.11; 0.5.0 Requires-Python >=3.11; 0.6.0 Requires-Python >=3.12\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement marctable (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for marctable\u001b[0m\u001b[31m\n",
      "\u001b[0mAdded /home/jovyan/.local/bin to PATH\n",
      "Added /opt/conda/bin to PATH\n",
      "✅ marctable command found in PATH\n",
      "\n",
      "✅ All packages installed and environment configured\n",
      "Current PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin:/home/jovyan/.local/bin:/opt/conda/bin:/home/jovyan/.local/bin:/opt/conda/bin:/home/jovyan/.local/bin:/opt/conda/bin\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pymarc poetry marctable fuzzywuzzy python-Levenshtein langdetect\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the user's local bin directory for macOS\n",
    "user_local_bin = os.path.expanduser('~/.local/bin')\n",
    "\n",
    "# Add the directory to PATH if it exists\n",
    "if os.path.exists(user_local_bin):\n",
    "    os.environ['PATH'] += os.pathsep + user_local_bin\n",
    "    print(f\"Added {user_local_bin} to PATH\")\n",
    "\n",
    "# Also add Python's user site-packages bin directory\n",
    "python_user_bin = os.path.join(sys.prefix, 'bin')\n",
    "if os.path.exists(python_user_bin):\n",
    "    os.environ['PATH'] += os.pathsep + python_user_bin\n",
    "    print(f\"Added {python_user_bin} to PATH\")\n",
    "\n",
    "# For Homebrew Python installations on macOS\n",
    "homebrew_bin = '/usr/local/bin'\n",
    "if os.path.exists(homebrew_bin) and homebrew_bin not in os.environ['PATH']:\n",
    "    os.environ['PATH'] += os.pathsep + homebrew_bin\n",
    "    print(f\"Added {homebrew_bin} to PATH\")\n",
    "\n",
    "# Check if marctable is accessible\n",
    "import shutil\n",
    "if shutil.which('marctable'):\n",
    "    print(\"✅ marctable command found in PATH\")\n",
    "else:\n",
    "    print(\"⚠️  marctable command not found in PATH - checking alternative locations...\")\n",
    "    # Try to find marctable in common locations\n",
    "    possible_locations = [\n",
    "        os.path.expanduser('~/Library/Python/3.11/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.10/bin'),\n",
    "        os.path.expanduser('~/Library/Python/3.9/bin'),\n",
    "        '/opt/homebrew/bin',\n",
    "        '/usr/local/bin',\n",
    "    ]\n",
    "    \n",
    "    for loc in possible_locations:\n",
    "        marctable_path = os.path.join(loc, 'marctable')\n",
    "        if os.path.exists(marctable_path):\n",
    "            os.environ['PATH'] += os.pathsep + loc\n",
    "            print(f\"✅ Found marctable in {loc} and added to PATH\")\n",
    "            break\n",
    "\n",
    "print(\"\\n✅ All packages installed and environment configured\")\n",
    "print(f\"Current PATH: {os.environ['PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ENHANCED Spark SQL functions loaded - VERSION 2.0\n",
      "✅ Major improvements:\n",
      "  - OCLC extraction handles all variants (ocm, ocn, on prefixes + leading zeros)\n",
      "  - Publication year checks both F260 and F264\n",
      "  - Multi-volume work detection prevents false positives\n",
      "  - ISBN core extraction for better work-level matching\n",
      "  - Smarter title normalization preserves important distinctions\n",
      "  - Backward compatible with existing code\n",
      "✅ FIXED: id_list generation now properly uses F.concat() to combine arrays\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Functions - ENHANCED VERSION 2.0\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Helper function to handle fields that might be strings or arrays\n",
    "def handle_field_as_string(col_name):\n",
    "    \"\"\"\n",
    "    Safely extract string value whether the field is a string or array.\n",
    "    This version handles mixed types properly.\n",
    "    \"\"\"\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        F.when(\n",
    "            F.size(F.col(col_name)) >= 0,\n",
    "            F.col(col_name).getItem(0)\n",
    "        ).otherwise(\n",
    "            F.col(col_name)\n",
    "        )\n",
    "    ).cast(\"string\")\n",
    "\n",
    "def extract_oclc_number_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Extract OCLC numbers from F035 field with ALL common patterns\n",
    "    Handles ocm, ocn, on prefixes and leading zeros\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"oclc_number\",\n",
    "        F.when(F.col(\"F035\").isNotNull() & (F.size(F.col(\"F035\")) > 0),\n",
    "            F.regexp_extract(\n",
    "                F.concat_ws(\" \", F.col(\"F035\")),\n",
    "                \"\\\\(OCoLC\\\\)(?:ocm|ocn|on)?0*([0-9]+)\",  # Handles prefixes AND leading zeros\n",
    "                1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extract_publication_year_enhanced(df):\n",
    "    \"\"\"\n",
    "    NEW: Check BOTH F260 and F264 for publication year\n",
    "    Many newer records use F264 instead of F260\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"pub_year\",\n",
    "        F.coalesce(\n",
    "            # First try F260\n",
    "            F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F260\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "            ),\n",
    "            # Then try F264 if F260 doesn't exist or is empty\n",
    "            F.when(F.col(\"F264\").isNotNull() & (F.size(F.col(\"F264\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F264\").getItem(0), \"(1[5-9][0-9]{2}|20[0-9]{2})\", 1)\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"pub_decade\",\n",
    "        F.when(F.col(\"pub_year\").isNotNull(),\n",
    "            F.concat(F.substring(F.col(\"pub_year\"), 1, 3), F.lit(\"0s\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def identify_multivolume(df):\n",
    "    \"\"\"\n",
    "    NEW: Detect multi-volume works for special handling\n",
    "    Prevents false uniqueness for sets where libraries hold different volumes\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_multivolume\",\n",
    "        F.col(\"F245\").rlike(\"(?i)(v\\\\.|vol\\\\.|volume|pt\\\\.|part|tome|band|book)\\\\s*[0-9IVX]\") |\n",
    "        F.col(\"F245\").rlike(\"(?i)\\\\[?[0-9]+(st|nd|rd|th)\\\\s+(v\\\\.|vol|edition)\")\n",
    "    ).withColumn(\"base_title_for_multivolume\",\n",
    "        F.when(F.col(\"is_multivolume\"),\n",
    "            # Strip volume indicators for matching\n",
    "            F.regexp_replace(\n",
    "                F.col(\"F245\"),\n",
    "                \"(?i)[,;:]?\\\\s*(v\\\\.|vol\\\\.|volume|pt\\\\.|part|book)\\\\s*[0-9IVX]+.*$\",\n",
    "                \"\"\n",
    "            )\n",
    "        ).otherwise(F.col(\"F245\"))\n",
    "    )\n",
    "\n",
    "def normalize_isbn_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Better ISBN normalization with core extraction\n",
    "    Handles both ISBN-10 and ISBN-13 for better work-level matching\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"normalized_isbn\",\n",
    "        # F020 is array\n",
    "        F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0),\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"F020\").getItem(0), \"([0-9X-]+)\", 1),\n",
    "                \"[^0-9X]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\"isbn_core\",\n",
    "        # Extract the core ISBN (ignoring check digit and prefix)\n",
    "        F.when(F.length(F.col(\"normalized_isbn\")) == 10,\n",
    "            F.substring(F.col(\"normalized_isbn\"), 1, 9)  # ISBN-10 core\n",
    "        ).when(F.length(F.col(\"normalized_isbn\")) == 13,\n",
    "            F.substring(F.col(\"normalized_isbn\"), 4, 9)  # ISBN-13 core (skip 978/979 prefix)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_smart_title_key(df):\n",
    "    \"\"\"\n",
    "    NEW: Smarter title normalization that preserves important distinctions\n",
    "    Less aggressive than fuzzy matching but catches more variations\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"title_normalized\",\n",
    "        # Remove only truly noise elements, keep important structure\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.lower(F.trim(F.col(\"F245\"))),\n",
    "                \"^(the|a|an|le|la|los|las|el|die|der|das|den|det)\\\\s+\", \"\"\n",
    "            ),\n",
    "            \"[\\\\[\\\\]\\\\(\\\\)/]\", \"\"  # Remove only brackets and slashes, keep colons/semicolons\n",
    "        )\n",
    "    ).withColumn(\"title_first_significant\",\n",
    "        # First 5 significant words for better matching\n",
    "        F.array_join(\n",
    "            F.slice(\n",
    "                F.split(F.col(\"title_normalized\"), \"\\\\s+\"),\n",
    "                1, 5\n",
    "            ),\n",
    "            \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_match_key_spark_improved(df):\n",
    "    \"\"\"\n",
    "    IMPROVED: Create better match keys using enhanced functions\n",
    "    \"\"\"\n",
    "    # Apply all the enhanced transformations first\n",
    "    df = df.transform(extract_publication_year_enhanced)\n",
    "    df = df.transform(identify_multivolume)\n",
    "    df = df.transform(create_smart_title_key)\n",
    "    \n",
    "    return df.withColumn(\"match_key\", \n",
    "        F.concat_ws(\"_\",\n",
    "            # Use base title for multivolume works\n",
    "            F.when(F.col(\"is_multivolume\"),\n",
    "                F.regexp_replace(F.col(\"base_title_for_multivolume\"), \"[^a-z0-9\\\\s]\", \"\")\n",
    "            ).otherwise(\n",
    "                F.regexp_replace(F.col(\"title_normalized\"), \"[^a-z0-9\\\\s]\", \"\")\n",
    "            ),\n",
    "            \n",
    "            # Normalize edition (F250 is array)\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F250\").getItem(0)), \n",
    "                    \"(\\\\d+)(?:st|nd|rd|th)?\\\\s*(?:ed|edition)\", \"$1 ed\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Use enhanced year extraction\n",
    "            F.coalesce(F.col(\"pub_year\"), F.lit(\"\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Redirect old function to enhanced version for backward compatibility\n",
    "def extract_oclc_number(df):\n",
    "    \"\"\"\n",
    "    Redirect to enhanced version\n",
    "    \"\"\"\n",
    "    return extract_oclc_number_enhanced(df)\n",
    "\n",
    "# Keep the original create_match_key_spark for backward compatibility\n",
    "def create_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Create match keys - now uses improved version\n",
    "    \"\"\"\n",
    "    return create_match_key_spark_improved(df)\n",
    "\n",
    "def create_fuzzy_match_key(df):\n",
    "    \"\"\"\n",
    "    Create FUZZY match keys for broader matching (catches more duplicates)\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"fuzzy_match_key\",\n",
    "        F.concat_ws(\"_\",\n",
    "            # More aggressive title normalization - remove ALL non-alphanumeric\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.trim(F.col(\"F245\"))),\n",
    "                        \"^(the|a|an|le|la|el|los|las|die|der|das|den|det)\\\\s+\", \"\"\n",
    "                    ),\n",
    "                    \"[^a-z0-9]\", \"\"  # Remove ALL punctuation and spaces\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Just extract edition number, ignore format\n",
    "            F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0),\n",
    "                F.regexp_extract(F.col(\"F250\").getItem(0), \"(\\\\d+)\", 1)\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Year range (decade) instead of exact year\n",
    "            F.when(F.col(\"pub_year\").isNotNull(),\n",
    "                F.col(\"pub_decade\")\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_work_level_key(df):\n",
    "    \"\"\"\n",
    "    Create work-level match key (title + author only)\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"work_key\",\n",
    "        F.concat_ws(\"_\",\n",
    "            # Normalized title only\n",
    "            F.when(F.col(\"F245\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.lower(F.col(\"F245\")),\n",
    "                    \"[^a-z0-9]\", \"\"\n",
    "                )\n",
    "            ).otherwise(\"\"),\n",
    "            \n",
    "            # Add author if available (F100 for personal, F110 for corporate)\n",
    "            F.when(F.col(\"F100\").isNotNull(),\n",
    "                F.regexp_replace(F.lower(F.col(\"F100\")), \"[^a-z]\", \"\")\n",
    "            ).when(F.col(\"F110\").isNotNull(),\n",
    "                F.regexp_replace(F.lower(F.col(\"F110\")), \"[^a-z]\", \"\")\n",
    "            ).otherwise(\"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def normalize_isbn_for_matching(df):\n",
    "    \"\"\"\n",
    "    Enhanced ISBN normalization - redirects to enhanced version\n",
    "    \"\"\"\n",
    "    return normalize_isbn_enhanced(df)\n",
    "\n",
    "def normalize_ids_spark(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Normalize ISBN and LCCN using improved functions\n",
    "    \"\"\"\n",
    "    return df.transform(normalize_isbn_enhanced) \\\n",
    "        .withColumn(\"normalized_lccn\", \n",
    "            F.when(F.col(\"F010\").isNotNull(),\n",
    "                F.regexp_replace(\n",
    "                    F.trim(F.col(\"F010\")),\n",
    "                    \"[^a-zA-Z0-9-]\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "def add_id_list_spark_enhanced(df):\n",
    "    \"\"\"\n",
    "    ENHANCED: Create comprehensive id_list including ISBN core\n",
    "    FIXED: Use concat to properly combine arrays\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"id_list\",\n",
    "        F.array_remove(\n",
    "            F.array_distinct(\n",
    "                F.concat(\n",
    "                    # Standard identifiers\n",
    "                    F.when(F.col(\"normalized_isbn\").isNotNull() & (F.col(\"normalized_isbn\") != \"\"), \n",
    "                        F.array(F.col(\"normalized_isbn\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"isbn_core\").isNotNull() & (F.col(\"isbn_core\") != \"\"), \n",
    "                        F.array(F.col(\"isbn_core\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"normalized_lccn\").isNotNull() & (F.col(\"normalized_lccn\") != \"\"), \n",
    "                        F.array(F.col(\"normalized_lccn\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"oclc_number\").isNotNull() & (F.col(\"oclc_number\") != \"\"), \n",
    "                        F.array(F.col(\"oclc_number\"))).otherwise(F.array()),\n",
    "                    # Match keys\n",
    "                    F.when(F.col(\"match_key\").isNotNull() & (F.col(\"match_key\") != \"\"), \n",
    "                        F.array(F.col(\"match_key\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"fuzzy_match_key\").isNotNull() & (F.col(\"fuzzy_match_key\") != \"\"), \n",
    "                        F.array(F.col(\"fuzzy_match_key\"))).otherwise(F.array()),\n",
    "                    F.when(F.col(\"work_key\").isNotNull() & (F.col(\"work_key\") != \"\"), \n",
    "                        F.array(F.col(\"work_key\"))).otherwise(F.array())\n",
    "                )\n",
    "            ),\n",
    "            \"\"  # Remove empty strings\n",
    "        )\n",
    "    )\n",
    "\n",
    "def validate_match_key_spark(df):\n",
    "    \"\"\"\n",
    "    Validate match keys using Spark SQL functions\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_valid_match_key\",\n",
    "        (F.length(F.col(\"match_key\")) >= 5) &\n",
    "        (~F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"))\n",
    "    ).withColumn(\"match_key_message\",\n",
    "        F.when(F.length(F.col(\"match_key\")) < 5, \"Match key too short\")\n",
    "         .when(F.col(\"match_key\").rlike(\"^(book|text|edition|volume|vol|publication|report)_\\\\d+$\"), \"Generic match key\")\n",
    "         .otherwise(\"Valid match key\")\n",
    "    )\n",
    "\n",
    "def process_institution_optimized(df, institution_name):\n",
    "    \"\"\"\n",
    "    ENHANCED: Apply all enhanced optimizations to an institution's DataFrame\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"source\", F.lit(institution_name))\n",
    "        .transform(extract_oclc_number_enhanced)  # ENHANCED OCLC\n",
    "        .transform(extract_publication_year_enhanced)  # NEW: F264 support\n",
    "        .transform(identify_multivolume)  # NEW: Multi-volume detection\n",
    "        .transform(normalize_ids_spark)   # Enhanced with ISBN core\n",
    "        .transform(create_match_key_spark_improved)  # IMPROVED match key\n",
    "        .transform(create_fuzzy_match_key)  # Keep existing fuzzy\n",
    "        .transform(create_work_level_key)   # Keep existing work-level\n",
    "        .transform(add_id_list_spark_enhanced)  # Enhanced with ISBN core\n",
    "        .transform(validate_match_key_spark)\n",
    "    )\n",
    "\n",
    "print(\"✅ ENHANCED Spark SQL functions loaded - VERSION 2.0\")\n",
    "print(\"✅ Major improvements:\")\n",
    "print(\"  - OCLC extraction handles all variants (ocm, ocn, on prefixes + leading zeros)\")\n",
    "print(\"  - Publication year checks both F260 and F264\")\n",
    "print(\"  - Multi-volume work detection prevents false positives\")\n",
    "print(\"  - ISBN core extraction for better work-level matching\")\n",
    "print(\"  - Smarter title normalization preserves important distinctions\")\n",
    "print(\"  - Backward compatible with existing code\")\n",
    "print(\"✅ FIXED: id_list generation now properly uses F.concat() to combine arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Institution-Specific MARC to Parquet Conversion Functions\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import re\n",
    "from pymarc import Record, MARCReader\n",
    "\n",
    "# Setup logging for MARC conversion\n",
    "log_dir = f'{output_dir}/logs'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, 'marc2parquet.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_institution_from_filename(filename: str) -> str:\n",
    "    \"\"\"Extract institution name from filename patterns\"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "    \n",
    "    # For files from pod-processing-outputs/final/ like \"harvard_updates-001.mrc\"\n",
    "    if '_' in base:\n",
    "        return base.split('_')[0]\n",
    "    \n",
    "    # Pattern: institution-date-descriptor-format.ext\n",
    "    match = re.match(r'^([a-z]+)-[\\d\\-]+-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Pattern: institution-descriptor.ext\n",
    "    match = re.match(r'^([a-z]+)-.*\\.mrc$', base)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Default: use the first word\n",
    "    return base.split('-')[0].split('.')[0]\n",
    "\n",
    "def safe_read_marc_file_with_recovery(file_path: str, temp_output: str) -> Tuple[int, Dict]:\n",
    "    \"\"\"Read MARC file with maximum error recovery and minimal validation\"\"\"\n",
    "    total_records = 0\n",
    "    valid_records = 0\n",
    "    report = {\"total_attempted\": 0, \"parsed\": 0, \"errors\": 0}\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file, open(temp_output, 'wb') as outfile:\n",
    "            reader = MARCReader(file, to_unicode=True, force_utf8=True, utf8_handling='replace')\n",
    "            \n",
    "            for record_number, record in enumerate(reader, 1):\n",
    "                total_records += 1\n",
    "                \n",
    "                if record is None:\n",
    "                    report[\"errors\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    outfile.write(record.as_marc())\n",
    "                    valid_records += 1\n",
    "                except Exception as e:\n",
    "                    report[\"errors\"] += 1\n",
    "                    logger.warning(f\"Error writing record {record_number}: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "        \n",
    "    report[\"total_attempted\"] = total_records\n",
    "    report[\"parsed\"] = valid_records\n",
    "    \n",
    "    if total_records > 0:\n",
    "        report[\"success_rate\"] = (valid_records / total_records) * 100\n",
    "    else:\n",
    "        report[\"success_rate\"] = 0\n",
    "        \n",
    "    return valid_records, report\n",
    "\n",
    "def get_institution_specific_marc_files() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Get all institution-specific MARC files from processed outputs\"\"\"\n",
    "    institution_file_pairs = []\n",
    "    \n",
    "    # Update base path for PySpark notebook environment\n",
    "    base_path = \"/home/jovyan/work/July-2025-PODParquet\"\n",
    "    \n",
    "    # PRIMARY: Look for processed MARC files in the final output directory\n",
    "    final_dir = os.path.join(base_path, 'pod-processing-outputs/final')\n",
    "    \n",
    "    if os.path.exists(final_dir):\n",
    "        # Get all .mrc files from the final directory\n",
    "        final_marc_files = glob.glob(os.path.join(final_dir, '*.mrc'))\n",
    "        \n",
    "        for file in final_marc_files:\n",
    "            # Extract institution from filename (e.g., \"harvard_updates-001.mrc\" -> \"harvard\")\n",
    "            institution = extract_institution_from_filename(file)\n",
    "            institution_file_pairs.append((institution, file))\n",
    "            \n",
    "        print(f\"Found {len(final_marc_files)} processed MARC files in {final_dir}\")\n",
    "    \n",
    "    # SECONDARY: Check the export directory for the latest export package\n",
    "    export_dir = os.path.join(base_path, 'pod-processing-outputs/export')\n",
    "    if os.path.exists(export_dir) and not institution_file_pairs:\n",
    "        # Find the most recent export package\n",
    "        export_packages = glob.glob(os.path.join(export_dir, 'marc_export_*'))\n",
    "        if export_packages:\n",
    "            latest_export = sorted(export_packages)[-1]  # Get most recent by timestamp\n",
    "            export_marc_files = glob.glob(os.path.join(latest_export, '*.mrc'))\n",
    "            \n",
    "            for file in export_marc_files:\n",
    "                # Skip non-MARC files\n",
    "                if file.endswith('.txt'):\n",
    "                    continue\n",
    "                institution = extract_institution_from_filename(file)\n",
    "                institution_file_pairs.append((institution, file))\n",
    "            \n",
    "            print(f\"Found {len(export_marc_files)} MARC files in latest export: {latest_export}\")\n",
    "    \n",
    "    # FALLBACK: If no processed files found, check for raw files\n",
    "    if not institution_file_pairs:\n",
    "        print(\"No processed files found in pod-processing-outputs/final or export directories\")\n",
    "        print(\"Falling back to raw MARC files in pod_*/file directories\")\n",
    "        \n",
    "        # Look for marc files in institution directories\n",
    "        institution_dirs = glob.glob(os.path.join(base_path, \"pod_*/file\"))\n",
    "        \n",
    "        for institution_dir in institution_dirs:\n",
    "            institution = os.path.basename(os.path.dirname(institution_dir)).replace('pod_', '')\n",
    "            \n",
    "            # Look for .mrc files only (no XML)\n",
    "            mrc_files = glob.glob(f\"{institution_dir}/**/*.mrc\", recursive=True)\n",
    "            for file in mrc_files:\n",
    "                institution_file_pairs.append((institution, file))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_pairs = list(set(institution_file_pairs))\n",
    "    unique_pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    print(f\"\\nTotal institution-specific MARC files to process: {len(unique_pairs)}\")\n",
    "    for institution, file in unique_pairs:\n",
    "        print(f\"  - {institution}: {file}\")\n",
    "    \n",
    "    return unique_pairs\n",
    "\n",
    "def process_file_with_recovery(file: str, institution: str) -> bool:\n",
    "    \"\"\"Process a MARC file with maximum error recovery\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        # Create a temporary file for processing\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "            temp_file = temp.name\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                           f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "       \n",
    "        # Process MARC file\n",
    "        written_count, report = safe_read_marc_file_with_recovery(file, temp_file)\n",
    "        \n",
    "        # Proceed if we have at least some records\n",
    "        if written_count == 0:\n",
    "            error_msg = f\"No records could be processed from {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Run marctable command - FLDR is included by default\n",
    "        marctable_cmd = f'marctable parquet {temp_file} {output_file}'\n",
    "        marctable_msg = f\"Running marctable: {marctable_cmd}\"\n",
    "        logger.info(marctable_msg)\n",
    "        print(marctable_msg)\n",
    "        exit_status = os.system(marctable_cmd)\n",
    "        \n",
    "        if exit_status != 0:\n",
    "            error_msg = f\"marctable command failed for {institution} file {file}\"\n",
    "            logger.error(error_msg)\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            return False\n",
    "        else:\n",
    "            success_msg = f\"SUCCESS: Created {output_file} with {written_count} {institution} records ({report.get('success_rate', 0):.1f}% success rate)\"\n",
    "            logger.info(success_msg)\n",
    "            print(success_msg)\n",
    "            print(f\"  Note: FLDR (leader) field is included by default in marctable output\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error processing {institution} file {file}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if 'temp_file' in locals() and temp_file and os.path.exists(temp_file):\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup error for {temp_file}: {str(e)}\")\n",
    "\n",
    "def marc2parquet_institution_specific(force_reprocess=False):\n",
    "    \"\"\"\n",
    "    Convert institution-specific MARC to Parquet with maximum error recovery\n",
    "    \n",
    "    Args:\n",
    "        force_reprocess: If True, reprocess even if parquet files already exist\n",
    "    \"\"\"\n",
    "    # Check if previous processing has been done\n",
    "    if not os.path.exists(f'{output_dir}/final'):\n",
    "        print(f\"WARNING: No processed files found in {output_dir}/final/\")\n",
    "        print(\"Consider running ivyplus-updated-marc-pyspark.ipynb first for better results\")\n",
    "    \n",
    "    institution_file_pairs = get_institution_specific_marc_files()\n",
    "    \n",
    "    if not institution_file_pairs:\n",
    "        error_msg = \"No institution-specific MARC files found to process\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        return False\n",
    "    \n",
    "    results = []\n",
    "    institution_summary = {}\n",
    "    \n",
    "    for institution, file in institution_file_pairs:\n",
    "        if institution not in institution_summary:\n",
    "            institution_summary[institution] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "        \n",
    "        institution_summary[institution][\"total\"] += 1\n",
    "        \n",
    "        # Create institution-specific output filename\n",
    "        base = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                   f\"{institution}_{base.replace('.mrc', '-marc21.parquet')}\")\n",
    "\n",
    "        # Skip if already processed unless force_reprocess is True\n",
    "        if not force_reprocess and os.path.exists(output_file):\n",
    "            skip_msg = f\"Skipping already processed {institution} file {file}\"\n",
    "            logger.info(skip_msg)\n",
    "            print(skip_msg)\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "            results.append(True)\n",
    "            continue\n",
    "            \n",
    "        result = process_file_with_recovery(file, institution)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result:\n",
    "            institution_summary[institution][\"success\"] += 1\n",
    "        else:\n",
    "            institution_summary[institution][\"failed\"] += 1\n",
    "    \n",
    "    # Print summary by institution\n",
    "    print(\"\\n=== Institution Processing Summary ===\")\n",
    "    for institution, stats in institution_summary.items():\n",
    "        print(f\"{institution.upper()}: Processed {stats['total']} files - {stats['success']} succeeded, {stats['failed']} failed\")\n",
    "    \n",
    "    # Overall success rate\n",
    "    total_success = sum(results)\n",
    "    total_files = len(results)\n",
    "    if total_files > 0:\n",
    "        print(f\"\\nOverall: Successfully processed {total_success} of {total_files} files ({total_success/total_files*100:.1f}%)\")\n",
    "        return total_success == total_files\n",
    "    else:\n",
    "        print(\"\\nNo files were processed\")\n",
    "        return False\n",
    "\n",
    "# Check if conversion is needed or if we can skip directly to processing\n",
    "print(\"Checking for existing parquet files...\")\n",
    "existing_parquet = glob.glob(f\"{output_dir}/*_marc21.parquet\")\n",
    "if existing_parquet:\n",
    "    print(f\"Found {len(existing_parquet)} existing parquet files\")\n",
    "    print(\"You can skip to the next cell unless you want to reprocess\")\n",
    "else:\n",
    "    print(\"No parquet files found. Running conversion...\")\n",
    "    marc2parquet_institution_specific()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Processing with Memory-Optimized Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before re-processing\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING MAIN PROCESSING ===\n",
      "This will process all institution parquet files and create the exploded dataset\n",
      "\n",
      "Found 13 institution parquet files to process\n",
      "\n",
      "Processing chicago...\n",
      "  - Records: 12,294,163\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/chicago_processed.parquet\n",
      "\n",
      "Processing brown...\n",
      "  - Records: 737,290\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/brown_processed.parquet\n",
      "\n",
      "Processing columbia...\n",
      "  - Records: 16,836,893\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/columbia_processed.parquet\n",
      "\n",
      "Processing cornell...\n",
      "  - Records: 6,944,453\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/cornell_processed.parquet\n",
      "\n",
      "Processing dartmouth...\n",
      "  - Records: 3,855,421\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/dartmouth_processed.parquet\n",
      "\n",
      "Processing duke...\n",
      "  - Records: 10,549,680\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/duke_processed.parquet\n",
      "\n",
      "Processing harvard...\n",
      "  - Records: 54,109,207\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/harvard_processed.parquet\n",
      "\n",
      "Processing johns...\n",
      "  - Records: 3,416,688\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/johns_processed.parquet\n",
      "\n",
      "Processing mit...\n",
      "  - Records: 5,338,129\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/mit_processed.parquet\n",
      "\n",
      "Processing penn...\n",
      "  - Records: 3,663,990\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/penn_processed.parquet\n",
      "\n",
      "Processing princeton...\n",
      "  - Records: 30,228,583\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/princeton_processed.parquet\n",
      "\n",
      "Processing stanford...\n",
      "  - Records: 976,982\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/stanford_processed.parquet\n",
      "\n",
      "Processing yale...\n",
      "  - Records: 7,431,259\n",
      "  ✅ Saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/temp_processed/yale_processed.parquet\n",
      "\n",
      "✅ Processed 13 institutions\n",
      "\n",
      "=== CREATING EXPLODED DATASET ===\n",
      "This creates a row for each identifier/key in each record...\n",
      "\n",
      "Checking id_list population...\n",
      "  - Average keys per record: 5.32\n",
      "  - Records with empty id_list: 0\n",
      "  - Total records: 156,382,738\n",
      "\n",
      "✅ Created exploded dataset with 831,355,950 rows\n",
      "\n",
      "Key type distribution:\n",
      "+--------+---------+\n",
      "|key_type|    count|\n",
      "+--------+---------+\n",
      "|MatchKey|469063500|\n",
      "|    OCLC|205445076|\n",
      "|    ISBN|115244429|\n",
      "|    LCCN| 41563700|\n",
      "|   Other|    39245|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Save the exploded dataset\u001b[39;00m\n\u001b[1;32m    102\u001b[0m exploded_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all_records_exploded.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mall_df_exploded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexploded_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Saved exploded dataset to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexploded_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis file will be used for all subsequent analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Processing - Memory-Optimized with Batch Processing\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=== STARTING MAIN PROCESSING ===\")\n",
    "print(\"This will process all institution parquet files and create the exploded dataset\\n\")\n",
    "\n",
    "# Get all institution parquet files\n",
    "parquet_files = glob.glob(f\"{output_dir}/*.parquet\")\n",
    "print(f\"Found {len(parquet_files)} institution parquet files to process\")\n",
    "\n",
    "# Process each institution and save to temp directory\n",
    "temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "processed_institutions = []\n",
    "\n",
    "for file_path in parquet_files:\n",
    "    # Extract institution name from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    institution = filename.split('_')[0]\n",
    "    \n",
    "    print(f\"\\nProcessing {institution}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read institution data\n",
    "        df = spark.read.parquet(file_path)\n",
    "        record_count = df.count()\n",
    "        print(f\"  - Records: {record_count:,}\")\n",
    "        \n",
    "        # Apply all enhanced processing\n",
    "        processed_df = process_institution_optimized(df, institution)\n",
    "        \n",
    "        # Save processed data\n",
    "        temp_path = f\"{temp_output_dir}/{institution}_processed.parquet\"\n",
    "        processed_df.write.mode(\"overwrite\").parquet(temp_path)\n",
    "        \n",
    "        processed_institutions.append((institution, temp_path))\n",
    "        print(f\"  ✅ Saved to {temp_path}\")\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {institution}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Processed {len(processed_institutions)} institutions\")\n",
    "\n",
    "# Now create the exploded dataset by reading all processed files\n",
    "print(\"\\n=== CREATING EXPLODED DATASET ===\")\n",
    "print(\"This creates a row for each identifier/key in each record...\")\n",
    "\n",
    "# Read all processed institution files\n",
    "processed_paths = [path for _, path in processed_institutions]\n",
    "all_df = spark.read.parquet(*processed_paths)\n",
    "\n",
    "# Check id_list population before exploding\n",
    "print(\"\\nChecking id_list population...\")\n",
    "id_list_stats = all_df.select(\n",
    "    F.avg(F.size(\"id_list\")).alias(\"avg_keys_per_record\"),\n",
    "    F.sum(F.when(F.size(\"id_list\") == 0, 1).otherwise(0)).alias(\"empty_id_lists\"),\n",
    "    F.count(\"*\").alias(\"total_records\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  - Average keys per record: {id_list_stats['avg_keys_per_record']:.2f}\")\n",
    "print(f\"  - Records with empty id_list: {id_list_stats['empty_id_lists']:,}\")\n",
    "print(f\"  - Total records: {id_list_stats['total_records']:,}\")\n",
    "\n",
    "if id_list_stats['avg_keys_per_record'] == 0:\n",
    "    print(\"\\n⚠️  WARNING: All id_lists are empty! Check the add_id_list_spark_enhanced function\")\n",
    "    print(\"The analysis will not work correctly with empty id_lists\")\n",
    "\n",
    "# Create exploded dataset with id_list as key_array\n",
    "all_df_with_key_array = all_df.withColumn(\"key_array\", F.col(\"id_list\"))\n",
    "\n",
    "# Explode the key_array to create one row per key\n",
    "all_df_exploded = all_df_with_key_array.select(\n",
    "    \"F001\", \"source\", \"match_key\", \"is_valid_match_key\",\n",
    "    F.explode(\"key_array\").alias(\"key\")\n",
    ").filter(F.col(\"key\").isNotNull())\n",
    "\n",
    "# Check exploded dataset\n",
    "exploded_count = all_df_exploded.count()\n",
    "print(f\"\\n✅ Created exploded dataset with {exploded_count:,} rows\")\n",
    "\n",
    "# Check key distribution\n",
    "key_type_stats = all_df_exploded.withColumn(\"key_type\",\n",
    "    F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), \"ISBN\")\n",
    "    .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), \"OCLC\")\n",
    "    .when(F.col(\"key\").rlike(\"^[a-zA-Z0-9]+$\") & ~F.col(\"key\").contains(\"_\"), \"LCCN\")\n",
    "    .when(F.col(\"key\").contains(\"_\"), \"MatchKey\")\n",
    "    .otherwise(\"Other\")\n",
    ").groupBy(\"key_type\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "print(\"\\nKey type distribution:\")\n",
    "key_type_stats.show()\n",
    "\n",
    "# Save the exploded dataset\n",
    "# exploded_path = f\"{output_dir}/all_records_exploded.parquet\"\n",
    "# all_df_exploded.write.mode(\"overwrite\").parquet(exploded_path)\n",
    "\n",
    "# Just cache it in memory instead\n",
    "all_df_exploded.cache()\n",
    "print(f\"\\n✅ Created exploded dataset with {exploded_count:,} rows\")\n",
    "print(f\"📌 Dataset cached in memory for subsequent analysis\")\n",
    "\n",
    "\n",
    "# Clean up temporary files if desired\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_output_dir)\n",
    "# print(f\"\\n🧹 Cleaned up temporary files in {temp_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached exploded dataset from memory\n",
      "Calculating statistics...\n",
      "\n",
      "=== Analysis Results ===\n",
      "Total Penn records: 2,443,080\n",
      "Unique Penn records: 1,559,207\n",
      "Uniqueness rate: 63.8%\n",
      "Overlap rate: 36.2%\n",
      "\n",
      "=== Overlap Analysis ===\n",
      "Distribution of Penn records by number of libraries holding them:\n",
      "+-------------+-------+\n",
      "|num_libraries|  count|\n",
      "+-------------+-------+\n",
      "|            1|4115630|\n",
      "|            2|2185102|\n",
      "|            3|1253813|\n",
      "|            4| 773343|\n",
      "|            5| 591346|\n",
      "|            6| 477994|\n",
      "|            7| 395741|\n",
      "|            8| 334991|\n",
      "|            9| 294822|\n",
      "|           10| 250606|\n",
      "|           11| 163279|\n",
      "|           12|  45234|\n",
      "|           13|   4739|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uniqueness Analysis and Overlap Detection\n",
    "from pyspark.sql.functions import collect_set, array_contains, size, col\n",
    "import pyspark.sql.functions as F  # Add this for F.count()\n",
    "import glob  # Add this import for the fallback logic\n",
    "import os  # Add this import for os.path.exists()\n",
    "\n",
    "# Temporarily disable broadcast joins to prevent timeout errors\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Check if exploded dataset is already in memory from previous cell\n",
    "if 'all_df_exploded' not in locals():\n",
    "    # Try to recreate from temp_processed files\n",
    "    temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "    \n",
    "    if os.path.exists(temp_output_dir):\n",
    "        print(\"Recreating exploded dataset from temp_processed files...\")\n",
    "        \n",
    "        # Get all processed institution files\n",
    "        processed_paths = glob.glob(f\"{temp_output_dir}/*_processed.parquet\")\n",
    "        \n",
    "        if processed_paths:\n",
    "            # Read all processed institution files\n",
    "            all_df = spark.read.parquet(*processed_paths)\n",
    "            \n",
    "            # Recreate the exploded dataset\n",
    "            all_df_with_key_array = all_df.withColumn(\"key_array\", F.col(\"id_list\"))\n",
    "            \n",
    "            all_df_exploded = all_df_with_key_array.select(\n",
    "                \"F001\", \"source\", \"match_key\", \"is_valid_match_key\",\n",
    "                F.explode(\"key_array\").alias(\"key\")\n",
    "            ).filter(F.col(\"key\").isNotNull())\n",
    "            \n",
    "            # Cache it for performance\n",
    "            all_df_exploded.cache()\n",
    "            print(f\"✅ Recreated exploded dataset from {len(processed_paths)} institution files\")\n",
    "        else:\n",
    "            raise ValueError(\"No processed files found in temp directory! Please run the Main Processing cell first.\")\n",
    "    else:\n",
    "        raise ValueError(\"Temp processed directory not found! Please run the Main Processing cell first.\")\n",
    "else:\n",
    "    print(\"Using cached exploded dataset from memory\")\n",
    "\n",
    "# Group by key and collect sources where that key appears\n",
    "grouped = all_df_exploded.groupBy(\"key\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# IMPORTANT: Don't broadcast the grouped DataFrame - it's too large\n",
    "# Instead, we'll use a regular join which Spark will optimize automatically\n",
    "\n",
    "# Find Penn records that exist in OTHER libraries\n",
    "# A Penn record is NOT unique if it exists in ANY other library\n",
    "penn_keys_in_other_libs = grouped.filter(\n",
    "    (array_contains(col(\"sources\"), \"penn\")) & \n",
    "    (F.size(col(\"sources\")) > 1)  # Penn + at least one other library\n",
    ").select(\"key\")\n",
    "\n",
    "# Get Penn records that are truly unique to Penn\n",
    "# Start with all Penn records\n",
    "all_penn_exploded = all_df_exploded.filter(col(\"source\") == \"penn\")\n",
    "\n",
    "# Anti-join to remove Penn records found in other libraries\n",
    "# Let Spark decide the join strategy based on data size\n",
    "unique_penn_exploded = all_penn_exploded.join(\n",
    "    penn_keys_in_other_libs,\n",
    "    on=\"key\", \n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Deduplicate by Penn's F001 (not match_key) to get unique Penn records\n",
    "unique_penn = unique_penn_exploded.drop(\"key\").dropDuplicates([\"F001\"])\n",
    "\n",
    "# Cache the unique Penn records for better performance\n",
    "unique_penn.cache()\n",
    "\n",
    "# Calculate statistics efficiently\n",
    "print(\"Calculating statistics...\")\n",
    "unique_penn_count = unique_penn.count()  # Force cache materialization\n",
    "\n",
    "# Get total Penn records from the deduplicated exploded DataFrame\n",
    "total_penn = all_penn_exploded.select(\"F001\").distinct().count()\n",
    "\n",
    "print(f\"\\n=== Analysis Results ===\")\n",
    "print(f\"Total Penn records: {total_penn:,}\")\n",
    "print(f\"Unique Penn records: {unique_penn_count:,}\")\n",
    "\n",
    "# Add robust checking for division by zero\n",
    "if total_penn > 0:\n",
    "    print(f\"Uniqueness rate: {unique_penn_count/total_penn*100:.1f}%\")\n",
    "    print(f\"Overlap rate: {(total_penn - unique_penn_count)/total_penn*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Uniqueness rate: N/A (no Penn records found)\")\n",
    "\n",
    "# For analysis, let's also see overlap statistics\n",
    "print(\"\\n=== Overlap Analysis ===\")\n",
    "\n",
    "# More efficient: get Penn overlap stats without re-filtering\n",
    "penn_keys = grouped.filter(array_contains(col(\"sources\"), \"penn\")).cache()\n",
    "\n",
    "penn_overlap_stats = penn_keys \\\n",
    "    .withColumn(\"num_libraries\", F.size(col(\"sources\"))) \\\n",
    "    .groupBy(\"num_libraries\").count() \\\n",
    "    .orderBy(\"num_libraries\")\n",
    "\n",
    "print(\"Distribution of Penn records by number of libraries holding them:\")\n",
    "penn_overlap_stats.show()\n",
    "\n",
    "# Save results with consistent paths using pod-processing-outputs directory\n",
    "output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Save unique Penn records\n",
    "unique_penn.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "\n",
    "# Save detailed overlap information for analysis\n",
    "# Note: Using cached penn_keys for efficiency\n",
    "penn_with_overlap_info = all_penn_exploded.join(\n",
    "    penn_keys.select(\"key\", \"sources\", F.size(\"sources\").alias(\"num_libraries\")),\n",
    "    on=\"key\",\n",
    "    how=\"left\"\n",
    ").drop(\"key\")\n",
    "\n",
    "penn_with_overlap_info.write.mode(\"overwrite\").parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "\n",
    "# Load all_df from intermediate files for validation statistics\n",
    "# Check if processed_institutions variable exists from previous cell\n",
    "if 'processed_institutions' in locals():\n",
    "    # Use the paths from the previous processing\n",
    "    processed_paths = [path for _, path in processed_institutions]\n",
    "    all_df = spark.read.parquet(*processed_paths)\n",
    "else:\n",
    "    # Fallback: read from temp directory\n",
    "    temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "    processed_paths = glob.glob(f\"{temp_output_dir}/*_processed.parquet\")\n",
    "    if processed_paths:\n",
    "        all_df = spark.read.parquet(*processed_paths)\n",
    "    else:\n",
    "        print(\"WARNING: Could not load all_df for validation statistics\")\n",
    "        print(\"Skipping validation statistics save\")\n",
    "        all_df = None\n",
    "\n",
    "# Save validation statistics for analysis if all_df is available\n",
    "if all_df is not None:\n",
    "    validation_stats = all_df.select(\"F001\", \"match_key\", \"is_valid_match_key\", \"match_key_message\", \"id_list\") \\\n",
    "        .filter(col(\"source\") == \"penn\")\n",
    "    \n",
    "    validation_stats.write.mode(\"overwrite\").parquet(f\"{output_dir}/match_key_validation_stats.parquet\")\n",
    "else:\n",
    "    print(\"Validation statistics not saved due to missing all_df\")\n",
    "\n",
    "# Unpersist cached DataFrames to free memory\n",
    "penn_keys.unpersist()\n",
    "unique_penn.unpersist()\n",
    "\n",
    "# Re-enable broadcast joins for subsequent operations\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"30m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONSERVATIVE UNIQUENESS ANALYSIS ===\n",
      "Applying stricter criteria to identify truly unique records\n",
      "\n",
      "1. Records with multiple types of unique identifiers:\n",
      "  - Penn records unique by 2+ identifier types: 992,367\n",
      "\n",
      "2. Uniqueness by identifier type:\n",
      "  - Unique Penn records with ISBN: 594,158\n",
      "  - Unique Penn records with OCLC: 883,744\n",
      "  - Unique Penn records with LCCN: 189,183\n",
      "  - Unique Penn records with match key: 1,559,207\n",
      "\n",
      "3. Conservative uniqueness estimates:\n",
      "\n",
      "Conservative uniqueness (standard identifiers only):\n",
      "  - Total Penn records: 2,443,080\n",
      "  - Conservative unique count: 992,230\n",
      "  - Conservative uniqueness rate: 40.6%\n",
      "\n",
      "4. High-confidence unique records:\n",
      "  - High-confidence unique (multiple identifiers): 992,367\n",
      "  - High-confidence uniqueness rate: 40.6%\n",
      "\n",
      "=== UNIQUENESS SUMMARY ===\n",
      "Original unique records: 1,559,207 (63.8%)\n",
      "Conservative unique records: 992,230 (40.6%)\n",
      "High-confidence unique records: 992,367 (40.6%)\n",
      "\n",
      "Difference: 566,977 records (23.2%)\n",
      "These are records unique only by match keys, which may be less reliable\n",
      "\n",
      "✅ Conservative analysis complete!\n",
      "Results saved to: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/conservative_unique_penn.parquet\n"
     ]
    }
   ],
   "source": [
    "# Conservative Uniqueness Filtering - Alternative Analysis\n",
    "# This provides a more conservative estimate of uniqueness by applying stricter criteria\n",
    "\n",
    "from pyspark.sql.functions import col, size, array_contains, collect_set, count, when\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=== CONSERVATIVE UNIQUENESS ANALYSIS ===\")\n",
    "print(\"Applying stricter criteria to identify truly unique records\\n\")\n",
    "\n",
    "# Load the Penn overlap analysis data\n",
    "penn_overlap = spark.read.parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "\n",
    "# Get the baseline unique Penn count if not already in memory\n",
    "if 'unique_penn_count' not in locals():\n",
    "    print(\"Loading baseline unique Penn statistics...\")\n",
    "    unique_penn_df = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "    unique_penn_count = unique_penn_df.count()\n",
    "    print(f\"Baseline unique Penn records: {unique_penn_count:,}\\n\")\n",
    "\n",
    "\n",
    "# Check if we need to reload the exploded dataset\n",
    "if 'all_df_exploded' not in locals():\n",
    "    # Try to recreate from temp_processed files\n",
    "    temp_output_dir = f\"{output_dir}/temp_processed\"\n",
    "    \n",
    "    if os.path.exists(temp_output_dir):\n",
    "        print(\"Recreating exploded dataset from temp_processed files...\")\n",
    "        \n",
    "        # Get all processed institution files\n",
    "        processed_paths = glob.glob(f\"{temp_output_dir}/*_processed.parquet\")\n",
    "        \n",
    "        if processed_paths:\n",
    "            # Read all processed institution files\n",
    "            all_df = spark.read.parquet(*processed_paths)\n",
    "            \n",
    "            # Recreate the exploded dataset\n",
    "            all_df_with_key_array = all_df.withColumn(\"key_array\", F.col(\"id_list\"))\n",
    "            \n",
    "            all_df_exploded = all_df_with_key_array.select(\n",
    "                \"F001\", \"source\", \"match_key\", \"is_valid_match_key\",\n",
    "                F.explode(\"key_array\").alias(\"key\")\n",
    "            ).filter(F.col(\"key\").isNotNull())\n",
    "            \n",
    "            # Cache it for performance\n",
    "            all_df_exploded.cache()\n",
    "            print(f\"✅ Recreated exploded dataset from {len(processed_paths)} institution files\")\n",
    "\n",
    "# Re-create grouped DataFrame with match key type information\n",
    "grouped_with_type = all_df_exploded.withColumn(\"key_type\",\n",
    "    F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), \"ISBN\")\n",
    "    .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), \"OCLC\")\n",
    "    .when(F.col(\"key\").rlike(\"^[a-zA-Z0-9]+$\") & ~F.col(\"key\").contains(\"_\"), \"LCCN\")\n",
    "    .when(F.col(\"key\").contains(\"_\"), \"MatchKey\")\n",
    "    .otherwise(\"Other\")\n",
    ").groupBy(\"key\", \"key_type\").agg(\n",
    "    collect_set(\"source\").alias(\"sources\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "# ANALYSIS 1: Records unique by multiple identifier types\n",
    "print(\"1. Records with multiple types of unique identifiers:\")\n",
    "\n",
    "# Get Penn records that are UNIQUE to Penn first (from the overlap analysis)\n",
    "unique_penn_f001s = penn_overlap.filter(col(\"num_libraries\") == 1).select(\"F001\")\n",
    "\n",
    "# Get key types for unique Penn records only\n",
    "penn_key_types_unique = all_df_exploded.filter(col(\"source\") == \"penn\") \\\n",
    "    .join(unique_penn_f001s, on=\"F001\", how=\"inner\") \\\n",
    "    .withColumn(\"key_type\",\n",
    "        F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), \"ISBN\")\n",
    "        .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), \"OCLC\")\n",
    "        .when(F.col(\"key\").rlike(\"^[a-zA-Z0-9]+$\") & ~F.col(\"key\").contains(\"_\"), \"LCCN\")\n",
    "        .when(F.col(\"key\").contains(\"_\"), \"MatchKey\")\n",
    "        .otherwise(\"Other\")\n",
    "    ) \\\n",
    "    .groupBy(\"F001\") \\\n",
    "    .agg(\n",
    "        collect_set(\"key_type\").alias(\"identifier_types\"),\n",
    "        count(\"key\").alias(\"total_keys\")\n",
    "    )\n",
    "\n",
    "# Now filter for those with multiple identifier types\n",
    "unique_by_multiple = penn_key_types_unique \\\n",
    "    .filter(size(col(\"identifier_types\")) >= 2)  # Has at least 2 different types\n",
    "\n",
    "unique_by_multiple_count = unique_by_multiple.count()\n",
    "print(f\"  - Penn records unique by 2+ identifier types: {unique_by_multiple_count:,}\")\n",
    "\n",
    "# ANALYSIS 2: Records with standard identifiers (more reliable)\n",
    "print(\"\\n2. Uniqueness by identifier type:\")\n",
    "\n",
    "# For each unique Penn record, check what types of identifiers it has\n",
    "unique_penn_with_id_types = all_df_exploded.filter(col(\"source\") == \"penn\") \\\n",
    "    .join(unique_penn_f001s, on=\"F001\", how=\"inner\") \\\n",
    "    .withColumn(\"id_type\",\n",
    "        F.when(F.col(\"key\").rlike(\"^[0-9X]{10,13}$\"), \"ISBN\")\n",
    "        .when(F.col(\"key\").rlike(\"^[0-9]{8,}$\"), \"OCLC\")\n",
    "        .when(F.col(\"key\").rlike(\"^[a-zA-Z0-9]+$\") & ~F.col(\"key\").contains(\"_\"), \"LCCN\")\n",
    "        .when(F.col(\"key\").contains(\"_\"), \"MatchKey\")\n",
    "        .otherwise(\"Other\")\n",
    "    ) \\\n",
    "    .groupBy(\"F001\").agg(\n",
    "        F.collect_set(\"id_type\").alias(\"id_types\")\n",
    "    )\n",
    "\n",
    "# Count records by identifier type presence\n",
    "unique_with_isbn = unique_penn_with_id_types.filter(F.array_contains(col(\"id_types\"), \"ISBN\")).count()\n",
    "unique_with_oclc = unique_penn_with_id_types.filter(F.array_contains(col(\"id_types\"), \"OCLC\")).count()\n",
    "unique_with_lccn = unique_penn_with_id_types.filter(F.array_contains(col(\"id_types\"), \"LCCN\")).count()\n",
    "unique_with_matchkey = unique_penn_with_id_types.filter(F.array_contains(col(\"id_types\"), \"MatchKey\")).count()\n",
    "\n",
    "print(f\"  - Unique Penn records with ISBN: {unique_with_isbn:,}\")\n",
    "print(f\"  - Unique Penn records with OCLC: {unique_with_oclc:,}\")\n",
    "print(f\"  - Unique Penn records with LCCN: {unique_with_lccn:,}\")\n",
    "print(f\"  - Unique Penn records with match key: {unique_with_matchkey:,}\")\n",
    "\n",
    "# ANALYSIS 3: Conservative estimate - must have at least one standard identifier\n",
    "print(\"\\n3. Conservative uniqueness estimates:\")\n",
    "\n",
    "# Records that have at least one standard identifier (not just match keys)\n",
    "conservative_unique = unique_penn_with_id_types.filter(\n",
    "    F.array_contains(col(\"id_types\"), \"ISBN\") |\n",
    "    F.array_contains(col(\"id_types\"), \"OCLC\") |\n",
    "    F.array_contains(col(\"id_types\"), \"LCCN\")\n",
    ")\n",
    "\n",
    "conservative_unique_count = conservative_unique.count()\n",
    "\n",
    "print(f\"\\nConservative uniqueness (standard identifiers only):\")\n",
    "print(f\"  - Total Penn records: {total_penn:,}\")\n",
    "print(f\"  - Conservative unique count: {conservative_unique_count:,}\")\n",
    "print(f\"  - Conservative uniqueness rate: {conservative_unique_count/total_penn*100:.1f}%\")\n",
    "\n",
    "# ANALYSIS 4: High-confidence unique records\n",
    "print(\"\\n4. High-confidence unique records:\")\n",
    "\n",
    "# Records that are unique AND have been verified by multiple match methods\n",
    "# Use penn_key_types_unique from ANALYSIS 1\n",
    "high_confidence = penn_key_types_unique \\\n",
    "    .filter(\n",
    "        (size(col(\"identifier_types\")) >= 2) &  # Multiple identifier types\n",
    "        (col(\"total_keys\") >= 3)  # At least 3 different keys\n",
    "    )\n",
    "\n",
    "high_confidence_count = high_confidence.count()\n",
    "print(f\"  - High-confidence unique (multiple identifiers): {high_confidence_count:,}\")\n",
    "print(f\"  - High-confidence uniqueness rate: {high_confidence_count/total_penn*100:.1f}%\")\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\n=== UNIQUENESS SUMMARY ===\")\n",
    "print(f\"Original unique records: {unique_penn_count:,} ({unique_penn_count/total_penn*100:.1f}%)\")\n",
    "print(f\"Conservative unique records: {conservative_unique_count:,} ({conservative_unique_count/total_penn*100:.1f}%)\")\n",
    "print(f\"High-confidence unique records: {high_confidence_count:,} ({high_confidence_count/total_penn*100:.1f}%)\")\n",
    "\n",
    "# Calculate the difference\n",
    "difference = unique_penn_count - conservative_unique_count\n",
    "print(f\"\\nDifference: {difference:,} records ({difference/total_penn*100:.1f}%)\")\n",
    "print(\"These are records unique only by match keys, which may be less reliable\")\n",
    "\n",
    "# Save conservative results\n",
    "conservative_unique.write.mode(\"overwrite\").parquet(f\"{output_dir}/conservative_unique_penn.parquet\")\n",
    "\n",
    "print(f\"\\n✅ Conservative analysis complete!\")\n",
    "print(f\"Results saved to: {output_dir}/conservative_unique_penn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADDITIONAL FILTERING FOR ACCURATE UNIQUENESS ===\n",
      "Applying ISBN deduplication and reproduction filtering\n",
      "\n",
      "📋 STEP 1: ISBN DEDUPLICATION\n",
      "When multiple records share the same ISBN, keeping only one...\n",
      "  - Records before ISBN deduplication: 992,230\n",
      "  - Records after ISBN deduplication: 952,277\n",
      "  - Removed by ISBN deduplication: 39,953\n",
      "\n",
      "📋 STEP 2: REMOVING REPRODUCTIONS (F533 FIELD)\n",
      "Filtering out records with reproduction notes...\n",
      "  - Records before F533 filter: 952,277\n",
      "  - Records after F533 filter: 903,858\n",
      "  - Removed by F533 filter: 48,419\n",
      "\n",
      "📋 STEP 3: REMOVING HSP (HISTORICAL SOCIETY OF PENNSYLVANIA) RECORDS\n",
      "Loading HSP exclusion list from file...\n",
      "  - Loaded 189,793 HSP F001 values from file\n",
      "  - Records before HSP filter: 903,858\n",
      "  - Records after HSP filter: 810,502\n",
      "  - Removed by HSP filter: 93,356\n",
      "\n",
      "=== FINAL FILTERED UNIQUENESS SUMMARY ===\n",
      "Original uniqueness count: 1,337,666\n",
      "Conservative (standard IDs only): 992,230\n",
      "After ISBN deduplication: 952,277\n",
      "After removing reproductions: 903,858\n",
      "After removing HSP records: 810,502\n",
      "\n",
      "Final uniqueness rate: 33.2%\n",
      "Total records filtered out: 181,728\n",
      "\n",
      "✅ Additional filtering complete!\n",
      "Final filtered results saved to: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/conservative_unique_penn_filtered.parquet\n"
     ]
    }
   ],
   "source": [
    "# Additional Filtering - ISBN Deduplication and Reproduction Removal\n",
    "from pyspark.sql.functions import col, when, size, array_contains, collect_set, min\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=== ADDITIONAL FILTERING FOR ACCURATE UNIQUENESS ===\")\n",
    "print(\"Applying ISBN deduplication and reproduction filtering\\n\")\n",
    "\n",
    "# Load the conservative unique records if not already loaded\n",
    "if 'conservative_unique' not in locals():\n",
    "    conservative_unique = spark.read.parquet(f\"{output_dir}/conservative_unique_penn.parquet\")\n",
    "\n",
    "# Get the count if not already available\n",
    "if 'conservative_unique_count' not in locals():\n",
    "    conservative_unique_count = conservative_unique.count()\n",
    "    print(f\"Conservative unique records loaded: {conservative_unique_count:,}\\n\")\n",
    "\n",
    "# Load the full Penn records with all fields\n",
    "penn_full = spark.read.parquet(f\"{input_dir}/pod-processing-outputs/penn_penn_filtered-marc21.parquet\")\n",
    "\n",
    "# IMPORTANT: Ensure we only get unique F001s from penn_full to avoid duplicates\n",
    "penn_full_unique = penn_full.dropDuplicates([\"F001\"])\n",
    "\n",
    "# Join to get full records for conservative unique items\n",
    "conservative_unique_full = conservative_unique.join(penn_full_unique, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Verify the join didn't create duplicates\n",
    "joined_count = conservative_unique_full.count()\n",
    "if joined_count != conservative_unique_count:\n",
    "    print(f\"⚠️  WARNING: Join created duplicates! Expected {conservative_unique_count:,}, got {joined_count:,}\")\n",
    "    print(\"Deduplicating by F001...\")\n",
    "    conservative_unique_full = conservative_unique_full.dropDuplicates([\"F001\"])\n",
    "    joined_count = conservative_unique_full.count()\n",
    "    print(f\"After deduplication: {joined_count:,} records\\n\")\n",
    "\n",
    "print(\"📋 STEP 1: ISBN DEDUPLICATION\")\n",
    "print(\"When multiple records share the same ISBN, keeping only one...\")\n",
    "\n",
    "# Extract clean ISBN from F020 field\n",
    "conservative_with_isbn = conservative_unique_full.withColumn(\"clean_isbn\",\n",
    "    F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0),\n",
    "        F.regexp_extract(\n",
    "            F.concat_ws(\" \", F.col(\"F020\")),\n",
    "            \"([0-9X]{10,13})\",\n",
    "            1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# For records with ISBNs, keep only one per ISBN (the one with lowest F001)\n",
    "isbn_dedupe = conservative_with_isbn.filter(col(\"clean_isbn\").isNotNull()) \\\n",
    "    .groupBy(\"clean_isbn\") \\\n",
    "    .agg(F.min(\"F001\").alias(\"F001_to_keep\"))\n",
    "\n",
    "# Keep records that either have no ISBN or are the chosen record for their ISBN\n",
    "conservative_isbn_deduped = conservative_with_isbn.alias(\"a\").join(\n",
    "    isbn_dedupe.alias(\"b\"),\n",
    "    (col(\"a.clean_isbn\") == col(\"b.clean_isbn\")) & (col(\"a.F001\") != col(\"b.F001_to_keep\")),\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "isbn_deduped_count = conservative_isbn_deduped.count()\n",
    "removed_by_isbn = joined_count - isbn_deduped_count\n",
    "\n",
    "print(f\"  - Records before ISBN deduplication: {joined_count:,}\")\n",
    "print(f\"  - Records after ISBN deduplication: {isbn_deduped_count:,}\")\n",
    "print(f\"  - Removed by ISBN deduplication: {removed_by_isbn:,}\")\n",
    "\n",
    "print(\"\\n📋 STEP 2: REMOVING REPRODUCTIONS (F533 FIELD)\")\n",
    "print(\"Filtering out records with reproduction notes...\")\n",
    "\n",
    "# Remove records with F533 (reproduction note)\n",
    "if \"F533\" in conservative_isbn_deduped.columns:\n",
    "    conservative_no_reproductions = conservative_isbn_deduped.filter(col(\"F533\").isNull())\n",
    "    no_repro_count = conservative_no_reproductions.count()\n",
    "    removed_by_f533 = isbn_deduped_count - no_repro_count\n",
    "    \n",
    "    print(f\"  - Records before F533 filter: {isbn_deduped_count:,}\")\n",
    "    print(f\"  - Records after F533 filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Removed by F533 filter: {removed_by_f533:,}\")\n",
    "else:\n",
    "    print(\"  - F533 field not found, skipping reproduction filter\")\n",
    "    conservative_no_reproductions = conservative_isbn_deduped\n",
    "    no_repro_count = isbn_deduped_count\n",
    "\n",
    "print(\"\\n📋 STEP 3: REMOVING HSP (HISTORICAL SOCIETY OF PENNSYLVANIA) RECORDS\")\n",
    "print(\"Loading HSP exclusion list from file...\")\n",
    "\n",
    "# Load HSP F001 values from text file\n",
    "hsp_file_path = f\"{input_dir}/hsp_removed_mmsid.txt\"\n",
    "try:\n",
    "    # Read the HSP F001 values from the text file\n",
    "    with open(hsp_file_path, 'r') as f:\n",
    "        hsp_f001_list = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"  - Loaded {len(hsp_f001_list):,} HSP F001 values from file\")\n",
    "    \n",
    "    # Convert to DataFrame for efficient joining\n",
    "    hsp_df = spark.createDataFrame([(f001,) for f001 in hsp_f001_list], [\"F001\"])\n",
    "    \n",
    "    # Remove HSP records using anti-join\n",
    "    conservative_no_hsp = conservative_no_reproductions.join(\n",
    "        hsp_df,\n",
    "        on=\"F001\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    \n",
    "    no_hsp_count = conservative_no_hsp.count()\n",
    "    removed_by_hsp = no_repro_count - no_hsp_count\n",
    "    \n",
    "    print(f\"  - Records before HSP filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Records after HSP filter: {no_hsp_count:,}\")\n",
    "    print(f\"  - Removed by HSP filter: {removed_by_hsp:,}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"  ⚠️  WARNING: HSP file not found at {hsp_file_path}\")\n",
    "    print(\"  Falling back to pattern-based HSP detection...\")\n",
    "    \n",
    "    # Fallback: Use pattern matching approach\n",
    "    hsp_patterns = [\n",
    "        \"HSP\",\n",
    "        \"Historical Society of Pennsylvania\",\n",
    "        \"Hist Soc Penn\",\n",
    "        \"Hist.Soc.Penn\"\n",
    "    ]\n",
    "    \n",
    "    # Create filter conditions for HSP detection\n",
    "    hsp_filter_conditions = F.lit(False)\n",
    "    \n",
    "    # Check F710 (corporate name added entry)\n",
    "    if \"F710\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F710\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Check F590 (local note)\n",
    "    if \"F590\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F590\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Check F500 (general note)\n",
    "    if \"F500\" in conservative_no_reproductions.columns:\n",
    "        for pattern in hsp_patterns:\n",
    "            hsp_filter_conditions = hsp_filter_conditions | \\\n",
    "                F.array_contains(F.transform(F.col(\"F500\"), lambda x: F.upper(x)), pattern.upper())\n",
    "    \n",
    "    # Apply HSP filter\n",
    "    conservative_no_hsp = conservative_no_reproductions.filter(~hsp_filter_conditions)\n",
    "    no_hsp_count = conservative_no_hsp.count()\n",
    "    removed_by_hsp = no_repro_count - no_hsp_count\n",
    "    \n",
    "    print(f\"  - Records before HSP filter: {no_repro_count:,}\")\n",
    "    print(f\"  - Records after HSP filter: {no_hsp_count:,}\")\n",
    "    print(f\"  - Removed by HSP filter: {removed_by_hsp:,}\")\n",
    "\n",
    "# Calculate final statistics\n",
    "print(\"\\n=== FINAL FILTERED UNIQUENESS SUMMARY ===\")\n",
    "print(f\"Original uniqueness count: {unique_penn_count:,}\")\n",
    "print(f\"Conservative (standard IDs only): {conservative_unique_count:,}\")\n",
    "print(f\"After ISBN deduplication: {isbn_deduped_count:,}\")\n",
    "print(f\"After removing reproductions: {no_repro_count:,}\")\n",
    "print(f\"After removing HSP records: {no_hsp_count:,}\")\n",
    "\n",
    "# Add check for total_penn\n",
    "if 'total_penn' in locals():\n",
    "    print(f\"\\nFinal uniqueness rate: {no_hsp_count/total_penn*100:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nFinal uniqueness rate: Unable to calculate (total_penn not available)\")\n",
    "\n",
    "print(f\"Total records filtered out: {conservative_unique_count - no_hsp_count:,}\")\n",
    "\n",
    "# Save the final filtered dataset\n",
    "conservative_no_hsp.select(\"F001\").write.mode(\"overwrite\").parquet(\n",
    "    f\"{output_dir}/conservative_unique_penn_filtered.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Additional filtering complete!\")\n",
    "print(f\"Final filtered results saved to: {output_dir}/conservative_unique_penn_filtered.parquet\")\n",
    "\n",
    "# Update the unique_penn variable for downstream processing\n",
    "unique_penn = conservative_no_hsp.select(\"F001\")\n",
    "unique_penn_count = no_hsp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADDITIONAL HSP DETECTION VIA F035 ===\n",
      "Checking F035 field for (hsp) identifier...\n",
      "\n",
      "Records before F035 HSP check: 810,502\n",
      "\n",
      "📋 F035 HSP Detection Results:\n",
      "  - Records with (hsp) in F035: 0\n",
      "  - Records after F035 filter: 810,502\n",
      "\n",
      "✅ No additional HSP records found in F035\n"
     ]
    }
   ],
   "source": [
    "# Additional HSP Detection - Check F035 field for (hsp)\n",
    "from pyspark.sql.functions import col, when, size, array_contains, lower\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=== ADDITIONAL HSP DETECTION VIA F035 ===\")\n",
    "print(\"Checking F035 field for (hsp) identifier...\\n\")\n",
    "\n",
    "# Load the filtered dataset if not in memory\n",
    "if 'conservative_no_hsp' not in locals():\n",
    "    print(\"Loading filtered dataset...\")\n",
    "    conservative_no_hsp = spark.read.parquet(f\"{output_dir}/conservative_unique_penn_filtered.parquet\")\n",
    "    \n",
    "    # Need to join with full records to get F035\n",
    "    penn_full = spark.read.parquet(f\"{input_dir}/pod-processing-outputs/penn_penn_filtered-marc21.parquet\")\n",
    "    penn_full_unique = penn_full.dropDuplicates([\"F001\"])\n",
    "    conservative_no_hsp = conservative_no_hsp.join(penn_full_unique, on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Get current count before additional filtering\n",
    "current_count = conservative_no_hsp.count()\n",
    "print(f\"Records before F035 HSP check: {current_count:,}\")\n",
    "\n",
    "# Check if F035 field exists\n",
    "if \"F035\" in conservative_no_hsp.columns:\n",
    "    # Filter out records where F035 contains (hsp) - case insensitive\n",
    "    additional_hsp_filter = conservative_no_hsp.filter(\n",
    "        F.col(\"F035\").isNull() | \n",
    "        ~F.array_contains(\n",
    "            F.transform(F.col(\"F035\"), lambda x: F.lower(x)), \n",
    "            \"(hsp)\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Count how many HSP records were found\n",
    "    after_f035_count = additional_hsp_filter.count()\n",
    "    removed_by_f035 = current_count - after_f035_count\n",
    "    \n",
    "    print(f\"\\n📋 F035 HSP Detection Results:\")\n",
    "    print(f\"  - Records with (hsp) in F035: {removed_by_f035:,}\")\n",
    "    print(f\"  - Records after F035 filter: {after_f035_count:,}\")\n",
    "    \n",
    "    # If we found additional HSP records, update the saved files\n",
    "    if removed_by_f035 > 0:\n",
    "        print(f\"\\n✅ Found and removed {removed_by_f035:,} additional HSP records!\")\n",
    "        \n",
    "        # Update the filtered dataset\n",
    "        additional_hsp_filter.select(\"F001\").write.mode(\"overwrite\").parquet(\n",
    "            f\"{output_dir}/conservative_unique_penn_filtered_no_f035_hsp.parquet\"\n",
    "        )\n",
    "        \n",
    "        # Update variables for downstream processing\n",
    "        conservative_no_hsp = additional_hsp_filter\n",
    "        no_hsp_count = after_f035_count\n",
    "        unique_penn = additional_hsp_filter.select(\"F001\")\n",
    "        unique_penn_count = no_hsp_count\n",
    "        \n",
    "        print(f\"\\n=== UPDATED FINAL STATISTICS ===\")\n",
    "        print(f\"Final unique Penn records (all HSP removed): {no_hsp_count:,}\")\n",
    "        if 'total_penn' in locals():\n",
    "            print(f\"Final uniqueness rate: {no_hsp_count/total_penn*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nUpdated results saved to: {output_dir}/conservative_unique_penn_filtered_no_f035_hsp.parquet\")\n",
    "    else:\n",
    "        print(\"\\n✅ No additional HSP records found in F035\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n⚠️  F035 field not found in dataset - cannot perform additional HSP check\")\n",
    "\n",
    "# Show a sample of F035 fields that contain (hsp) if any were found\n",
    "if \"F035\" in conservative_no_hsp.columns and removed_by_f035 > 0:\n",
    "    print(\"\\n📋 Sample F035 fields containing (hsp):\")\n",
    "    hsp_sample = conservative_no_hsp.filter(\n",
    "        F.array_contains(\n",
    "            F.transform(F.col(\"F035\"), lambda x: F.lower(x)), \n",
    "            \"(hsp)\"\n",
    "        )\n",
    "    ).select(\"F001\", \"F035\").limit(5)\n",
    "    \n",
    "    hsp_sample.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing unique_penn DataFrame\n",
      "Working with 810,502 filtered unique Penn records\n",
      "\n",
      "=== PENN DATA SOURCE VERIFICATION ===\n",
      "\n",
      "Found Penn records at: /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/penn_penn_filtered-marc21.parquet\n",
      "  - Source type: Processed updates\n",
      "  - Data date: unknown\n",
      "  - Total records: 3,663,990\n",
      "  - Recently updated records (2024-2025): 609,205 (16.6%)\n",
      "\n",
      "=== PROCEEDING WITH ANALYSIS ===\n",
      "\n",
      "=== Pre-Join Dataset Verification ===\n",
      "Penn full dataset columns (216 total):\n",
      "Sample columns: FLDR, F001, F003, F005, F006, F007, F008, F010, F013, F015...\n",
      "\n",
      "=== Checking for duplicate F001s in penn_full ===\n",
      "⚠️  WARNING: Found 739,289 F001 values with duplicates in penn_full\n",
      "Deduplicating penn_full by F001...\n",
      "\n",
      "Sample of duplicate F001s:\n",
      "+----------------+-----+\n",
      "|            F001|count|\n",
      "+----------------+-----+\n",
      "|   9955913503681|  400|\n",
      "|   9973703503681|  202|\n",
      "|9915957333503681|  200|\n",
      "|   9972463503681|  118|\n",
      "| 991853173503681|  106|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "After deduplication: 2,443,080 records\n",
      "\n",
      "Joining 810,502 unique Penn F001s with full Penn records...\n",
      "Joined result: 810,502 records\n",
      "✅ Join count matches expected - all filtered records found in penn_full\n",
      "\n",
      "=== Post-Join Dataset Verification ===\n",
      "Joined dataset columns (216 total):\n",
      "Sample columns: F001, FLDR, F003, F005, F006, F007, F008, F010, F013, F015...\n",
      "\n",
      "=== Checking available columns for filtering ===\n",
      "Looking for F533 column to filter reproduction notes...\n",
      "Filtering out records with F533 (reproduction notes)\n",
      "\n",
      "=== Material Type Distribution ===\n",
      "print_book: 697,799\n",
      "other: 38,909\n",
      "visual_material: 37,367\n",
      "print_serial: 22,719\n",
      "print_music: 11,382\n",
      "audio_material: 1,013\n",
      "print_maps: 903\n",
      "electronic_resource: 410\n",
      "\n",
      "=== Print Material Analysis ===\n",
      "Total unique Penn records: 810,502\n",
      "Print materials: 732,803 (90.4%)\n",
      "Non-print materials: 77,699 (9.6%)\n",
      "\n",
      "=== Print Material Categories ===\n",
      "print_book: 697,799 (95.2% of print materials)\n",
      "print_serial: 22,719 (3.1% of print materials)\n",
      "print_music: 11,382 (1.6% of print materials)\n",
      "print_maps: 903 (0.1% of print materials)\n"
     ]
    }
   ],
   "source": [
    "# Data Source Validation (Updated: July 2025)\n",
    "# Validates Penn MARC data sources and ensures current data is used\n",
    "# Requires explicit confirmation for legacy data usage\n",
    "\n",
    "# Use Leader field FLDR to make a print set from unique penn and non-print\n",
    "from pyspark.sql.functions import col, substring, when, concat, lit\n",
    "import pyspark.sql.functions as F\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load the unique Penn dataset if not already loaded - UPDATED TO USE FILTERED DATASET\n",
    "if 'unique_penn' not in locals() or unique_penn is None:\n",
    "    print(\"Loading conservatively filtered unique Penn dataset...\")\n",
    "    # Load from the most filtered dataset (after HSP and F035 filtering)\n",
    "    if os.path.exists(f\"{output_dir}/conservative_unique_penn_filtered_no_f035_hsp.parquet\"):\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/conservative_unique_penn_filtered_no_f035_hsp.parquet\")\n",
    "        print(\"✅ Loaded conservative_unique_penn_filtered_no_f035_hsp.parquet\")\n",
    "    elif os.path.exists(f\"{output_dir}/conservative_unique_penn_filtered.parquet\"):\n",
    "        print(\"Note: Using filtered dataset without F035 HSP check\")\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/conservative_unique_penn_filtered.parquet\")\n",
    "    else:\n",
    "        print(\"Warning: Filtered datasets not found, falling back to conservative unique dataset\")\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/conservative_unique_penn.parquet\")\n",
    "    \n",
    "    # Get count for later statistics\n",
    "    if 'unique_penn_count' not in locals():\n",
    "        unique_penn_count = unique_penn.count()\n",
    "        print(f\"Loaded {unique_penn_count:,} filtered unique Penn records\")\n",
    "else:\n",
    "    print(\"Using existing unique_penn DataFrame\")\n",
    "    # Ensure we have the count\n",
    "    if 'unique_penn_count' not in locals():\n",
    "        unique_penn_count = unique_penn.count()\n",
    "\n",
    "print(f\"Working with {unique_penn_count:,} filtered unique Penn records\")\n",
    "\n",
    "# CRITICAL: Verify Penn data currency before processing\n",
    "def verify_penn_data_source(matching_files):\n",
    "    \"\"\"\n",
    "    Verify the Penn data source and warn if using outdated data\n",
    "    \"\"\"\n",
    "    if not matching_files:\n",
    "        return None\n",
    "        \n",
    "    selected_file = matching_files[0]\n",
    "    file_info = {\n",
    "        'path': selected_file,\n",
    "        'filename': os.path.basename(selected_file),\n",
    "        'is_legacy': 'penn-2022-07-20' in selected_file,\n",
    "        'is_processed': 'pod-processing-outputs' in selected_file\n",
    "    }\n",
    "    \n",
    "    # Extract date from filename if possible\n",
    "    date_pattern = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "    date_match = re.search(date_pattern, file_info['filename'])\n",
    "    if date_match:\n",
    "        file_info['data_date'] = date_match.group(1)\n",
    "    else:\n",
    "        file_info['data_date'] = 'unknown'\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Load full Penn records - prioritize most recent processed data\n",
    "penn_full_paths = [\n",
    "    # PRIMARY: Direct path to known Penn data\n",
    "    f\"{input_dir}/penn_penn_filtered-marc21.parquet\",\n",
    "    \n",
    "    # SECONDARY: Penn parquet files from current processing pipeline\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*updates*marc21.parquet\",\n",
    "    \n",
    "    # TERTIARY: Any Penn marc21 parquet files in processing outputs\n",
    "    f\"{input_dir}/pod-processing-outputs/penn_*marc21.parquet\",\n",
    "    \n",
    "    # QUATERNARY: Check for raw Penn parquet files (less preferred)\n",
    "    f\"{input_dir}/pod_penn/file/**/*.parquet\"\n",
    "]\n",
    "\n",
    "# Add data source verification\n",
    "penn_full = None\n",
    "selected_source = None\n",
    "\n",
    "print(\"\\n=== PENN DATA SOURCE VERIFICATION ===\")\n",
    "for path_pattern in penn_full_paths:\n",
    "    try:\n",
    "        matching_files = glob.glob(path_pattern, recursive=True)\n",
    "        if matching_files:\n",
    "            # Sort files by modification time to get most recent\n",
    "            matching_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "            \n",
    "            source_info = verify_penn_data_source(matching_files)\n",
    "            if source_info:\n",
    "                print(f\"\\nFound Penn records at: {source_info['path']}\")\n",
    "                print(f\"  - Source type: {'Processed updates' if source_info['is_processed'] else 'Raw data'}\")\n",
    "                print(f\"  - Data date: {source_info['data_date']}\")\n",
    "                \n",
    "                # Warn if data appears old\n",
    "                if source_info['data_date'] != 'unknown':\n",
    "                    try:\n",
    "                        data_date = datetime.strptime(source_info['data_date'], '%Y-%m-%d')\n",
    "                        days_old = (datetime.now() - data_date).days\n",
    "                        if days_old > 365:\n",
    "                            print(f\"  ⚠️  WARNING: Data is {days_old} days old!\")\n",
    "                            print(f\"  ⚠️  Results may not reflect current Penn holdings\")\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Load the data\n",
    "                penn_full = spark.read.parquet(source_info['path'])\n",
    "                \n",
    "                # CRITICAL: Verify this is a MARC dataset with FLDR field\n",
    "                if \"FLDR\" not in penn_full.columns:\n",
    "                    print(f\"  ⚠️  WARNING: File does not contain FLDR field - not a valid MARC dataset\")\n",
    "                    penn_full = None\n",
    "                    continue\n",
    "                \n",
    "                selected_source = source_info\n",
    "                \n",
    "                # Verify record count and sample for currency check\n",
    "                record_count = penn_full.count()\n",
    "                print(f\"  - Total records: {record_count:,}\")\n",
    "                \n",
    "                # Sample check for recent cataloging activity\n",
    "                if 'F005' in penn_full.columns:\n",
    "                    recent_updates = penn_full.filter(\n",
    "                        col(\"F005\").rlike(\"202[4-5]\")\n",
    "                    ).count()\n",
    "                    recent_percentage = (recent_updates / record_count * 100) if record_count > 0 else 0\n",
    "                    print(f\"  - Recently updated records (2024-2025): {recent_updates:,} ({recent_percentage:.1f}%)\")\n",
    "                    \n",
    "                    if recent_percentage < 5:\n",
    "                        print(f\"  ⚠️  WARNING: Only {recent_percentage:.1f}% of records updated recently\")\n",
    "                        print(f\"  ⚠️  Data may be significantly outdated\")\n",
    "                \n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {path_pattern}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# If no MARC file with FLDR found, search broader for MARC datasets\n",
    "if penn_full is None or \"FLDR\" not in penn_full.columns:\n",
    "    print(\"\\n⚠️  MARC files without FLDR field detected! Searching for proper MARC datasets...\")\n",
    "    \n",
    "    # Search for any MARC21 parquet files\n",
    "    marc_paths = glob.glob(f\"{input_dir}/**/*marc21*.parquet\", recursive=True)\n",
    "    \n",
    "    if marc_paths:\n",
    "        print(f\"Found {len(marc_paths)} potential MARC datasets\")\n",
    "        for path in marc_paths:\n",
    "            try:\n",
    "                test_df = spark.read.parquet(path)\n",
    "                if \"FLDR\" in test_df.columns:\n",
    "                    # Verify this is a Penn dataset\n",
    "                    filename = os.path.basename(path)\n",
    "                    if \"penn\" in filename.lower():\n",
    "                        print(f\"✅ Found valid Penn MARC dataset with FLDR field: {path}\")\n",
    "                        penn_full = test_df\n",
    "                        selected_source = {\n",
    "                            'path': path,\n",
    "                            'filename': filename,\n",
    "                            'is_legacy': 'penn-2022-07-20' in path,\n",
    "                            'is_processed': 'pod-processing-outputs' in path\n",
    "                        }\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {path}: {str(e)}\")\n",
    "\n",
    "# Final fallback with strong warning\n",
    "if penn_full is None:\n",
    "    print(\"\\n⚠️  CRITICAL WARNING: No current Penn data found!\")\n",
    "    print(\"As a last resort, checking for legacy data...\")\n",
    "    \n",
    "    legacy_path = \"/home/jovyan/work/marc/parquet/penn-2022-07-20-full-marc21.parquet\"\n",
    "    if os.path.exists(legacy_path):\n",
    "        response = input(\"\\n🚨 Found 2022 Penn data. This is SEVERELY OUTDATED. Use anyway? (yes/no): \")\n",
    "        if response.lower() == 'yes':\n",
    "            penn_full = spark.read.parquet(legacy_path)\n",
    "            selected_source = {'is_legacy': True, 'filename': 'penn-2022-07-20-full-marc21.parquet'}\n",
    "            print(\"⚠️  Using 2022 data - results will NOT reflect current Penn holdings!\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No Penn full MARC records found and user declined legacy data\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not find Penn full MARC records!\")\n",
    "        print(\"Please ensure Penn MARC data has been converted to Parquet format.\")\n",
    "        print(\"Run the previous cells to process MARC files first.\")\n",
    "        raise FileNotFoundError(\"Penn full MARC records not found\")\n",
    "\n",
    "print(\"\\n=== PROCEEDING WITH ANALYSIS ===\")\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"⚠️  USING OUTDATED DATA - RESULTS MAY BE INACCURATE\")\n",
    "\n",
    "# CRITICAL: Verify penn_full dataset has the required MARC fields\n",
    "print(\"\\n=== Pre-Join Dataset Verification ===\")\n",
    "print(f\"Penn full dataset columns ({len(penn_full.columns)} total):\")\n",
    "# Print first 10 columns as a sample\n",
    "print(f\"Sample columns: {', '.join(penn_full.columns[:10])}...\")\n",
    "\n",
    "if \"FLDR\" not in penn_full.columns:\n",
    "    raise ValueError(\"ERROR: Penn dataset is missing the FLDR field required for analysis!\")\n",
    "\n",
    "# FIX: Check for duplicates in penn_full before joining\n",
    "print(\"\\n=== Checking for duplicate F001s in penn_full ===\")\n",
    "duplicate_check = penn_full.groupBy(\"F001\").count().filter(col(\"count\") > 1)\n",
    "duplicate_count = duplicate_check.count()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"⚠️  WARNING: Found {duplicate_count:,} F001 values with duplicates in penn_full\")\n",
    "    print(\"Deduplicating penn_full by F001...\")\n",
    "    \n",
    "    # Show sample of duplicates\n",
    "    print(\"\\nSample of duplicate F001s:\")\n",
    "    duplicate_check.orderBy(col(\"count\").desc()).show(5)\n",
    "    \n",
    "    # Deduplicate by keeping first occurrence\n",
    "    penn_full = penn_full.dropDuplicates([\"F001\"])\n",
    "    print(f\"After deduplication: {penn_full.count():,} records\")\n",
    "\n",
    "# OPTIMIZATION: Use broadcast join for better performance with small unique_penn_ids DataFrame\n",
    "unique_penn_ids = unique_penn.select(\"F001\").distinct()\n",
    "print(f\"\\nJoining {unique_penn_ids.count():,} unique Penn F001s with full Penn records...\")\n",
    "\n",
    "unique_penn_full = penn_full.join(F.broadcast(unique_penn_ids), on=\"F001\", how=\"inner\")\n",
    "\n",
    "# Verify join results\n",
    "joined_count = unique_penn_full.count()\n",
    "print(f\"Joined result: {joined_count:,} records\")\n",
    "\n",
    "# FIX: Now the counts should match after deduplication\n",
    "if joined_count != unique_penn_count:\n",
    "    print(f\"⚠️  WARNING: Join count mismatch! Expected {unique_penn_count:,}, got {joined_count:,}\")\n",
    "    \n",
    "    # Investigate missing records\n",
    "    missing_count = unique_penn_count - joined_count\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing {missing_count:,} records - these F001s exist in filtered dataset but not in penn_full\")\n",
    "        \n",
    "        # Find which F001s are missing\n",
    "        missing_f001s = unique_penn_ids.join(penn_full.select(\"F001\").distinct(), on=\"F001\", how=\"left_anti\")\n",
    "        missing_sample = missing_f001s.limit(10).collect()\n",
    "        if missing_sample:\n",
    "            print(\"Sample of missing F001s:\")\n",
    "            for row in missing_sample:\n",
    "                print(f\"  - {row['F001']}\")\n",
    "else:\n",
    "    print(\"✅ Join count matches expected - all filtered records found in penn_full\")\n",
    "\n",
    "# Verify join kept FLDR column\n",
    "print(\"\\n=== Post-Join Dataset Verification ===\")\n",
    "print(f\"Joined dataset columns ({len(unique_penn_full.columns)} total):\")\n",
    "# Print first 10 columns as a sample\n",
    "print(f\"Sample columns: {', '.join(unique_penn_full.columns[:10])}...\")\n",
    "\n",
    "if \"FLDR\" not in unique_penn_full.columns:\n",
    "    raise ValueError(\"ERROR: FLDR column was lost during join operation!\")\n",
    "\n",
    "# Check available columns before filtering\n",
    "print(\"\\n=== Checking available columns for filtering ===\")\n",
    "available_columns = unique_penn_full.columns\n",
    "print(f\"Looking for F533 column to filter reproduction notes...\")\n",
    "\n",
    "# Start with the base dataset\n",
    "df_with_material_type = unique_penn_full\n",
    "\n",
    "# Only apply F533 filter if the column exists\n",
    "if \"F533\" in available_columns:\n",
    "    print(\"Filtering out records with F533 (reproduction notes)\")\n",
    "    df_with_material_type = df_with_material_type.filter(col(\"F533\").isNull())\n",
    "else:\n",
    "    print(\"Note: F533 column not found in dataset, skipping reproduction filter\")\n",
    "\n",
    "# Continue with the rest of the transformations\n",
    "unique_penn_with_material_type = (df_with_material_type\n",
    "    # Add material type columns\n",
    "    .withColumn(\"record_type\", substring(col(\"FLDR\"), 7, 1))\n",
    "    .withColumn(\"bib_level\", substring(col(\"FLDR\"), 8, 1))\n",
    "    .withColumn(\"combined_type\", concat(col(\"record_type\"), col(\"bib_level\")))\n",
    "    .withColumn(\"material_category\", \n",
    "        when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"m\")), \"print_book\")\n",
    "        .when((col(\"record_type\") == \"a\") & (col(\"bib_level\").isin(\"s\")), \"print_serial\")\n",
    "        .when((col(\"record_type\") == \"c\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_music\")\n",
    "        .when((col(\"record_type\") == \"e\") & (col(\"bib_level\").isin(\"m\", \"s\")), \"print_maps\")\n",
    "        .when(col(\"record_type\") == \"m\", \"electronic_resource\")\n",
    "        .when(col(\"record_type\").isin(\"g\", \"k\"), \"visual_material\")\n",
    "        .when(col(\"record_type\") == \"i\", \"audio_material\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "    .withColumn(\"is_print\", \n",
    "        col(\"material_category\").isin(\"print_book\", \"print_serial\", \"print_music\", \"print_maps\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache before multiple operations\n",
    "unique_penn_with_material_type.cache()\n",
    "\n",
    "# OPTIMIZATION: Get all statistics in one pass\n",
    "print(\"\\n=== Material Type Distribution ===\")\n",
    "material_stats = unique_penn_with_material_type.groupBy(\"material_category\", \"is_print\").count().collect()\n",
    "\n",
    "# Process statistics\n",
    "material_counts_dict = {}\n",
    "print_count = 0\n",
    "non_print_count = 0\n",
    "\n",
    "for row in material_stats:\n",
    "    material_counts_dict[row[\"material_category\"]] = row[\"count\"]\n",
    "    if row[\"is_print\"]:\n",
    "        print_count += row[\"count\"]\n",
    "    else:\n",
    "        non_print_count += row[\"count\"]\n",
    "\n",
    "total_unique = print_count + non_print_count\n",
    "\n",
    "# Display material distribution\n",
    "for category, count in sorted(material_counts_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{category}: {count:,}\")\n",
    "\n",
    "# Filter for print materials only\n",
    "print_only_df = unique_penn_with_material_type.filter(col(\"is_print\") == True)\n",
    "\n",
    "# Add metadata if we have source information\n",
    "if selected_source:\n",
    "    print_only_df_with_metadata = print_only_df.withColumn(\n",
    "        \"processing_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    ).withColumn(\n",
    "        \"source_file\", lit(selected_source.get('filename', 'unknown'))\n",
    "    ).withColumn(\n",
    "        \"data_currency_warning\", \n",
    "        lit(\"OUTDATED - 2022 data\" if selected_source.get('is_legacy') else \"Current\")\n",
    "    )\n",
    "else:\n",
    "    print_only_df_with_metadata = print_only_df\n",
    "\n",
    "# Save datasets\n",
    "unique_penn_with_material_type.write.mode(\"overwrite\").parquet(f\"{output_dir}/unique_penn_full_no_533.parquet\")\n",
    "print_only_df_with_metadata.write.mode(\"overwrite\").parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\n=== Print Material Analysis ===\")\n",
    "print(f\"Total unique Penn records: {total_unique:,}\")\n",
    "\n",
    "if total_unique > 0:\n",
    "    print(f\"Print materials: {print_count:,} ({print_count/total_unique*100:.1f}%)\")\n",
    "    print(f\"Non-print materials: {non_print_count:,} ({non_print_count/total_unique*100:.1f}%)\")\n",
    "    \n",
    "    # Show print categories breakdown\n",
    "    print(\"\\n=== Print Material Categories ===\")\n",
    "    print_categories = [\"print_book\", \"print_serial\", \"print_music\", \"print_maps\"]\n",
    "    for category in print_categories:\n",
    "        if category in material_counts_dict:\n",
    "            count = material_counts_dict[category]\n",
    "            print(f\"{category}: {count:,} ({count/print_count*100:.1f}% of print materials)\")\n",
    "else:\n",
    "    print(\"No unique Penn records found to analyze\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "unique_penn_with_material_type.unpersist()\n",
    "\n",
    "# Final warning if using outdated data\n",
    "if selected_source and selected_source.get('is_legacy'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚨 CRITICAL WARNING: Analysis completed using 2022 Penn data\")\n",
    "    print(\"🚨 Results do NOT reflect current Penn holdings\")\n",
    "    print(\"🚨 Recommended: Re-run with current Penn MARC export\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing print_only_df DataFrame\n",
      "Creating stratified sample based on material_category...\n",
      "Strata distribution:\n",
      "  - print_book: 697,799 records (95.22%)\n",
      "  - print_maps: 903 records (0.12%)\n",
      "  - print_music: 11,382 records (1.55%)\n",
      "  - print_serial: 22,719 records (3.10%)\n",
      "First pass sample size: 964\n",
      "Need 36 more records to reach target sample size\n",
      "Added 36 additional records\n",
      "Final sample size: 1041\n",
      "\n",
      "Sample distribution by material_category:\n",
      "  - print_book: 1002 (96.25% of sample vs 95.22% of population)\n",
      "  - print_maps: 4 (0.38% of sample vs 0.12% of population)\n",
      "  - print_music: 13 (1.25% of sample vs 1.55% of population)\n",
      "  - print_serial: 22 (2.11% of sample vs 3.10% of population)\n",
      "\n",
      "✅ Processing complete!\n",
      "Results saved to /home/jovyan/work/July-2025-PODParquet/pod-processing-outputs/\n",
      "\n",
      "Final outputs:\n",
      "  - unique_penn.parquet: All unique Penn records\n",
      "  - physical_books_no_533.parquet: Unique Penn physical books\n",
      "  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\n",
      "  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\n",
      "  - sample_summary_no_hsp.json: Summary statistics\n",
      "\n",
      "📊 Summary Statistics:\n",
      "  - Total Penn records: 2,443,080\n",
      "  - Unique Penn records: 810,502\n",
      "  - Uniqueness rate: 33.2%\n",
      "  - Print materials: 732,803\n",
      "  - Print materials percentage: 0.9%\n",
      "  - Sample size: 262\n",
      "\n",
      "📚 Material Category Breakdown:\n",
      "  - audio_material: 1,013 (0.1%)\n",
      "  - electronic_resource: 410 (0.1%)\n",
      "  - other: 38,909 (5.3%)\n",
      "  - print_book: 697,799 (95.2%)\n",
      "  - print_maps: 903 (0.1%)\n",
      "  - print_music: 11,382 (1.6%)\n",
      "  - print_serial: 22,719 (3.1%)\n",
      "  - visual_material: 37,367 (5.1%)\n"
     ]
    }
   ],
   "source": [
    "# Stratified Sampling and Final Analysis\n",
    "from pyspark.sql.functions import rand, col\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "from datetime import datetime \n",
    "import builtins  # Import builtins to access Python's built-in functions\n",
    "\n",
    "# Define output directory if not already defined\n",
    "if 'output_dir' not in locals():\n",
    "    output_dir = \"/home/jovyan/work/July-2025-PODParquet/pod-processing-outputs\"\n",
    "\n",
    "# Load print materials dataset if not already loaded\n",
    "if 'print_only_df' not in locals() or print_only_df is None:\n",
    "    print(\"Loading print materials dataset...\")\n",
    "    print_only_df_raw = spark.read.parquet(f\"{output_dir}/physical_books_no_533.parquet\")\n",
    "    \n",
    "    # Check if metadata columns exist and drop them for sampling\n",
    "    metadata_cols = [\"processing_date\", \"source_file\", \"data_currency_warning\"]\n",
    "    existing_metadata_cols = [col for col in metadata_cols if col in print_only_df_raw.columns]\n",
    "    \n",
    "    if existing_metadata_cols:\n",
    "        print(f\"Dropping metadata columns: {existing_metadata_cols}\")\n",
    "        print_only_df = print_only_df_raw.drop(*existing_metadata_cols)\n",
    "    else:\n",
    "        print_only_df = print_only_df_raw\n",
    "else:\n",
    "    print(\"Using existing print_only_df DataFrame\")\n",
    "\n",
    "# Load or compute necessary statistics if not available\n",
    "if 'total_penn' not in locals() or 'unique_penn_count' not in locals():\n",
    "    print(\"Loading required statistics...\")\n",
    "    # Load from saved parquet files\n",
    "    if 'unique_penn' not in locals():\n",
    "        unique_penn = spark.read.parquet(f\"{output_dir}/unique_penn.parquet\")\n",
    "    unique_penn_count = unique_penn.count()\n",
    "    \n",
    "    # Load Penn overlap analysis to get total Penn records\n",
    "    penn_overlap = spark.read.parquet(f\"{output_dir}/penn_overlap_analysis.parquet\")\n",
    "    total_penn = penn_overlap.select(\"F001\").distinct().count()\n",
    "\n",
    "# Compute print statistics if not available\n",
    "if 'print_count' not in locals() or 'material_counts_dict' not in locals():\n",
    "    print(\"Computing material type statistics...\")\n",
    "    # Check for material_category column\n",
    "    if 'material_category' not in print_only_df.columns:\n",
    "        print(\"ERROR: material_category column not found in print_only_df\")\n",
    "        raise ValueError(\"Missing required column: material_category\")\n",
    "    \n",
    "    material_stats = print_only_df.groupBy(\"material_category\").count().collect()\n",
    "    material_counts_dict = {row[\"material_category\"]: row[\"count\"] for row in material_stats}\n",
    "    print_count = sum(material_counts_dict.values())\n",
    "\n",
    "# Define sampling function with improved stratification\n",
    "def create_stratified_sample(df, strata_column, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with improved randomization.\n",
    "    Uses multiple passes to ensure representation of all strata.\n",
    "    \"\"\"\n",
    "    print(f\"Creating stratified sample based on {strata_column}...\")\n",
    "    \n",
    "    # Verify the strata column exists\n",
    "    if strata_column not in df.columns:\n",
    "        print(f\"ERROR: Column '{strata_column}' not found in DataFrame\")\n",
    "        print(f\"Available columns: {df.columns}\")\n",
    "        raise ValueError(f\"Missing required column: {strata_column}\")\n",
    "    \n",
    "    # Get counts by strata for weighting\n",
    "    strata_counts = df.groupBy(strata_column).count().collect()\n",
    "    total_records = df.count()\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"WARNING: No records to sample from!\")\n",
    "        return df\n",
    "    \n",
    "    strata_map = {row[strata_column]: row[\"count\"] for row in strata_counts}\n",
    "    print(f\"Strata distribution:\")\n",
    "    for strata, count in sorted(strata_map.items()):\n",
    "        print(f\"  - {strata}: {count:,} records ({count/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate proportional sample sizes with minimum threshold\n",
    "    min_per_strata = 5  # Ensure at least a few records from each stratum\n",
    "    sample_fractions = {}\n",
    "    \n",
    "    for strata, count in strata_map.items():\n",
    "        # Proportional sampling with minimum threshold\n",
    "        if count > 0:\n",
    "            # Calculate proportional share but ensure at least min_per_strata\n",
    "            prop_size = builtins.max(\n",
    "                min_per_strata,\n",
    "                int((count / total_records) * sample_size)\n",
    "            )\n",
    "            \n",
    "            # Don't sample more than we have - use Python's built-in min\n",
    "            prop_size = builtins.min(prop_size, count)\n",
    "            \n",
    "            # Calculate fraction\n",
    "            sample_fractions[strata] = prop_size / count\n",
    "    \n",
    "    # First pass: Stratified sampling\n",
    "    sampled_df = df.sampleBy(strata_column, fractions=sample_fractions, seed=42)\n",
    "    \n",
    "    # Check if we need a second pass to reach target size\n",
    "    current_size = sampled_df.count()\n",
    "    print(f\"First pass sample size: {current_size}\")\n",
    "    \n",
    "    if current_size < sample_size and current_size < total_records:\n",
    "        # Second pass: Sample from under-represented strata\n",
    "        remaining = builtins.min(sample_size - current_size, total_records - current_size)\n",
    "        print(f\"Need {remaining} more records to reach target sample size\")\n",
    "        \n",
    "        # Get records not in first sample\n",
    "        sampled_ids = sampled_df.select(\"F001\").distinct()\n",
    "        remaining_df = df.join(sampled_ids, on=\"F001\", how=\"left_anti\")\n",
    "        \n",
    "        remaining_count = remaining_df.count()\n",
    "        if remaining_count > 0:\n",
    "            # Simple random sample from remaining records\n",
    "            additional_sample = remaining_df.orderBy(rand(seed=43)).limit(remaining)\n",
    "            \n",
    "            # Union the samples\n",
    "            sampled_df = sampled_df.union(additional_sample)\n",
    "            print(f\"Added {builtins.min(remaining, remaining_count)} additional records\")\n",
    "    \n",
    "    final_size = sampled_df.count()\n",
    "    print(f\"Final sample size: {final_size}\")\n",
    "    \n",
    "    # Check distribution in final sample\n",
    "    sample_distribution = sampled_df.groupBy(strata_column).count().collect()\n",
    "    print(f\"\\nSample distribution by {strata_column}:\")\n",
    "    sample_dict = {row[strata_column]: row[\"count\"] for row in sample_distribution}\n",
    "    \n",
    "    for strata_val in sorted(strata_map.keys()):\n",
    "        original_count = strata_map.get(strata_val, 0)\n",
    "        sample_count = sample_dict.get(strata_val, 0)\n",
    "        if original_count > 0 and final_size > 0:\n",
    "            print(f\"  - {strata_val}: {sample_count} ({sample_count/final_size*100:.2f}% of sample vs {original_count/total_records*100:.2f}% of population)\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Create a stratified sample by material category\n",
    "sample_df = create_stratified_sample(print_only_df, \"material_category\", sample_size=1000)\n",
    "\n",
    "# Cache the sample for better performance\n",
    "sample_df.cache()\n",
    "\n",
    "# Save the sample for API validation\n",
    "sample_df.write.mode(\"overwrite\").parquet(f\"{output_dir}/statistical_sample_for_api_no_hsp.parquet\")\n",
    "\n",
    "# Select key fields for the CSV, handling array fields\n",
    "sample_for_csv = sample_df.select(\n",
    "    \"F001\", \n",
    "    # F020 is an array - get first ISBN if available\n",
    "    F.when(F.col(\"F020\").isNotNull() & (F.size(F.col(\"F020\")) > 0), \n",
    "           F.col(\"F020\").getItem(0)).otherwise(\"\").alias(\"F020\"),\n",
    "    \"F010\",  # This is already a string\n",
    "    \"F245\",  # This is already a string\n",
    "    # F250 is an array - get first edition statement if available\n",
    "    F.when(F.col(\"F250\").isNotNull() & (F.size(F.col(\"F250\")) > 0), \n",
    "           F.col(\"F250\").getItem(0)).otherwise(\"\").alias(\"F250\"),\n",
    "    # F260 is an array - get first publication info if available\n",
    "    F.when(F.col(\"F260\").isNotNull() & (F.size(F.col(\"F260\")) > 0), \n",
    "           F.col(\"F260\").getItem(0)).otherwise(\"\").alias(\"F260\"),\n",
    "    \"material_category\"\n",
    ")\n",
    "\n",
    "# Save as CSV (single file for easier review)\n",
    "sample_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/statistical_sample_for_api_no_hsp.csv\")\n",
    "\n",
    "# Generate final summary statistics in JSON format\n",
    "summary_stats = {\n",
    "    \"processing_timestamp\": datetime.now().isoformat(),\n",
    "    \"total_penn_records\": int(total_penn),\n",
    "    \"unique_penn_records\": int(unique_penn_count),\n",
    "    \"uniqueness_rate\": float(unique_penn_count/total_penn) if total_penn > 0 else 0.0,\n",
    "    \"print_materials\": int(print_count),\n",
    "    \"print_materials_percentage\": float(print_count/unique_penn_count) if unique_penn_count > 0 else 0.0,\n",
    "    \"sample_size\": int(sample_df.count()),\n",
    "    \"material_categories\": {}\n",
    "}\n",
    "\n",
    "# Add material categories to summary\n",
    "for category, count in sorted(material_counts_dict.items()):\n",
    "    summary_stats[\"material_categories\"][category] = {\n",
    "        \"count\": int(count),\n",
    "        \"percentage\": float(count/print_count*100) if print_count > 0 else 0.0\n",
    "    }\n",
    "\n",
    "# Write summary to JSON file\n",
    "with open(f\"{output_dir}/sample_summary_no_hsp.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "# Unpersist the cached sample\n",
    "sample_df.unpersist()\n",
    "\n",
    "print(\"\\n✅ Processing complete!\")\n",
    "print(f\"Results saved to {output_dir}/\")\n",
    "print(\"\\nFinal outputs:\")\n",
    "print(f\"  - unique_penn.parquet: All unique Penn records\")\n",
    "print(f\"  - physical_books_no_533.parquet: Unique Penn physical books\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.parquet: Statistical sample for validation\")\n",
    "print(f\"  - statistical_sample_for_api_no_hsp.csv: CSV version of sample\")\n",
    "print(f\"  - sample_summary_no_hsp.json: Summary statistics\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n📊 Summary Statistics:\")\n",
    "print(f\"  - Total Penn records: {summary_stats['total_penn_records']:,}\")\n",
    "print(f\"  - Unique Penn records: {summary_stats['unique_penn_records']:,}\")\n",
    "print(f\"  - Uniqueness rate: {summary_stats['uniqueness_rate']*100:.1f}%\")\n",
    "print(f\"  - Print materials: {summary_stats['print_materials']:,}\")\n",
    "print(f\"  - Print materials percentage: {summary_stats['print_materials_percentage']:.1f}%\")\n",
    "print(f\"  - Sample size: {summary_stats['sample_size']:,}\")\n",
    "\n",
    "# Display material category breakdown\n",
    "if material_counts_dict:\n",
    "    print(\"\\n📚 Material Category Breakdown:\")\n",
    "    for category, info in sorted(summary_stats[\"material_categories\"].items()):\n",
    "        print(f\"  - {category}: {info['count']:,} ({info['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned up 1 cached DataFrames and 0 temporary views\n",
      "💡 Memory freed. You can safely re-run the notebook or close it.\n",
      "\n",
      "📊 Spark UI still available at: http://7d7ed4cc3e7b:4040\n",
      "Check the Storage tab to verify all caches are cleared\n"
     ]
    }
   ],
   "source": [
    "# Optional Cleanup - Run this to free memory after analysis\n",
    "def cleanup_spark_resources():\n",
    "    \"\"\"Clean up all cached DataFrames and temporary views\"\"\"\n",
    "    try:\n",
    "        # Get all cached DataFrames\n",
    "        cached_count = len(spark.sparkContext._jsc.getPersistentRDDs().items())\n",
    "        \n",
    "        for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "            rdd.unpersist()\n",
    "        \n",
    "        # Drop all temporary views\n",
    "        temp_views = [view.name for view in spark.catalog.listTables() if view.isTemporary]\n",
    "        for view_name in temp_views:\n",
    "            spark.catalog.dropTempView(view_name)\n",
    "        \n",
    "        print(f\"✅ Cleaned up {cached_count} cached DataFrames and {len(temp_views)} temporary views\")\n",
    "        print(\"💡 Memory freed. You can safely re-run the notebook or close it.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning: {e}\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_spark_resources()\n",
    "\n",
    "# Optional: Show memory status\n",
    "print(\"\\n📊 Spark UI still available at:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"Check the Storage tab to verify all caches are cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
